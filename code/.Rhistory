# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutations
require(assocInd)
# Run multiple cores for faster computing
require(doParallel)
require(microbenchmark)
require(parallel)
require(foreach)
require(progress)
# Read file in
orig_data<- read.csv("secondgen_data.csv")
# Read file in
orig_data<- read.csv("secondgen_data.csv")
library(readr)
secondgen_data <- read_csv("~/Documents/GitHub/Dolphins/data/secondgen_data.csv")
View(secondgen_data)
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data<-secondgen_data
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data <- subset(orig_data, subset=c(orig_data$Year == 2005))
# Make sure every ID has >10 obs
ID <- unique(sample_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(sample_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(sample_data, sample_data$Code %in% c(sub$ID))
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Gambit of the group index
gbi<- get_group_by_individual(sample_data, data_format = "individuals")
write.csv(gbi, "gbi.csv")
# Create association matrix
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
n.cores
system.time({
registerDoParallel(n.cores)
nxn<- SRI.func(gbi)
})
# end parallel processing
stopImplicitCluster()
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
#  Create 1000 random group-by-individual binary matrices
reps<- 1000
nF <- null(gbi, iter=reps)
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_null <- rep(NA,reps)
registerDoParallel(n.cores)
null <- foreach(i = 1:reps,
.combine = c) %dopar% {
# replace c with rbind to create a dataframe
sri_null = as.matrix(SRI.func(nF[[i]]))
( sd(sri_null) / mean(sri_null) ) * 100}
nF <- null(gbi, iter=reps)
#  Create 1000 random group-by-individual binary matrices
reps<- 1000
nF <- null(gbi, iter=reps)
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
orig_data <- read_csv("~/Documents/GitHub/Dolphins/data/secondgen_data.csv")
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data <- subset(orig_data, subset=c(orig_data$Year == 2005))
# Make sure every ID has >10 obs
ID <- unique(sample_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(sample_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(sample_data, sample_data$Code %in% c(sub$ID))
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Gambit of the group index
gbi<- get_group_by_individual(sample_data, data_format = "individuals")
View(gbi)
# Gambit of the group index
gbi<- get_group_by_individual(group_data, data_format = "individuals")
write.csv(gbi, "gbi.csv")
# Create association matrix
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn<- SRI.func(gbi)
})
nxn<-as.matrix(nxn)
# end parallel processing
stopImplicitCluster()
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
#  Create 1000 random group-by-individual binary matrices
reps<- 1000
nF <- null(gbi, iter=reps)
library(vegan)
help(vegdist)
data(varespec)
str(varespec)
varespec
help("varespec")
t(varespec)
View(orig_data)
View(sample_data)
View(group_data)
View(group_data)
View(orig_data)
cbind(orig_data$Code, orig_data$Behaviors, orig_data$HumanInteraction)
orig_data[1:100, c('Code', 'Behaviors', 'HumanInteraction')]
# toy raw data
aux = orig_data[1:100, c('Code', 'Behaviors', 'HumanInteraction')]
str(aux)
table(aux)
table(aux$Behaviors)
# toy raw data
aux = orig_data[1:1000, c('Code', 'Behaviors', 'HumanInteraction')]
aux[which(aux$Behaviors == 'pFeed')]
table(aux$Behaviors)
aux[which(aux$Behaviors == 'Feed')]
aux[which(aux$Behaviors == 'Feed'), ]
aux[which(aux$Behaviors == 'pFeed'), ]
aux
table(aux$HumanInteraction)
#
aux[which(aux$HumanInteraction != 999), ]
#
t(varespec)[,1:2]
#
t(varespec)[,1]
#
fake_HIprop = t(varespec)[,1]
vegdist(fake_HIprop, method = 'Euclidean')
vegdist(fake_HIprop, method = 'euclidean')
as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
t(varespec)[,1]
# toy raw data
aux = orig_data[1:1000, c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
library(tidyverse)
# toy raw data
aux = orig_data[1:1000,
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
aux
sub("Feed", "", aux$Behaviors)
sub("Feed", "", aux$Behaviors[1:10])
table(aux$Behaviors)
as.vector(table(aux$Behaviors))
grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)
aux$Behaviors[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)]
aux$Foraging = NA
aux
aux$Behaviors[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)]
aux$Foraging = "Other"
aux$Foraging = "Other"
aux
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
aux
table(aux$Code, aux$Foraging)
table(aux$Code, aux$ConfHI)
rawHI = table(aux$Code, aux$ConfHI)
rawHI = as.matrix(table(aux$Code, aux$ConfHI))
rawHI
rawHI = as.matrix(table(aux$Code, aux$ConfHI)[2:6])
rawHI
rawHI = as.matrix(table(aux$Code, aux$ConfHI)[,2:6])
rawHI
IDdata = table(aux$Code, aux$Foraging)
IDdata
IDbehav = table(aux$Code, aux$Foraging)
IDbehav$Feed
IDbehav[,1]
IDdata = IDbehav[,1]
rowSum(rawHI)
rowSums(rawHI)
table(aux$Code, aux$ConfHI)
rowSums(rawHI)
IDdata$HI = rowSums(rawHI)
IDdata$HI = as.vector(rowSums(rawHI))
IDdata
IDdata = IDbehav[,1]
IDdata
IDdata = data.frame(Foraging = IDbehav[,1])
IDdata = data.frame(Foraging = IDbehav[,1])
IDdata
IDdata$HI = as.vector(rowSums(rawHI))
IDdata
IDdata$HIprop = IDdata$HI/IDdata$Foraging
IDdata
table(aux$Code)
IDdata = data.frame(Foraging = table(aux$Code))
IDdata = data.frame(Obs = table(aux$Code))
IDdata
IDdata = table(aux$Code)
IDdata
IDdata = as.data.frame(Obs = table(aux$Code))
IDdata = as.data.frame(table(aux$Code)
)
IDdata
IDdata$HI = as.vector(rowSums(rawHI))
IDdata$HIprop = IDdata$HI/IDdata$Freq
IDdata
rawHI
row.names(rawHI)
IDdata
identical(row.names(rawHI), aux$Var1)
identical(row.names(rawHI), IDdata$Var1)
