sample_data <- read.csv("sample_data.csv")
length(unique(sample_data$Code))
list_years <- readRDS("list_years.RData")
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- tapply(sample_data$Date, sample_data$Year, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years$Date, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
effort
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
unique_ID_year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- cbind(effort, pop) # Days per year and pop size per year
pop_effort
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
pop_effort
rownames(pop_effort) <- c('Days Surveyed', 'Number of Indivduals')
pop_effort
plot(pop_effort$effort ~ pop_effort$unique_ID_year)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,] ~ pop_effort[2,])
pop_effort[1,]
pop_effort[2,]
pop_effort[1,c(1:7)]
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
## Compare effort to population size
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
effort
pop_effort
# Read in different behavior's data frames
IDbehav_Beg <- readRDS("IDbehav_Beg.RData")
IDbehav_Pat <- readRDS("IDbehav_Pat.RData")
IDbehav_Dep <- readRDS("IDbehav_Dep.RData")
Beg_effort <- lapply(IDbehav_Beg, function(df) length(unique(df$Code)))
View(IDbehav_Beg)
length(unique(IDbehav_Beg[[1]]$Code=='B'))
IDbehav_Beg[[1]][["HI"]]
length(unique(IDbehav_Beg[[1]]$Code[IDbehav_Beg[[1]]$HI > 0]))
Beg_effort <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0])))
Beg_effort
Pat_effort <- as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI > 0]))))
Pat_effort
Beg_effort <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0]))))
Beg_effort
Dep_effort <- as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI > 0]))))
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
colnames(Beg_effort) <- c(1:7)
colnames(Pat_effort) <- c(1:7)
colnames(Dep_effort) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
effort
colnames(effort) <- c(1:7)
colnames(unique_ID_year) <- c(1:7)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
unique_ID_year
colnames(unique_ID_year) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
pop_effort
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals', 'Beggars', 'Patrollers', 'Depredators')
pop_effort
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
length(unique(orig_data$Code))
length(unique(sample_data$Code))
sum(pop_effort[2,])
sum(pop_effort[c(3:5),])
sum(unique(sample_data$Code[sample_data$ConfHI != 0]))
unique(sample_data$Code[sample_data$ConfHI != 0])
length(unique(sample_data$Code[sample_data$ConfHI != 0]))
sum(pop_effort[c(3),])
sum(pop_effort[c(4),])
sum(pop_effort[c(5),])
lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
sum(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
nb <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
nb <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
sum(nb)
sum(as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI == 0])))))
sum(as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI == 0])))))
all <- subset(IDbehav_Beg, IDbehav_Beg$Code %in% IDbehav_Pat$Code)
all
Beg <- as.data.frame(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0]))
View(Beg)
?append
Beg <- append(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0])))
Beg
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI = 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0]))))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0])))
Beg %in% Pat %in% Dep
sum(Beg %in% Pat %in% Dep)
sum(Beg %in% Dep)
sum(Pat %in% Dep)
Reduce(intersect, list(Beg, Pat, Dep))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI != 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI != 0])))
Reduce(intersect, list(Beg, Pat, Dep))
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("../data")
# Load all necessary packages
library(asnipe) # get_group_by_individual--Damien Farine
library(assocInd) # Could do permutatioNP
library(vegan)
library(assortnet) # associative indices
library(kinship2) # genetic relatedness
library(ggplot2) # Visualization
library(abind) # array
library(MCMCglmm) # MCMC models
library(coda)
library(bayesplot) # plot parameters
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(rgdal) # Overlap
source("../code/functions.R") # nxn
# Read in data
orig_data <- read.csv("orig_data.csv") # original data
# Read in data
orig_data <- read.csv("../data/orig_data.csv") # original data
# Visualize data: HAB v HI
HAB_HI_data <- orig_data[, c("Year", "ConfHI")]
HAB_HI_data$ConfHI <- ifelse(HAB_HI_data$ConfHI != "0", 1, 0)
HAB_HI_data <- aggregate(ConfHI ~ Year, data = HAB_HI_data, FUN = function(x) sum(x == 1))
HAB_HI_data$HAB <- c(22, 13, rep(0, 2), 5, 0, 12, 8, 18, 5, 38, 19, 2, rep(0, 4), 9)
# Create a barplot
ggplot(aes(x = Year), data = HAB_HI_data) +
geom_bar(aes(y = ConfHI, fill = "ConfHI"), stat = "identity", alpha = 0.5, position = position_dodge(width = 0.8)) +
geom_bar(aes(y = HAB, fill = "HAB"), stat = "identity", alpha = 0.5, position = position_dodge(width = 0.8)) +
scale_y_continuous(name = "ConfHI", sec.axis = sec_axis(~., name = "HAB")) +
labs(title = "HAB and HI over Years", x = "Year") +
scale_fill_manual(values = c("ConfHI" = "blue", "HAB" = "orange"),
name = "Variables",
labels = c("ConfHI", "HAB"))
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("../data")
# Load all necessary packages
library(asnipe) # get_group_by_individual--Damien Farine
library(assocInd) # Could do permutatioNP
library(vegan)
library(assortnet) # associative indices
library(kinship2) # genetic relatedness
library(ggplot2) # Visualization
library(abind) # array
library(MCMCglmm) # MCMC models
library(coda)
library(bayesplot) # plot parameters
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(rgdal) # Overlap
source("../code/functions.R") # nxn
# Read in data
orig_data <- read.csv("../data/orig_data.csv") # original data
# Visualize data: HAB v HI
HAB_HI_data <- orig_data[, c("Year", "ConfHI")]
HAB_HI_data$ConfHI <- ifelse(HAB_HI_data$ConfHI != "0", 1, 0)
HAB_HI_data <- aggregate(ConfHI ~ Year, data = HAB_HI_data, FUN = function(x) sum(x == 1))
HAB_HI_data$HAB <- c(22, 13, rep(0, 2), 5, 0, 12, 8, 18, 5, 38, 19, 2, rep(0, 4), 9)
# Create a barplot
ggplot(aes(x = Year), data = HAB_HI_data) +
geom_bar(aes(y = ConfHI, fill = "ConfHI"), stat = "identity", alpha = 0.5, position = position_dodge(width = 0.8)) +
geom_bar(aes(y = HAB, fill = "HAB"), stat = "identity", alpha = 0.5, position = position_dodge(width = 0.8)) +
scale_y_continuous(name = "ConfHI", sec.axis = sec_axis(~., name = "HAB")) +
labs(title = "HAB and HI over Years", x = "Year") +
scale_fill_manual(values = c("ConfHI" = "blue", "HAB" = "orange"),
name = "Variables",
labels = c("ConfHI", "HAB"))
# Read in social association matrix and listed data
## Two period data
dist_HI <- readRDS("dist_HI.RData") # HI Sim Matrix
# Read in social association matrix and listed data
## Two period data
dist_HI <- readRDS("../data/dist_HI.RData") # HI Sim Matrix
ILV_mat <-readRDS("ILV_mat.RData") # Age and Sex Matrices
# Read in social association matrix and listed data
## Two period data
dist_HI <- readRDS("../data/dist_HI.RData") # HI Sim Matrix
ILV_mat <-readRDS("../data/ILV_mat.RData") # Age and Sex Matrices
kov <- readRDS("../data/kov.RDS")  # Home range overlap
nxn <- readRDS("../data/nxn.RData") # Association Matrix
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
# Format data
period = 2
upper_tri <- lapply(nxn, function(df) upper.tri(df, diag = TRUE))
edge_nxn <- abind(lapply(nxn, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
## Split by 2 for data
HAB_data <- as.data.frame(cbind(c(edge_nxn[,1], edge_nxn[,2]), c(rep(0, nrow(edge_nxn)), rep(1, nrow(edge_nxn))))) # Two
colnames(HAB_data) <- c("SRI", "HAB")
HI <- abind(lapply(dist_HI, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Put data into a dataframe
df_list = data.frame(edge_weight = HAB_data[, 1],
HAB = HAB_data[, 2],
HRO = unlist(lapply(kov, function (df) df[upper.tri(df, diag = TRUE)])),
sex_similarity = rep(ILV_mat[[1]][upper.tri(ILV_mat[[1]], diag = TRUE)], period),
age_difference = rep(ILV_mat[[2]][upper.tri(ILV_mat[[2]], diag = TRUE)], period),
HI_differences = c(HI[,c(1:period)]),
node_id_1 = unlist(one),
node_id_2 = unlist(two))
## HI Behavior Combined Two Year Period ##
fit_mcmc.1 <- MCMCglmm(edge_weight ~ HI_differences * HAB + HRO + age_difference + sex_similarity,
random=~mm(node_id_1 + node_id_2), data = df_list, nitt = 20000)
# Check for model convergence
model <- fit_mcmc.2
# Check for model convergence
model <- fit_mcmc.1
# Extract Posteriors
posterior <- model$Sol
# Summary of parameters
summary(fit_mcmc.1)
# Plot the posterior distribution
mcmc_intervals(posterior, pars = c("(Intercept)", "HI_differences", "HI_differences:HAB", "HAB", "age_difference", "sex_similarity", "HRO"))
## Three period data
dist_HI <- readRDS("dist_HI_int.RData") # HI Sim Matrix
## Three period data
dist_HI <- readRDS("../data/dist_HI_int.RData") # HI Sim Matrix
ILV_mat <-readRDS("../data/ILV_mat_int.RData") # Age and Sex Matrices
kov <- readRDS("../data/kov_int.RDS")  # Home range overlap
nxn <- readRDS("../data/nxn_int.RData") # Association Matrix
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
# Format data
period = 3
upper_tri <- lapply(nxn, function(df) upper.tri(df, diag = TRUE))
edge_nxn <- abind(lapply(nxn, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
## Split by 3 for int data
HAB_data <- as.data.frame(cbind(c(edge_nxn[,1], edge_nxn[,2], edge_nxn[,3]), c(rep(1, nrow(edge_nxn)), rep(2, nrow(edge_nxn)), rep(3, nrow(edge_nxn))))) # Three
colnames(HAB_data) <- c("SRI", "HAB")
HAB_data$During <- ifelse(HAB_data$HAB == 2, 1, 0)
HAB_data$After <- ifelse(HAB_data$HAB == 3, 1, 0)
HI <- abind(lapply(dist_HI, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Put data into a dataframe
df_list = data.frame(edge_weight = HAB_data[, 1],
HAB_During = HAB_data[, 3],
HAB_After = HAB_data[, 4],
HRO = unlist(lapply(kov, function (df) df[upper.tri(df, diag = TRUE)])),
sex_similarity = rep(ILV_mat[[1]][upper.tri(ILV_mat[[1]], diag = TRUE)], period),
age_difference = rep(ILV_mat[[2]][upper.tri(ILV_mat[[2]], diag = TRUE)], period),
HI_differences = c(HI[,c(1:period)]),
node_id_1 = unlist(one),
node_id_2 = unlist(two))
## HI Behavior Combined Three Year Period ##
fit_mcmc.2 <- MCMCglmm(edge_weight ~ HI_differences * HAB_During + HI_differences * HAB_After + HRO + age_difference + sex_similarity,
random=~mm(node_id_1 + node_id_2), data = df_list, nitt = 20000)
## HI Behavior Combined Three Year Period ##
fit_mcmc.2 <- MCMCglmm(edge_weight ~ HI_differences * HAB_During + HI_differences * HAB_After + HRO + age_difference + sex_similarity,
random=~mm(node_id_1 + node_id_2), data = df_list, nitt = 20000)
# Check for model convergence
model <- fit_mcmc.2
plot(model$Sol)
# Check for model convergence
model <- fit_mcmc.2
# Extract Posteriors
posterior <- model$Sol
# Summary of parameters
summary(fit_mcmc.2)
# Plot the posterior distribution
mcmc_intervals(posterior, pars = c("(Intercept)", "HI_differences", "HI_differences:HAB_During", "HI_differences:HAB_After", "HAB_During", "HAB_After", "age_difference", "sex_similarity", "HRO"))
library(ggplot2)
QAP <- function(Y, X) {
n <- dim(Y)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
obs <- .lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
null_dist <- sapply(1:1000, function(i) {
shuffle_rows <- sample(1:n)
X_ <- as.vector(X[shuffle_rows, shuffle_rows][upper.tri(X)])
.lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
})
list(estimate=obs, p_value=mean(abs(null_dist) > abs(obs)))
}
dyadic_regression <- function(Y, X) {
ls_dyadreg <- function(par, X, Y) {
beta <- par[1:2]
r <- par[3:(3 + n - 1)]
n <- dim(X)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
R <- matrix(rep(r, n), n)
R <- R + t(R)
R_ <- R[upper.tri(R)]
Y_pred <- beta[1] + beta[2] * X_ + R_
sum((Y_ - Y_pred)^2)
}
n <- dim(X)[1]
r <- runif(n, min=-1, max=1)
beta <- c(0, 0)
target <- function(par) ls_dyadreg(par, X, Y)
optim_obj <- optim(c(beta, r), target, method="BFGS", hessian=TRUE)
samples <- MASS::mvrnorm(1e5, optim_obj$par[1:3], solve(optim_obj$hessian[1:3, 1:3]))
summary_table <- t(apply(samples, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))))
rownames(summary_table) <- c("Intercept", "Slope", "Sigma")
summary_table <- signif(summary_table, 2)
# summary_table
summary_table <- cbind(summary_table, sapply(1:3, function(i) 2 * min(mean(samples[, i] < 0), mean(samples[, i] > 0))))
colnames(summary_table)[4] <- "P-value"
summary_table
}
mmlm <- function(Y, X) {
num_nodes <- dim(Y)[1]
node_ids_i <- matrix(rep(1:num_nodes, num_nodes), num_nodes, num_nodes)
node_ids_j <- t(node_ids_i)
df <- data.frame(
y=Y[upper.tri(Y)],
x=X[upper.tri(X)],
node_id_1=factor(node_ids_i[upper.tri(node_ids_i)], levels=1:num_nodes),
node_id_2=factor(node_ids_j[upper.tri(node_ids_j)], levels=1:num_nodes)
)
fit_mcmc <- MCMCglmm(y ~ x, random=~mm(node_id_1 + node_id_2), data=df, verbose=FALSE)
summary(fit_mcmc)$solutions
}
n <- 20
b <- 0.2
results <- data.frame(effect_size=numeric(), p_value=numeric(), method=numeric(), effect=numeric())
for (effect in c(TRUE, FALSE)) {
for (iter in 1:100) {
X_ <- matrix(runif(n^2), n, n)
X_ <- X_ * upper.tri(X_)
X_ <- X_ + t(X_)
Y_ <- matrix(runif(n^2), n, n)
Y_ <- Y_ * upper.tri(Y_)
Y_ <- Y_ + t(Y_)
R <- matrix(rep(runif(n), n), n, n)
S <- matrix(rep(runif(n), n), n, n)
X = R + t(R) + X_
Y = S + t(S) + Y_
if (effect) {
Y <- b * X + (1 - b) * Y
}
x <- X[upper.tri(X)]
y <- Y[upper.tri(Y)]
obj_lm <- summary(lm(y ~ x))
obj_perm <- QAP(Y, X)
obj_dyadreg <- mmlm(Y, X)
effect_lm <- obj_lm$coefficients[2, 1]
effect_perm <- obj_perm$estimate
# effect_dyadreg <- obj_dyadreg[2, 2]
effect_dyadreg <- obj_dyadreg[2, 1]
pval_lm <- obj_lm$coefficients[2, 4]
pval_perm <- obj_perm$p_value
pval_dyadreg <- obj_dyadreg[2, 5]
results[nrow(results) + 1, ] <- list(effect_lm, pval_lm, "OLS", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_perm, pval_perm, "QAP", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_dyadreg, pval_dyadreg, "OLS + Control", as.character(effect))
}
}
library(MCMCglmm) # MCMC models
library(ggplot2)
QAP <- function(Y, X) {
n <- dim(Y)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
obs <- .lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
null_dist <- sapply(1:1000, function(i) {
shuffle_rows <- sample(1:n)
X_ <- as.vector(X[shuffle_rows, shuffle_rows][upper.tri(X)])
.lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
})
list(estimate=obs, p_value=mean(abs(null_dist) > abs(obs)))
}
dyadic_regression <- function(Y, X) {
ls_dyadreg <- function(par, X, Y) {
beta <- par[1:2]
r <- par[3:(3 + n - 1)]
n <- dim(X)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
R <- matrix(rep(r, n), n)
R <- R + t(R)
R_ <- R[upper.tri(R)]
Y_pred <- beta[1] + beta[2] * X_ + R_
sum((Y_ - Y_pred)^2)
}
n <- dim(X)[1]
r <- runif(n, min=-1, max=1)
beta <- c(0, 0)
target <- function(par) ls_dyadreg(par, X, Y)
optim_obj <- optim(c(beta, r), target, method="BFGS", hessian=TRUE)
samples <- MASS::mvrnorm(1e5, optim_obj$par[1:3], solve(optim_obj$hessian[1:3, 1:3]))
summary_table <- t(apply(samples, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))))
rownames(summary_table) <- c("Intercept", "Slope", "Sigma")
summary_table <- signif(summary_table, 2)
# summary_table
summary_table <- cbind(summary_table, sapply(1:3, function(i) 2 * min(mean(samples[, i] < 0), mean(samples[, i] > 0))))
colnames(summary_table)[4] <- "P-value"
summary_table
}
mmlm <- function(Y, X) {
num_nodes <- dim(Y)[1]
node_ids_i <- matrix(rep(1:num_nodes, num_nodes), num_nodes, num_nodes)
node_ids_j <- t(node_ids_i)
df <- data.frame(
y=Y[upper.tri(Y)],
x=X[upper.tri(X)],
node_id_1=factor(node_ids_i[upper.tri(node_ids_i)], levels=1:num_nodes),
node_id_2=factor(node_ids_j[upper.tri(node_ids_j)], levels=1:num_nodes)
)
fit_mcmc <- MCMCglmm(y ~ x, random=~mm(node_id_1 + node_id_2), data=df, verbose=FALSE)
summary(fit_mcmc)$solutions
}
n <- 20
b <- 0.2
results <- data.frame(effect_size=numeric(), p_value=numeric(), method=numeric(), effect=numeric())
for (effect in c(TRUE, FALSE)) {
for (iter in 1:100) {
X_ <- matrix(runif(n^2), n, n)
X_ <- X_ * upper.tri(X_)
X_ <- X_ + t(X_)
Y_ <- matrix(runif(n^2), n, n)
Y_ <- Y_ * upper.tri(Y_)
Y_ <- Y_ + t(Y_)
R <- matrix(rep(runif(n), n), n, n)
S <- matrix(rep(runif(n), n), n, n)
X = R + t(R) + X_
Y = S + t(S) + Y_
if (effect) {
Y <- b * X + (1 - b) * Y
}
x <- X[upper.tri(X)]
y <- Y[upper.tri(Y)]
obj_lm <- summary(lm(y ~ x))
obj_perm <- QAP(Y, X)
obj_dyadreg <- mmlm(Y, X)
effect_lm <- obj_lm$coefficients[2, 1]
effect_perm <- obj_perm$estimate
# effect_dyadreg <- obj_dyadreg[2, 2]
effect_dyadreg <- obj_dyadreg[2, 1]
pval_lm <- obj_lm$coefficients[2, 4]
pval_perm <- obj_perm$p_value
pval_dyadreg <- obj_dyadreg[2, 5]
results[nrow(results) + 1, ] <- list(effect_lm, pval_lm, "OLS", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_perm, pval_perm, "QAP", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_dyadreg, pval_dyadreg, "OLS + Control", as.character(effect))
}
}
write.csv(results, "results/mmlm.test.csv")
results
