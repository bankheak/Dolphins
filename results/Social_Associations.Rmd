---
title: "Social Associations"
author: "Kyra Bankhead"
date: "2023-03-02"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


In this markdown I will:

1. Subset data to easily visualize analytical steps. Then, create an association matrix from the gambit of the group assumption using the simple-ratio index function.

2. Form repeated permutations from the true association matrix to create a null distribution of their coefficients of variations (CV). This will determine if the true CV ranges outside of what is expected by chance associations.


# PART 1: *Social Association Matrix*

## Association Matrices will have an annual temporal resolution.

## To ensure enough information is available to analyze individual association preference, I have made sure that each association matrix only indludes individuals that were seen at least 10 times in a year.

```{r subset, message=FALSE, warning=FALSE, include=FALSE}
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")

# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutations
require(assocInd)
require(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(parallel)
require(foreach)

# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)

# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))

# Make sure every ID has >10 obs
ID <- unique(orig_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
  obs_vect[i]<- sum(orig_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(orig_data, orig_data$Code %in% c(sub$ID))

```


## Create gambit of the group index

```{r gbi, echo=TRUE, message=FALSE, warning=FALSE}
# Group each individual by date and sighting
# Gambit of the group index
gbi <- readRDS("../data/gbi.RData")

```


## Calculate simple-ratio index with group by individual matrix above.

```{r matrix, echo=TRUE, message=FALSE, warning=FALSE}

# Create association matrix
nxn <- readRDS("../data/nxn.RData")

```

# PART 2: *Permutations*

## Create 1000 repeated matrices from true matrix to form null distribution

Permute association within samples to test long-term associations and control for gregariousness.

## True association index coefficient of variation
CV = (SD/mean)*100

```{r perm, message=FALSE, warning=FALSE, include=FALSE}

# Read in null cv values for one year
cv_null <- readRDS("../data/cv_years.RData")

# Calculate the CV of the observation association data
# CV = (SD/mean)*100
year <- 14
cv_obs=(sd(nxn[[year]]) / mean(nxn[[year]])) * 100  # Very high CV = unexpectedly 
# high or low association indices in the empirical distribution

```

## Form CV distribution

```{r dist, echo=TRUE, message=FALSE, warning=FALSE}

# Calculate 95% confidence interval, in a two-tailed test
cv_ci = quantile(cv_null[[year]], probs=c(0.025, 0.975), type=2)

# histogram of null CVs
hist(cv_null[[year]], 
     breaks=50, 
     col='grey70',
     main = 'Restrictive null model',
     xlab="Null CV SRI")
# empirical CV
abline(v= cv_obs, col="red")
# 2.5% CI
abline(v= cv_ci[1], col="blue")
# 97.5% CI
abline(v= cv_ci[2], col="blue")

```

We can reject the null hypothesis that individuals associate at random and conclude that there is evidence that associations are different from what we would expect by chance. Since the CV(TAI) is lower than the other CV, the associations are lower than expected.

