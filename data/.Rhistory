N_med <- rep(NA, numyears)
N_med[1] <- N0
for(t in 1:(numyears-1)){
N_med[t + 1] = N_med[t] + r * N_med[t] * (1 - N_med[t]/K) - F_m * N_med[t]
}
# Plot through time
plot(1:numyears, N_med, xlab = "Years", ylab = "Population Size")
F_m = 0.4
N_large <- rep(NA, numyears)
N_large[1] <- N0
for(t in 1:(numyears-1)){
N_large[t + 1] = N_large[t] + r * N_large[t] * (1 - N_large[t]/K) - F_m * N_large[t]
}
# Plot through time
plot(1:numyears, N_large, xlab = "Years", ylab = "Population Size")
N = 1:500
dndt = r * N * (1 - N/K)
plot(N, dndt, xlab = "Population Size", ylab = "dN/dt")
abline(0, 0.2)
abline(0, 0.3, lty=2)
abline(0, 0.4, lty=3)
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N_20, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 30
H = 30
N_30 <- rep(NA, numyears)
N_30[1] <- N0
for(t in 1:(numyears-1)){
N_30[t + 1] = N_30[t] + r * N_30[t] * (1 - N_30[t]/K) - H
}
# Plot through time
plot(1:numyears, N_30, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H= 40
H = 40
N_40 <- rep(NA, numyears)
N_40[1] <- N0
for(t in 1:(numyears-1)){
N_40[t + 1] = N_40[t] + r * N_40[t] * (1 - N_40[t]/K) - H
}
# Plot through time
plot(1:numyears, N_40, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
gc()
gc()
gc()
dist = matrix(c(0,5,1,8,5,0,6,3,1,6,0,9,8,3,9,0), nrow = 4, ncol = 4)
dist
1-(dist / max(dist))
gc()
citation()
library(ggOceanMaps)
register_google(key = "AIzaSyAgFfxIJmkL8LAWE7kHCqSqKBQDvqa9umI")
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
if(!require(ggplot2)){install.packages('ggplot2'); library(ggplot2)}
# Coordinates in decimal degrees
lat <- 48.749275
lon <- -122.490086
# Bounding box with a small buffer around the point
buffer <- 0.1
lims <- c(lon - buffer, lon + buffer, lat - buffer, lat + buffer)
BH_map <- basemap(limits = lims) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
if(!require(statnet)){install.packages('statnet'); library(statnet)}
if(!require(viridis)){install.packages('viridis'); library(viridis)}
if(!require(ggnetwork)){install.packages('ggnetwork'); library(ggnetwork)} # Get cluster coords
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # for drawing lines around social clusters
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
if(!require(intergraph)){install.packages('intergraph'); library(intergraph)} # To use igraph network in ggnet
if(!require(sna)){install.packages('sna'); library(sna)} # For network
if(!require(GGally)){install.packages('GGally'); library(GGally)} # For mapping networks in ggplot version = '2.2.1'
if(!require(ggplot2)){install.packages('ggplot2'); library(ggplot2)}
if(!require(sf)){install.packages('sf'); library(sf)} # Convert degrees to meters
if(!require(sp)){install.packages('sp'); library(sp)}
register_google(key = "AIzaSyAgFfxIJmkL8LAWE7kHCqSqKBQDvqa9umI")
if(!require(ggmap)){install.packages('ggmap'); library(ggmap)} # register API key version = '3.0.0'
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
register_google(key = "AIzaSyAgFfxIJmkL8LAWE7kHCqSqKBQDvqa9umI")
# Coordinates in decimal degrees
lat <- 48.749275
lon <- -122.490086
# Bounding box with a small buffer around the point
buffer <- 0.1
lims <- c(lon - buffer, lon + buffer, lat - buffer, lat + buffer)
BH_map <- basemap(limits = lims) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
# Coordinates in decimal degrees
lat <- 48.749275
lon <- -122.490086
# Buffer around point
buffer <- 0.1  # ~11 km
# Properly named limits
lims <- c(lon_min = lon - buffer,
lon_max = lon + buffer,
lat_min = lat - buffer,
lat_max = lat + buffer)
# Build map
BH_map <- basemap(limits = lims) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
# Coordinates in decimal degrees
lat <- 48.749275
lon <- -122.490086
# Buffer around point (adjust as needed)
buffer <- 0.1
# Properly named bounding box
lims <- c(lon_min = lon - buffer,
lon_max = lon + buffer,
lat_min = lat - buffer,
lat_max = lat + buffer)
# Make basemap
BH_map <- basemap(limits = lims) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
lims
# Properly named bounding box
lon_min <- lon - buffer
lon_max <- lon + buffer
lat_min <- lat - buffer
lat_max <- lat + buffer
# Make basemap
BH_map <- basemap(limits = c(lon_min, lon_max, lat_min, lat_max)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
?basemap
basemap(limits = c(lon_min, lon_max, lat_min, lat_max))
lon_min
lon_max
lat_min
lat_max
lat <- 48.749275
lon <- -122.490086
buffer <- 0.1
basemap(limits = c(
lon_min = lon - buffer,
lon_max = lon + buffer,
lat_min = lat - buffer,
lat_max = lat + buffer
))
lat <- 48.749275
lon <- -122.490086
buffer <- 0.1
BH_map <- basemap(lon = c(lon - buffer, lon + buffer),
lat = c(lat - buffer, lat + buffer)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map <- basemap(lon = c(lon - buffer, lon + buffer),
lat = c(lat - buffer, lat + buffer))
library(ggOceanMaps)
lat <- 48.749275
lon <- -122.490086
buffer <- 0.1
# Correctly named bounding box
lims <- c(lon_min = lon - buffer,
lon_max = lon + buffer,
lat_min = lat - buffer,
lat_max = lat + buffer)
BH_map <- basemap(limits = lims) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
florida_map <- basemap(limits = c(-87.6349, -79.9743, 24.3963, 31.0006)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
florida_map
lat <- 48.749275
lon <- -122.490086
buffer <- 0.1
lat <- 48.749275
lon <- -122.490086
BH_map <- basemap(limits = c(-123.5901, -121.5901, 49.749275, 47.749275)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
BH_map <- basemap(limits = c(-130.5901, -115.5901, 55.749275, 40.749275)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
BH_map
BH_map <- basemap(limits = c(-130.5901, -115.5901, 55.749275, 40.749275))
BH_map
lat <- 48.749275
lon <- -122.490086
buffer <- 10
basemap(limits = c(lat-buffer, lat+buffer, lon-buffer, lon+buffer))
lat <- 48.7492
lon <- -122.4900
buffer <- 10
basemap(limits = c(lat-buffer, lat+buffer, lon-buffer, lon+buffer))
lat-buffer
basemap(limits = c(lon-buffer, lon+buffer, lat-buffer, lat+buffer))
buffer <- 1
basemap(limits = c(lon-buffer, lon+buffer, lat-buffer, lat+buffer))
# Set working directory here
setwd("../data")
# Load all necessary packages
## Predictors
if(!require(igraph)){install.packages('igraph', version = '1.6.0'); library(igraph)} # graph_from_adjacency_matrix version = '1.6.0'
if(!require(kinship2)){install.packages('kinship2'); library(kinship2)} # genetic relatedness
if(!require(adehabitatHR)){install.packages('adehabitatHR'); library(adehabitatHR)} # Caluculate MCPs and Kernel density
## Network
if(!require(ggalt)){install.packages('ggalt'); library(ggalt)}
if(!require(network)){install.packages('network'); library(network)} # For assigning coordinates to nodes %v%
if(!require(ggmap)){install.packages('ggmap'); library(ggmap)} # register API key version = '3.0.0'
if(!require(graphlayouts)){install.packages('graphlayouts'); library(graphlayouts)}
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # mapping clusters geom_mark_hull
if(!require(ggraph)){install.packages('ggraph'); library(ggraph)} # For network plotting on map
if(!require(tnet)){install.packages('tnet'); library(tnet)} # For weights
if(!require(asnipe)){install.packages('asnipe'); library(asnipe)} # get_group_by_individual
if(!require(assortnet)){install.packages('assortnet'); library(assortnet)} # associative indices
## Mapping
if(!require(statnet)){install.packages('statnet'); library(statnet)}
if(!require(viridis)){install.packages('viridis'); library(viridis)}
if(!require(ggnetwork)){install.packages('ggnetwork'); library(ggnetwork)} # Get cluster coords
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # for drawing lines around social clusters
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
if(!require(intergraph)){install.packages('intergraph'); library(intergraph)} # To use igraph network in ggnet
if(!require(sna)){install.packages('sna'); library(sna)} # For network
if(!require(GGally)){install.packages('GGally'); library(GGally)} # For mapping networks in ggplot version = '2.2.1'
if(!require(ggplot2)){install.packages('ggplot2'); library(ggplot2)}
if(!require(sf)){install.packages('sf'); library(sf)} # Convert degrees to meters
if(!require(sp)){install.packages('sp'); library(sp)} # Convert degrees to meters
## Bayesian
if(!require(abind)){install.packages('abind'); library(abind)} # array
if(!require(brms)){install.packages('brms'); library(brms)} # For brm model
if(!require(coda)){install.packages('coda'); library(coda)}
if(!require(bayesplot)){install.packages('bayesplot'); library(bayesplot)} # plot parameters in mcmc_area
if(!require(magrittr)){install.packages('magrittr'); library(magrittr)} # For STAN
if(!require(dplyr)){install.packages('dplyr'); library(dplyr)}  # for organizing code
if(!require(rstan)){install.packages('rstan'); library(rstan)} # To make STAN run faster
if(!require(ggrepel)){install.packages('ggrepel'); library(ggrepel)} # for function labs
if(!require(RColorBrewer)){install.packages('RColorBrewer'); library(RColorBrewer)}
if(!require(gganimate)){install.packages('gganimate'); library(gganimate)}
if(!require(posterior)){install.packages('posterior'); library(posterior)} # Find the posterior sample names
if(!require(distributional)){install.packages('distributional'); library(distributional)}
if(!require(doParallel)){install.packages('doParallel'); library(doParallel)} # Faster computing
# Read in social association matrix and listed data
sim_HI <- readRDS("sim_HI.RData") # HI Sim Matrix
ILV_mat <-readRDS("ILV_mat.RData") # Age and Sex Matrices
kov <- readRDS("kov.RDS")  # Home range overlap
nxn <- readRDS("nxn.RData") # Association Matrix
nxn_counts <- readRDS("nxn_counts.RData")
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
# Format data
upper_tri <- lapply(nxn, function(df) upper.tri(df, diag = TRUE))
edge_nxn <- abind(lapply(nxn, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
## Split by 3 for int data
HAB_data <- as.data.frame(cbind(c(edge_nxn[,1], edge_nxn[,2], edge_nxn[,3]),
c(rep(1, nrow(edge_nxn)), rep(2, nrow(edge_nxn)),
rep(3, nrow(edge_nxn)))))
colnames(HAB_data) <- c("SRI", "HAB")
HAB_data$During <- ifelse(HAB_data$HAB == 2, 1, 0)
HAB_data$After <- ifelse(HAB_data$HAB == 3, 1, 0)
HI <- abind(lapply(sim_HI, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Put data into a dataframe
df_list = data.frame(edge_weight = HAB_data[, 1],
assoc_count = unlist(lapply(nxn_counts, function(x) x$assoc[upper.tri(x$assoc, diag = TRUE)])),
opportunity_count = unlist(lapply(nxn_counts, function(x) x$opps[upper.tri(x$opps, diag = TRUE)])),
HAB_During = HAB_data[, 3],
HAB_After = HAB_data[, 4],
Period = as.factor(HAB_data[, 2]),
HRO = unlist(lapply(kov, function (df) df[upper.tri(df, diag = TRUE)])),
sex_similarity = rep(ILV_mat[[1]][upper.tri(ILV_mat[[1]], diag = TRUE)], 3),
age_similarity = rep(ILV_mat[[2]][upper.tri(ILV_mat[[2]], diag = TRUE)], 3),
#GR = rep(gr[upper.tri(gr, diag = TRUE)], 3),
HI_similarity = c(HI[,c(1:3)]),
node_id_1 = unlist(one),
node_id_2 = unlist(two))
# Make sure that edge_weight is not whole numbers
df_list$edge_weight <- ifelse(df_list$edge_weight == 0, df_list$edge_weight + 0.00001,
ifelse(df_list$edge_weight == 1, df_list$edge_weight - 0.00001,
df_list$edge_weight))
# Standardize variables
numeric_vars <- c("HRO", "age_similarity", "HI_similarity")
df_list[numeric_vars] <- scale(df_list[numeric_vars])
# Decide on prior for SRI data
total_successes <- sum(df_list$assoc_count, na.rm = TRUE)
total_trials    <- sum(df_list$opportunity_count, na.rm = TRUE)
p_hat <- (total_successes + 0.5) / (total_trials + 1)   # add 0.5 / 1 for stability
logit_hat <- log(p_hat / (1 - p_hat))
full_priors <- c(
set_prior(sprintf("normal(%f, 1)", logit_hat), class = "Intercept"),
set_prior("normal(0, 0.5)", class = "b"),
set_prior("student_t(3, 0, 0.5)", class = "sd", lb = 0)  # shrink REs
)
# Help STAN run faster
rstan_options(auto_write = TRUE)
options(mc.cores = parallel::detectCores())
fit_sri.2 <- brm(
assoc_count | trials(opportunity_count) ~
HI_similarity * Period + HRO + age_similarity + sex_similarity +
(1 | mm(node_id_1, node_id_2)),
family = binomial(),
prior  = full_priors,
cores = 4,
control = list(adapt_delta = 0.999, max_treedepth = 15),
data = df_list
)
prior = c(
set_prior("normal(0, 1)", class = "Intercept"),
set_prior("exponential(1)", class = "phi")  # overdispersion parameter
)
beta_binomial2 <- custom_family( "beta_binomial2",
dpars = c("mu", "phi"),
links = c("logit", "log"),
lb = c(0, 0), ub = c(1, NA),
type = "int", vars = "vint1[n]" )
stan_funs <- "
real beta_binomial2_lpmf(int y, real mu, real phi, int T) {
return beta_binomial_lpmf(y | T, mu * phi, (1 - mu) * phi);
}
int beta_binomial2_rng(real mu, real phi, int T) {
return beta_binomial_rng(T, mu * phi, (1 - mu) * phi);
}
"
stanvars <- stanvar(scode = stan_funs, block = "functions")
fit_sri.bb <- brm(
assoc_count | vint(opportunity_count) ~ 1 + (1 | mm(node_id_1, node_id_2)),
family = beta_binomial2,
prior = priors,
stanvars = stanvars,
cores = 4,
data = df_list
)
fit_sri.bb <- brm(
assoc_count | vint(opportunity_count) ~ 1 + (1 | mm(node_id_1, node_id_2)),
family = beta_binomial2,
prior = prior,
stanvars = stanvars,
cores = 4,
data = df_list
)
# HMC diagnostics
check_hmc_diagnostics(fit_sri.bb$fit)
# Generate prior predictive samples
posterior_pred <- posterior_predict(fit_sri.bb)
# Check for model convergence
model <- fit_sri.bb
plot(model)
pp_check(model) # check to make sure they line up
summary(model)
# draws × N matrix of replicated counts
yrep <- posterior_predict(fit_sri.bb)   # might be large; consider using subset of draws
# Posterior predictive function for custom beta-binomial2
posterior_predict_beta_binomial2 <- function(i, draws, data = NULL) {
# observed trials for this observation
T <- data$vint1[i]
# number of posterior draws
ndraws <- nrow(draws$mu)
# extract parameters
mu  <- draws$mu[, i]
phi <- draws$phi[, i]
# simulate from the beta-binomial using Beta–Binomial mixture
# Step 1: sample per-draw probabilities from Beta(alpha, beta)
alpha <- mu * phi
beta  <- (1 - mu) * phi
p <- rbeta(ndraws, alpha, beta)
# Step 2: sample counts from Binomial(T, p)
rbinom(ndraws, size = T, prob = p)
}
yrep <- posterior_predict(fit_sri.bb, draws = 200)  # e.g., 200 draws
# Posterior predictive function for custom beta-binomial2
posterior_predict_beta_binomial2 <- function(i, draws, data = NULL, ...) {
# observed trials for this observation
T <- data$vint1[i]
# number of posterior draws
ndraws <- nrow(draws$mu)
# extract parameters
mu  <- draws$mu[, i]
phi <- draws$phi[, i]
# simulate from the beta-binomial using Beta–Binomial mixture
alpha <- mu * phi
beta  <- (1 - mu) * phi
p <- rbeta(ndraws, alpha, beta)
rbinom(ndraws, size = T, prob = p)
}
yrep <- posterior_predict(fit_sri.bb, draws = 200)  # e.g., 200 draws
# Posterior predictive function for custom beta-binomial2
posterior_predict_beta_binomial2 <- function(i, draws, data = NULL, ...) {
T <- data$vint1[i]
ndraws <- nrow(draws$mu)
mu  <- draws$mu[, i]
phi <- draws$phi[, i]
# Compute alpha/beta and enforce positivity
alpha <- pmax(mu * phi, 1e-8)
beta  <- pmax((1 - mu) * phi, 1e-8)
# Draw latent probabilities
p <- rbeta(ndraws, alpha, beta)
# Draw counts
rbinom(ndraws, size = T, prob = p)
}
draws_test <- as_draws_df(fit_sri.bb)
range(draws_test$mu)   # should be between 0 and 1
range(draws_test$phi)  # should be > 0
# Posterior predictive function for custom beta-binomial2
posterior_predict_beta_binomial2 <- function(i, draws, data = NULL, ...) {
T <- data$vint1[i]
mu  <- draws$mu[, i]    # matrix of mu draws for obs i
phi <- draws$phi        # vector of phi draws (same length)
alpha <- pmax(mu * phi, 1e-8)
beta  <- pmax((1 - mu) * phi, 1e-8)
p <- rbeta(length(mu), alpha, beta)
rbinom(length(mu), size = T, prob = p)
}
# Posterior predictive function for custom beta-binomial2
mu_draws <- fitted(fit_sri.bb, summary = FALSE)  # draws × obs
# Posterior predictive function for custom beta-binomial2
posterior_predict_beta_binomial2 <- function(i, draws, data = NULL, ...) {
T <- data$vint1[i]
mu  <- draws$mu[, i]    # matrix of mu draws for obs i
phi <- draws$phi        # vector of phi draws (same length)
alpha <- pmax(mu * phi, 1e-8)
beta  <- pmax((1 - mu) * phi, 1e-8)
p <- rbeta(length(mu), alpha, beta)
rbinom(length(mu), size = T, prob = p)
}
mu_draws <- fitted(fit_sri.bb, summary = FALSE)  # draws × obs
# Posterior predictive function for custom beta-binomial2
eta_draws <- posterior_linpred(fit_sri.bb, summary = FALSE)
# transform to mu (on probability scale)
mu_draws <- plogis(eta_draws)  # inv_logit
dim(mu_draws)  # e.g., 4000 × 20709
# extract phi (per-draw vector)
phi_draws <- exp(as_draws_df(fit_sri.bb)$phi)
# Now simulate from Beta-Binomial
make_yrep <- function(mu_mat, phi_vec, T) {
sapply(seq_along(T), function(i) {
alpha <- pmax(mu_mat[, i] * phi_vec, 1e-8)
beta  <- pmax((1 - mu_mat[, i]) * phi_vec, 1e-8)
p <- rbeta(length(phi_vec), alpha, beta)
rbinom(length(p), size = T[i], prob = p)
})
}
yrep <- make_yrep(mu_draws, phi_draws, df_list$opportunity_count)
# Use a subset of draws for clarity
set.seed(123)
yrep_sub <- yrep[sample(seq_len(nrow(yrep)), 200), ]
# Histogram overlay
ppc_hist(y = df_list$assoc_count, yrep = yrep_sub)
# Density overlay
ppc_dens_overlay(y = df_list$assoc_count, yrep = yrep_sub)
# Save data
saveRDS(fit_sri.bb, "fit_sri.bb.RData")
