# Set working directory here
setwd("../data")
setwd("C:/Users/Owner/Dolphins/code")
# Set working directory here
setwd("../data")
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
library(ncf) # For weights
library(vegan)
library(igraph) # graph_adj
require(asnipe) # mrqap.dsp
library(assortnet)
library(ggplot2)
library(doParallel)
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
kov <- readRDS("kov.RDS")  # Home range overlap
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# HI behaviors should be partitioned into 3 different types---------------------
#' B = Beg: F, G, H
#' P = Patrol: A, B, C
#' D = Depredation: D, E, P
# Change the code using ifelse statements
for (i in seq_along(aux)) {
aux[[i]]$DiffHI <- ifelse(aux[[i]]$ConfHI %in% c("F", "G", "H"), "Beg",
ifelse(aux[[i]]$ConfHI %in% c("A", "B", "C"), "Pat",
ifelse(aux[[i]]$ConfHI %in% c("P", "D", "E"), "Dep", "0")))
}
# Categorize DiffHI to IDs
rawHI_diff <- lapply(aux, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})
# Create a frequency count for each HI behavior
get_IDHI <- function(HI) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
HI_freq <- rawHI_diff[[i]]$Freq[rawHI_diff[[i]]$DiffHI == HI]
df$HI <- HI_freq[match(df$Code, rawHI_diff[[i]]$Code)]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
IDbehav_Beg <- get_IDHI("Beg")
IDbehav_Pat <- get_IDHI("Pat")
IDbehav_Dep <- get_IDHI("Dep")
# Clump all the HI behaviors together------------------------------------------
for (i in seq_along(aux)) {
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
# Get HI Freq
IDbehav_HI <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
prob_Beg <- Prop_HI(IDbehav_Beg)
prob_Pat <- Prop_HI(IDbehav_Pat)
prob_Dep <- Prop_HI(IDbehav_Dep)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
#dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dis_matr(prob_HI)
View(dist_HI)
dist_Beg <- dis_matr(prob_Beg)
dist_Pat <- dis_matr(prob_Pat)
dist_Dep <- dis_matr(prob_Dep)
# Dissimilarity Mantel Test
year <- 5
# Set a number of permutations
Nperm <- 1000
# Calculate QAP correlations for the association response matrix
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
mrqap
# Calculate QAP correlations for the association response matrix
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov[[year]] +
dist_Beg[[year]] + dist_Dep[[year]] + dist_Pat[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
mrqap
# Calculate QAP correlations for the association response matrix
mrqap <- mrqap.dsp(nxn[[year]] ~ kov[[year]] +
dist_Beg[[year]] + dist_Dep[[year]] + dist_Pat[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
# Match Code with matrix and vector
get_HI_vector <- function(prop_HI) {
HI_vector <- lapply(seq_along(nxn), function(i) {
matrix_index <- match(rownames(nxn[[i]]), prop_HI[[i]]$Code)
reordered_prob_HI <- prop_HI[[i]][matrix_index, ]
return(reordered_prob_HI)
})
return(HI_vector)
}
# Get each combined and seperate HI
HI_vector <- get_HI_vector(prob_HI)
Beg_vector <- get_HI_vector(prob_Beg)
Pat_vector <- get_HI_vector(prob_Pat)
Dep_vector <- get_HI_vector(prob_Dep)
# Look at HI assortivity coefficient over periods
calculate_assortment <- function(HI_vector) {
n.cores <- detectCores()
registerDoParallel(n.cores)
assort_HI <- NULL
# se <- NULL
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[i]][, "HIprop"], SE = FALSE)
assort_HI[i] <- coeff$r
# se[i] <- coeff$se
}
# End parallel processing
stopImplicitCluster()
assort_HI_df <- data.frame(HI_assort = unlist(assort_HI), Year = c(1:7))
return(assort_HI_df)
}
# Look at HI combined and separate
assort_HI <- calculate_assortment(HI_vector)
assort_Beg <- calculate_assortment(Beg_vector)
assort_Pat <- calculate_assortment(Pat_vector)
assort_Dep <- calculate_assortment(Dep_vector)
# Calculate QAP correlations for the association response matrix
mrqap <- mrqap.dsp(nxn[[year]] ~ kov[[year]] +
dist_Beg[[year]] + dist_Dep[[year]] + dist_Pat[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
View(HI_vector)
mrqap
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(rgdal) # Overlap
# Read in file
sample_data <- read.csv("sample_data.csv")
list_years <- readRDS("list_years.RData")
# Make a list of years
coord_data_list <- list_years
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
# Calculate kernel density estimates
kernel_obj <- kernelUD(coords_sp_utm, h = 1000)
# Add the kernel density estimates to the SpatialPointsDataFrame
coords_sp_utm$estUD <- kernel_obj$estUD
coords_sp_utm
})
# Create area of each polygon
year <- 5
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
dolph.kernel.poly
View(coord_data_list)
# Find HI events among individuals
ID_HI <- lapply(coord_data_list, function (df)
{subset(coord_data_list, subset=c(coord_data_list$ConfHI != 0))})
View(ID_HI)
# Find HI events among individuals
ID_HI <- lapply(coord_data_list, function (df)
{subset(df, subset=c(df$ConfHI != 0))})
View(ID_HI)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1500)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 500)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Find HI events among individuals
ID_HI <- lapply(coord_data_list, function(df) {
subset_df <- subset(df, subset = df$ConfHI != 0)
subset_df <- subset_df[, c('StartLat', 'StartLon', 'Code')]
return(subset_df)
})
View(ID_HI)
# Make sure there are at least 5 relocations
ID <- unique(ID_HI$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(ID_HI$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
# Make sure there are at least 5 relocations
ID <- lapply(ID_HI$Code, unique)
# Make sure there are at least 5 relocations
ID <- lapply(ID_HI, function(df) {
unique(df$Code)})
View(ID)
# Find HI events among individuals
ID_HI <- lapply(coord_data_list, function(df) {
subset_df <- subset(df, subset = df$ConfHI != 0)
subset_df <- subset_df[, c('StartLat', 'StartLon', 'Code')]
# Make sure there are at least 5 relocations
ID_df <- lapply(subset_df, function(df) {
unique(df$Code)})
obs_vect <- NULL
for (i in 1:length(ID_df)) {
obs_vect[i]<- sum(subset_df$Code == ID_df[i])
}
sub <- data.frame(ID_df, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 4))
subset_df <- subset(subset_df, subset_df$Code %in% sub$ID_df)
return(subset_df)
})
# Find HI events among individuals
ID_HI <- lapply(coord_data_list, function(df) {
subset_df <- subset(df, subset = df$ConfHI != 0)
subset_df <- subset_df[, c('StartLat', 'StartLon', 'Code')]
# Make sure there are at least 5 relocations
ID_df <- unique(subset_df$Code)
obs_vect <- numeric(length(ID_df))
for (i in seq_along(ID_df)) {
obs_vect[i] <- sum(subset_df$Code == ID_df[i])
}
sub <- data.frame(ID_df, obs_vect)
sub <- subset(sub, subset = obs_vect > 4)
subset_df <- subset_df[subset_df$Code %in% sub$ID_df, ]
return(subset_df)
})
View(ID_HI)
# Recalculate Coordinate data
ID_HI_sp <- lapply(ID_HI, function (df){
df_HI_sf <- st_as_sf(df, coords = c("x", "y"), crs = 4326)
df.sf <- st_transform(df_HI_sf, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
df$x <- st_coordinates(df.sf)[, 1]
df$y <- st_coordinates(df.sf)[, 2]
df <- df[!is.na(df$x) & !is.na(df$y),]
coordinates(df) <- c("x", "y")
proj4string(df) <- CRS( "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs" )
return(df)
})
# Recalculate Coordinate data
ID_HI_sp <- lapply(ID_HI, function(df) {
df_HI_sf <- st_as_sf(df, coords = c("StartLon", "StartLat"), crs = 4326)
df_sf <- st_transform(df_HI_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df <- df_sf
df <- df[complete.cases(df), ]
coordinates(df) <- c("StartLon", "StartLat")
proj4string(df) <- CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
return(df)
})
# Recalculate Coordinate data
ID_HI_sp <- lapply(ID_HI, function(df) {
df_HI_sf <- st_as_sf(df, coords = c("StartLon", "StartLat"), crs = 4326)
df_sf <- st_transform(df_HI_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_sf <- st_drop_geometry(df_sf[!is.na(df_sf$StartLon) & !is.na(df_sf$StartLat), ])
return(df_sf)
})
View(ID_HI)
# Recalculate Coordinate data
HI.sp <- lapply(ID_HI, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
# Calculate kernel density estimates
kernel_obj <- kernelUD(coords_sp_utm, h = 1000)
# Add the kernel density estimates to the SpatialPointsDataFrame
coords_sp_utm$estUD <- kernel_obj$estUD
coords_sp_utm
})
View(HI.sp)
# Kernel estimate
HI.kern <- lapply(HI.sp, function(sp_obj) {
kernelUD(sp_obj, h = 500)
})
HI.kernel.poly <- getverticeshr(HI.kern[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = "LSCV")
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000)
})
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
})
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 2000)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
print(dolph.kernel.poly)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 3000)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 3000, extent = 10000)
})
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000, extent = 10000)
})
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000, extent = 5000)
})
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000, extent = 2000)
})
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 1000, extent = 1000)
})
# Check data extent
dolph.sf <- do.call(rbind, dolph.sp)
ggplot() +
geom_sf(data = dolph.sf, aes(color = "Data Points")) +
theme_minimal() +
labs(title = "Distribution of Data Points")
# Visualize data extent
dolph.sf <- as.data.frame(do.call(rbind, dolph.sp))
View(dolph.sf)
ggplot() +
geom_sf(data = dolph.sf, aes(color = "Data Points")) +
theme_minimal() +
labs(title = "Distribution of Data Points")
ggplot() +
geom_sf(data = dolph.df, aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
# Visualize data extent
dolph.df <- as.data.frame(do.call(rbind, dolph.sp))
ggplot() +
geom_sf(data = dolph.df, aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
ggplot(dolph.df) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
# Visualize data extent
dolph.sf <- lapply(dolph.sp, function (df) {st_as_sf(df)})
View(dolph.sf)
ggplot(dolph.sf) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
ggplot(dolph.sf[[year]]) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
ggplot(dolph.sf[[3]]) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
# Calculate kernel values
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 10000)
})
dolph.kernel.poly <- getverticeshr(kernel[[year]], percent = 95)
