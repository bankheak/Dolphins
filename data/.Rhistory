var(pois)
nb <- rnbinom(x = 1000, size = 5, mu = 10)
nb <- rnbinom(1000, size = 5, mu = 10)
var(nb)
nb2 <- rnbinom(1000, size = 1, mu = 10)
var(nb2)
nb3 <- rnbinom(1000, size = 0.5, mu = 10)
var(nb3)
dpois(1000,10)
dpois(0,10)
dnbinom(0,5,10)
dnbinom(0,5,mu = 10)
dnbinom(0,1,mu = 10)
dnbinom(0,.5,mu = 10)
1-ppois(20, 10)
1-pnbinom(20, 5, mu = 10)
1-pnbinom(20, 1, mu = 10)
1-pnbinom(20, .5, mu = 10)
plot(dpois(pois, 10))
plot(dpois(1000, 10))
x_ppois <- seq(- 5, 1000, by = 1)
y_ppois <- ppois(x_ppois, lambda = 10)
plot(y_ppois)
x_ppois <- seq(- 5, 30, by = 1)
y_ppois <- ppois(x_ppois, lambda = 10)
plot(y_ppois)
x_nbinom <- seq(- 5, 30, by = 1)
y_nbinom <- pnbinom(x, size = 5, mu = 10)
y_nbinom <- pnbinom(x, size = 5, prob = 10)
y_nbinom <- pnbinom(x = 1000, size = 5, prob = 10)
x <- seq(0, 10, by = 1)
# Set working directory here
setwd("../data")
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
# Set working directory here
setwd("../data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutatioNP
require(assocInd)
require(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(foreach)
human_data <- read.csv("human_dolphin_data.csv")
human_data <- read.csv("human_dolphin_data.csv")
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- merge(firstgen_data, secondgen_data, human_data, all = T)
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- merge(orig_data, human_data, all = T)
View(orig_data)
orig_data <- rbind(firstgen_data, secondgen_data)
View(human_data)
?merge
orig_data <- merge(orig_data, human_data, by = human_data$SightingFID, all = T)
orig_data <- merge(orig_data, human_data, by = SightingFID, all = T)
orig_data <- merge(orig_data, human_data[,"X.Boats", "X.Lines", "X.CarbPots"], by = SightingFID, all = T)
orig_data <- merge(orig_data, human_data[,"X.Boats", "X.Lines", "X.CrabPots"], by = SightingFID, all = T)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
## Human
human_data_subset <- human_data[, c("SightingFID", "X.Boats", "X.Lines", "X.CrabPots")]
# Match ID to sex, age and human data
ILV <- read.csv("Individuals_Residency_Analysis.csv")
human_data <- read.csv("human_dolphin_data.csv")
## Sex
ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
## Human
human_data_subset <- human_data[, c("SightingFID", "X.Boats", "X.Lines", "X.CrabPots")]
orig_data <- merge(orig_data, human_data_subset, by = "SightingFID", all = TRUE)
View(orig_data)
orig_data <- ifelse(is.na(orig_data[,"X.Boats", "X.Lines", "X.CrabPots"]) == T, "0", ifelse(orig_data[,"X.Boats", "X.Lines", "X.CrabPots"] == -999, "0", orig_data[,"X.Boats", "X.Lines", "X.CrabPots"]))
is.na(orig_data[,"X.Boats", "X.Lines", "X.CrabPots"]) == T
is.na(orig_data[,"X.Boats", "X.Lines", "X.CrabPots"])
is.na(orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")])
orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")] == -999
sum(orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")] == -999)
sum(orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")] == "-999")
str(orig_data)
orig_data <- ifelse(is.na(orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")]), 0,
orig_data[,c("X.Boats", "X.Lines", "X.CrabPots")])
columns_to_replace <- c("X.Boats", "X.Lines", "X.CrabPots")
orig_data[, columns_to_replace][is.na(orig_data[, columns_to_replace])] <- 0
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Match ID to sex, age and human data
ILV <- read.csv("Individuals_Residency_Analysis.csv")
human_data <- read.csv("human_dolphin_data.csv")
## Sex
ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
## Human
human_data_subset <- human_data[, c("SightingFID", "X.Boats", "X.Lines", "X.CrabPots")]
orig_data <- merge(orig_data, human_data_subset, by = "SightingFID", all = TRUE)
columns_to_replace <- c("X.Boats", "X.Lines", "X.CrabPots")
orig_data[, columns_to_replace][is.na(orig_data[, columns_to_replace])] <- 0
View(orig_data)
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
sample_data <- subset(orig_data, subset=c(orig_data$StartLat != 999))
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_threeyears <- sub_locations(list_threeyears)
# Save list
saveRDS(list_threeyears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi <- create_gbi(list_years)
# Save gbi list
saveRDS(gbi, file="gbi.RData")
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn <- create_nxn(list_years, gbi)
# Save nxn lists
saveRDS(nxn, file="nxn.RData")
# Boat simularity matrix
Hactivity_list <- function(df, Hactivity) {
## Empty matrix to store boat similarity
num_ID <- length(unique(df$Code))
Hactivity_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of boat activity
for (i in 1:num_ID) {
for (j in 1:num_ID) {
Hactivity_matrix[i, j] <- abs(df$Hactivity[i] - df$Hactivity[j])
}
}
return(Hactivity_matrix)
}
boat_list <- Hactivity_list(df = list_years[[7]], Hactivity = X.Boats)
orig_data <- as.numeric(orig_data[, columns_to_replace])
orig_data[, columns_to_replace]
class(orig_data[, columns_to_replace])
str(orig_data[, columns_to_replace])
str(list_data[[7]][, columns_to_replace])
str(list_years[[7]][, columns_to_replace])
View(list_years)
sum(list_years[[7]][["X.Boats"]])
list_years[[7]][["X.Boats"]]
sum(list_years[[7]][["X.CrabPots"]])
class(list_years[[7]][["X.CrabPots"]])
negative_values <- list_years[[7]][["X.CrabPots"]][list_years[[7]][["X.CrabPots"]] < 0]
print(negative_values)
orig_data[, columns_to_replace][orig_data[, columns_to_replace] == -999] <- 0
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Match ID to sex, age and human data
ILV <- read.csv("Individuals_Residency_Analysis.csv")
human_data <- read.csv("human_dolphin_data.csv")
## Sex
ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
## Human
human_data_subset <- human_data[, c("SightingFID", "X.Boats", "X.Lines", "X.CrabPots")]
orig_data <- merge(orig_data, human_data_subset, by = "SightingFID", all = TRUE)
columns_to_replace <- c("X.Boats", "X.Lines", "X.CrabPots")
orig_data[, columns_to_replace][is.na(orig_data[, columns_to_replace])] <- 0
orig_data[, columns_to_replace][orig_data[, columns_to_replace] == -999] <- 0
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
sample_data <- subset(orig_data, subset=c(orig_data$StartLat != 999))
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_threeyears <- sub_locations(list_threeyears)
# Save list
saveRDS(list_threeyears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi <- create_gbi(list_years)
# Save gbi list
saveRDS(gbi, file="gbi.RData")
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn <- create_nxn(list_years, gbi)
# Save nxn lists
saveRDS(nxn, file="nxn.RData")
# Boat simularity matrices
Hactivity_list <- function(df, Hactivity) {
## Empty matrix to store boat similarity
num_ID <- length(unique(df$Code))
Hactivity_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of boat activity
for (i in 1:num_ID) {
for (j in 1:num_ID) {
Hactivity_matrix[i, j] <- abs(df$Hactivity[i] - df$Hactivity[j])
}
}
return(Hactivity_matrix)
}
boat_list <- Hactivity_list(df = list_years[[7]], Hactivity = X.Boats)
negative_values <- list_years[[7]][["X.CrabPots"]][list_years[[7]][["X.CrabPots"]] < 0]
print(negative_values)
View(list_years)
sum(list_years[[7]][["X.Boats"]])
# Boat simularity matrices
df <- list_years
Hactivity <- X.Boats
list_years[[7]]$X.Boats
Hactivity_list <- function(df, Hactivity) {
## Empty matrix to store boat similarity
num_ID <- length(unique(df$Code))
Hactivity_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of boat activity
for (i in 1:num_ID) {
for (j in 1:num_ID) {
Hactivity_matrix[i, j] <- abs(Hactivity[i] - Hactivity[j])
}
}
return(Hactivity_matrix)
}
boat_list <- Hactivity_list(df = list_years[[7]], Hactivity = list_years[[7]]$X.Boats)
View(boat_list)
## List years
seq_along(list_years)
## List years
seq_along(unique(list_years$Year))
unique(list_years$Year)
list_years[[]]$Year
## List years
unique(list_years[[seq_along(list_years)]]$Year)
list_years[[seq_along(list_years)]]$Year
list_years[[seq_along(list_years)]]
## List years
year_resolution <- lapply(list_years, function(df) unique(df$Year))
year_resolution
library(vegan)
# Run multiple cores for faster computing
require(doParallel)
library(sfsmisc, verbose=F)
## pred & density
boat_density <- sum(Hactivity_data$X.Boats)/length(Hactivity_data$X.Boats)
# Boat similarity matrices
Hactivity_data <- list_years[[7]]
## pred & density
boat_density <- sum(Hactivity_data$X.Boats)/length(Hactivity_data$X.Boats)
boat_density
sd(Hactivity_data$X.Boats)
boat_pred <- sd(Hactivity_data$X.Boats)/mean(Hactivity_data$X.Boats)
boat_pred
### Line
line_density <- sum(Hactivity_data$X.Lines)/length(Hactivity_data$X.Lines)
line_density
line_pred <- sd(Hactivity_data$X.Lines)/mean(Hactivity_data$X.Lines)
line_pred
### CrabPot
pot_density <- sum(Hactivity_data$X.CrabPots)/length(Hactivity_data$X.CrabPots)
pot_pred <- sd(Hactivity_data$X.CrabPots)/mean(Hactivity_data$X.CrabPots)
pot_density
pot_pred
# Read in different behavior's data frames
IDbehav_BG <- readRDS("IDbehav_BG.RData")
## Get estimate of population size within each HI group
IDbehav_BG <- readRDS("IDbehav_BG.RData")
# Set working directory here
setwd("../data")
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux <- aux_data(list_years)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav <- ID_forg(aux)
# HI behaviors should be partitioned into 3 different types---------------------
#' B = Beg: F, G
#' P = Scavenge and Depredation: B, C, D, E
#' D = Fixed Gear Interaction: P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(aux)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$HI <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
# Including zeros
IDbehav_BG <- get_IDHI("BG", IDbehav, rawHI_diff)
IDbehav_SD <- get_IDHI("SD", IDbehav, rawHI_diff)
IDbehav_FG <- get_IDHI("FG", IDbehav, rawHI_diff)
saveRDS(c(IDbehav_BG, IDbehav_SD, IDbehav_FG),
c("IDbehav_BG.RData","IDbehav_SD.RData", "IDbehav_FG.RData"))
saveRDS(IDbehav_BG, "IDbehav_BG.RData")
saveRDS(IDbehav_SD, "IDbehav_SD.RData")
saveRDS(IDbehav_FG, "IDbehav_FG.RData")
# Get unique behavior assignments
status <- function(IDbehav, HI, NonHI){
lapply(seq_along(IDbehav), function(i) {
IDbehav[[i]]$Stat <- ifelse(IDbehav[[i]]$HI > 0, HI, NonHI)
df <- IDbehav[[i]][, c('Code', 'Stat')]
df
})
}
## Match each individual with it's behavior
BG <- status(IDbehav_BG, "BG", "NBG")
SD <- status(IDbehav_SD, "SD", "NSD")
FG <- status(IDbehav_FG, "FG", "NFG")
# Replace individuals in the matrix with their assigned behavior
replace_ID_with_HI <- function(sri_matrix, ID_HI_df) {
# Create vector that matches IDs to their stat
id_to_stat <- setNames(ID_HI_df$Stat, ID_HI_df$Code)
# Replace each ID with stat in row and column names
row_names <- id_to_stat[rownames(sri_matrix)]
col_names <- id_to_stat[colnames(sri_matrix)]
# Create the replaced matrix
replaced_matrix <- sri_matrix
# Assign row and column names with behavioral states
dimnames(replaced_matrix) <- list(row_names, col_names)
return(replaced_matrix)
}
BG <- unique(unlist(sapply(IDbehav_BG, function(df) df$Code[df$HI != 0])))
SD <- unique(unlist(sapply(IDbehav_SD, function(df) df$Code[df$HI != 0])))
FG <- unique(unlist(sapply(IDbehav_FG, function(df) df$Code[df$HI != 0])))
Beg_effort <- as.data.frame(lapply(IDbehav_BG, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(Beg_effort) <- c(1:7)
Pat_effort <- as.data.frame(lapply(IDbehav_SD, function(df)
length(unique(df$Code[df$HI > 0]))))
BG_effort <- as.data.frame(lapply(IDbehav_BG, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(BG_effort) <- c(1:7)
SD_effort <- as.data.frame(lapply(IDbehav_SD, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(SD_effort) <- c(1:7)
FG_effort <- as.data.frame(lapply(IDbehav_FG, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(FG_effort) <- c(1:7)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
colnames(unique_ID_year) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(unique_ID_year, BG_effort, SD_effort, FG_effort)) # Days per year and pop size per year
rownames(pop_effort) <- c('Years', 'Beggars', 'Scavengers/Depredators', 'Fixed Gear Interactors')
pop_effort
year_list
# Estimate sampling effort and size for each triyear
## List years
year_list <- lapply(list_years, function(df) unique(df$Year))
year_list
# Estimate sampling effort and size for each triyear
## List years
year_list <- unlist(lapply(list_years, function(df) unique(df$Year)))
year_list
## Compare effort to population size
pop_effort <- as.data.frame(rbind(BG_effort, SD_effort, FG_effort)) # Days per year and pop size per year
rownames(pop_effort) <- c('Beggars', 'Scavengers/Depredators', 'Fixed Gear Interactors')
pop_effort
## Get estimate of sampling effort
effort <- as.data.frame(lapply(list_years, function(df) length(unique(df$Date))))
colnames(effort) <- c(1:7)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
colnames(unique_ID_year) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, BG_effort, SD_effort, FG_effort)) # Days per year and pop size per year
rownames(pop_effort) <- c('Number of Days Surveyed', 'Number of Individuals', 'Beggars', 'Scavengers/Depredators', 'Fixed Gear Interactors')
pop_effort
year_list
# Estimate sampling effort and size for each triyear
## List years
year_list <- lapply(list_years, function(df) unique(df$Year))
year_list
