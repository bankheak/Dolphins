sample_data <- read.csv("sample_data.csv")
# Get all unique Code values in the entire sample_data
all_codes <- unique(sample_data$Code)
# Create a function that counts the IDs in each element
count_instances <- function(df) {
code_counts <- table(df$Code)
code_counts <- code_counts[match(all_codes, names(code_counts))]
code_counts[is.na(code_counts)] <- 0
return(code_counts)
}
# -------------------- 22 sets of 1 year increments----------------------------
# Make a list of only 1 year per dataframe
list_years <- split(sample_data, sample_data$Year)
# Apply the count_instances function to each year
instances_per_year <- lapply(list_years, count_instances)
# Convert the list of counts to a data frame
p1y <- do.call(rbind, instances_per_year)
# Transforming into binary matrices
p1y <- as.matrix(p1y); p1y[which(p1y>=1)] = 1; p1y[which(p1y<1)] = 0
# -------------------- 11 sets of 2 year increments----------------------------
# Make a list of 2 years per dataframe
sample_data$TwoYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 2, by = 2), labels = FALSE)
list_twoyears <- split(sample_data, sample_data$TwoYearIncrement)
# Apply the count_instances function to each two years
instances_per_twoyear <- lapply(list_twoyears, count_instances)
# Convert the list of counts to a data frame
p2y <- do.call(rbind, instances_per_twoyear)
# Transforming into binary matrices
p2y <- as.matrix(p2y); p2y[which(p2y>=1)] = 1; p2y[which(p2y<1)] = 0
# -------------------- 7 sets of 3 year increments----------------------------
# Make a list of 3 years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Apply the count_instances function to each two years
instances_per_threeyear <- lapply(list_threeyears, count_instances)
# Convert the list of counts to a data frame
p3y <- do.call(rbind, instances_per_threeyear)
# Transforming into binary matrices
p3y <- as.matrix(p3y); p3y[which(p3y>=1)] = 1; p3y[which(p3y<1)] = 0
# -------------------- 6 sets of 4 year increments----------------------------
# Make a list of 4 years per dataframe
sample_data$FourYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 4, by = 4), labels = FALSE)
list_fouryears <- split(sample_data, sample_data$FourYearIncrement)
# Apply the count_instances function to each two years
instances_per_fouryear <- lapply(list_fouryears, count_instances)
# Convert the list of counts to a data frame
p4y <- do.call(rbind, instances_per_fouryear)
# Transforming into binary matrices
p4y <- as.matrix(p4y); p4y[which(p4y>=1)] = 1; p4y[which(p4y<1)] = 0
# -------------------- 4 sets of 5 year increments----------------------------
# Make a list of 5 years per dataframe
sample_data$FiveYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 5, by = 5), labels = FALSE)
list_fiveyears <- split(sample_data, sample_data$FiveYearIncrement)
# Apply the count_instances function to each two years
instances_per_fiveyear <- lapply(list_fiveyears, count_instances)
# Convert the list of counts to a data frame
p5y <- do.call(rbind, instances_per_fiveyear)
# Transforming into binary matrices
p5y <- as.matrix(p5y); p5y[which(p5y>=1)] = 1; p5y[which(p5y<1)] = 0
# -------------------- 4 sets of 6 year increments----------------------------
# Make a list of 6 years per dataframe
sample_data$SixYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 6, by = 6), labels = FALSE)
list_sixyears <- split(sample_data, sample_data$SixYearIncrement)
# Apply the count_instances function to each two years
instances_per_sixyear <- lapply(list_sixyears, count_instances)
# Convert the list of counts to a data frame
p6y <- do.call(rbind, instances_per_sixyear)
# Transforming into binary matrices
p6y <- as.matrix(p6y); p6y[which(p6y>=1)] = 1; p6y[which(p6y<1)] = 0
# -------------------- 3 sets of 7 year increments----------------------------
# Make a list of 7 years per dataframe
sample_data$SevenYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_sevenyears <- split(sample_data, sample_data$SevenYearIncrement)
# Apply the count_instances function to each two years
instances_per_sevenyear <- lapply(list_sevenyears, count_instances)
# Convert the list of counts to a data frame
p7y <- do.call(rbind, instances_per_sevenyear)
# Transforming into binary matrices
p7y <- as.matrix(p7y); p7y[which(p7y>=1)] = 1; p7y[which(p7y<1)] = 0
# -------------------- 3 sets of 8 year increments----------------------------
# Make a list of 8 years per dataframe
sample_data$EightYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 8, by = 8), labels = FALSE)
list_eightyears <- split(sample_data, sample_data$EightYearIncrement)
# Apply the count_instances function to each two years
instances_per_eightyear <- lapply(list_eightyears, count_instances)
# Convert the list of counts to a data frame
p8y <- do.call(rbind, instances_per_eightyear)
# Transforming into binary matrices
p8y <- as.matrix(p8y); p8y[which(p8y>=1)] = 1; p8y[which(p8y<1)] = 0
source("../code/functions.R") # WDI & WDI permutation
# Turn over results
t1 = turnover_w(data = p1y, iter = 1000, subseq=F, plot=FALSE)
t2 = turnover_w(data = p2y, iter = 1000, subseq=F, plot=FALSE)
t3 = turnover_w(data = p3y, iter = 1000, subseq=F, plot=FALSE)
t4 = turnover_w(data = p4y, iter = 1000, subseq=F, plot=FALSE)
t5 = turnover_w(data = p5y, iter = 1000, subseq=F, plot=FALSE)
t6 = turnover_w(data = p6y, iter = 1000, subseq=F, plot=FALSE)
t7 = turnover_w(data = p7y, iter = 1000, subseq=F, plot=FALSE)
t8 = turnover_w(data = p8y, iter = 1000, subseq=F, plot=FALSE)
all = rbind(t1, t2, t3, t4, t5, t6, t7, t8)
all = cbind(c(1, 2, 3, 4, 5, 6, 7, 8), all)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.34,0.43), xlim=c(1,8), cex.axis=0.8)
axis(1, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, cex.axis=0.7)
mtext(side = 1, "Length of periods (years)", line = 2, font = 1)
axis(3, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, labels=c(22, 11, 7, 6, 5, 4, 3, 3), cex.axis=0.7)
mtext(side = 3, "Number of periods", line = 2, font = 1)
# Print final results
all
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.30,0.7), xlim=c(1,8), cex.axis=0.8)
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.5,0.6), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.56,0.61), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.565,0.61), xlim=c(1,8), cex.axis=0.8)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(adehabitatHR) # Kernel density
library(rgdal) # Overlap
# Read in file
sample_data <- read.csv("sample_data.csv")
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'subYear', 'ConfHI')]) # Subset Date and Coordinates #
# Read in file
sample_data <- read.csv("sample_data.csv")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
gc()
gc()
citation("assortnet")
dbinom(0.1, 12)
dbinom(2,12,0.1)
132/2
.1^2
66*0.01*(0.9^10)
dbinom(0,12,0.1)
dbinom(12,12,0.9)
1/0
0.1^0
0.9^12
1-dbinom(1,12,0.1)
1-dbinom(1,12,0.1)-dbinom(0,12,0.1)
dpois(4,3)
ppois(2,3)
1-ppois(1,3)
1-ppois(0,3)
dpois(0,3)
1-dpois(0,3)
0.99 x 0.01/(0.01 x 0.99) + (0.02 x 0.99)
0.99 * 0.01/(0.01 * 0.99) + (0.02 * 0.99)
pois <- rpois(1000, 10)
var(pois)
nb <- rnbinom(x = 1000, size = 5, mu = 10)
nb <- rnbinom(1000, size = 5, mu = 10)
var(nb)
nb2 <- rnbinom(1000, size = 1, mu = 10)
var(nb2)
nb3 <- rnbinom(1000, size = 0.5, mu = 10)
var(nb3)
dpois(1000,10)
dpois(0,10)
dnbinom(0,5,10)
dnbinom(0,5,mu = 10)
dnbinom(0,1,mu = 10)
dnbinom(0,.5,mu = 10)
1-ppois(20, 10)
1-pnbinom(20, 5, mu = 10)
1-pnbinom(20, 1, mu = 10)
1-pnbinom(20, .5, mu = 10)
plot(dpois(pois, 10))
plot(dpois(1000, 10))
x_ppois <- seq(- 5, 1000, by = 1)
y_ppois <- ppois(x_ppois, lambda = 10)
plot(y_ppois)
x_ppois <- seq(- 5, 30, by = 1)
y_ppois <- ppois(x_ppois, lambda = 10)
plot(y_ppois)
x_nbinom <- seq(- 5, 30, by = 1)
y_nbinom <- pnbinom(x, size = 5, mu = 10)
y_nbinom <- pnbinom(x, size = 5, prob = 10)
y_nbinom <- pnbinom(x = 1000, size = 5, prob = 10)
x <- seq(0, 10, by = 1)
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
# Set working directory here
setwd("../data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
require(assocInd) # Could do permutatioNP
require(vegan)
require(doParallel) # Run multiple cores for faster computing
require(foreach)
library(reshape2) #
sample_data <- read.csv("sample_data.csv")
# Set working directory here
setwd("../data")
#################################################################################
# PART 1: Divide the data into different resolutions ----------------------------
## load all necessary packages
library(vegan)
# Run multiple cores for faster computing
library(doParallel)
library(sfsmisc, verbose=F)
# Make a list of years per dataframe
sample_data$SplitYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 1, by = 1), labels = FALSE)
list_splityears <- split(sample_data, sample_data$SplitYearIncrement)
# Subset only individuals that engage in HI
list_HI_splityears <- lapply(list_splityears, function(df) {subset(df, subset=c(df$ConfHI != "0"))})
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_splityears <- sub_locations(list_splityears)
list_HI_splityears <- sub_locations(list_HI_splityears)
# Save list
saveRDS(list_splityears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Estimate sampling effort and size for each period
## List years
year_list <- lapply(list_years, function(df) unique(df$Year))
## Get estimate of sampling effort
effort <- as.data.frame(lapply(list_years, function(df) length(unique(df$Date))))
colnames(effort) <- c(1,2)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
colnames(unique_ID_year) <- c(1,2)
## Get estimate of population size within each HI group
IDbehav_BG <- readRDS("IDbehav_BG.RData")
IDbehav_BG
###########################################################################
# PART 1: Create HI Disimilarity Matrix  ------------------------------------------------
## load all necessary packages
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(asnipe) # mrqap.dsp
library(assortnet) # associative indices
library(ggplot2) # Visualization
library(doParallel) # For faster coding
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux <- aux_data(list_years)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav <- ID_forg(aux)
# HI behaviors should be partitioned into 3 different types---------------------
#' BG = Beg: F, G
#' SD = Scavenge and Depredation: B, C, D, E
#' FG = Fixed Gear Interaction: P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(aux)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$HI <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
# Including zeros
IDbehav_BG <- get_IDHI("BG", IDbehav, rawHI_diff)
IDbehav_FG <- get_IDHI("FG", IDbehav, rawHI_diff)
IDbehav_SD <- get_IDHI("SD", IDbehav, rawHI_diff)
saveRDS(IDbehav_BG, "IDbehav_BG.RData")
saveRDS(IDbehav_FG, "IDbehav_FG.RData")
saveRDS(IDbehav_SD, "IDbehav_SD.RData")
## Get estimate of population size within each HI group
IDbehav_BG <- readRDS("IDbehav_BG.RData")
IDbehav_FG <- readRDS("IDbehav_FG.RData")
IDbehav_SD <- readRDS("IDbehav_SD.RData")
BG <- unique(unlist(sapply(IDbehav_BG, function(df) df$Code[df$HI != 0])))
FG <- unique(unlist(sapply(IDbehav_FG, function(df) df$Code[df$HI != 0])))
SD <- unique(unlist(sapply(IDbehav_SD, function(df) df$Code[df$HI != 0])))
BG_effort <- as.data.frame(lapply(IDbehav_BG, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(BG_effort) <- c(1,2)
FG_effort <- as.data.frame(lapply(IDbehav_FG, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(FG_effort) <- c(1,2)
SD_effort <- as.data.frame(lapply(IDbehav_SD, function(df)
length(unique(df$Code[df$HI > 0]))))
colnames(SD_effort) <- c(1,2)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, BG_effort, SD_effort, FG_effort)) # Days per year and pop size per year
rownames(pop_effort) <- c('Number of Days Surveyed', 'Number of Individuals', 'Beggars', 'Scavengers/Depredators', 'Fixed Gear Interactors')
# Get all unique Code values in the entire sample_data
all_codes <- unique(sample_data$Code)
pop_effort
# -------------------- 22 sets of 1 year increments----------------------------
# Make a list of only 1 year per dataframe
list_years <- split(sample_data, sample_data$Year)
# Apply the count_instances function to each year
instances_per_year <- lapply(list_years, count_instances)
# Convert the list of counts to a data frame
p1y <- do.call(rbind, instances_per_year)
# Create a function that counts the IDs in each element
count_instances <- function(df) {
code_counts <- table(df$Code)
code_counts <- code_counts[match(all_codes, names(code_counts))] # Add codes to table even if they aren't in that time period
code_counts[is.na(code_counts)] <- 0 # Replace NAs with 0
return(code_counts)
}
# -------------------- 22 sets of 1 year increments----------------------------
# Make a list of only 1 year per dataframe
list_years <- split(sample_data, sample_data$Year)
# Apply the count_instances function to each year
instances_per_year <- lapply(list_years, count_instances)
# Convert the list of counts to a data frame
p1y <- do.call(rbind, instances_per_year)
# Transforming into binary matrices
p1y <- as.matrix(p1y); p1y[which(p1y>=1)] = 1; p1y[which(p1y<1)] = 0
# -------------------- 7 sets of 3 year increments----------------------------
# Make a list of 3 years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Apply the count_instances function to each two years
instances_per_threeyear <- lapply(list_threeyears, count_instances)
# Convert the list of counts to a data frame
p3y <- do.call(rbind, instances_per_threeyear)
# Transforming into binary matrices
p3y <- as.matrix(p3y); p3y[which(p3y>=1)] = 1; p3y[which(p3y<1)] = 0
# -------------------- 3 sets of 7 year increments----------------------------
# Make a list of 7 years per dataframe
sample_data$SevenYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_sevenyears <- split(sample_data, sample_data$SevenYearIncrement)
# Apply the count_instances function to each two years
instances_per_sevenyear <- lapply(list_sevenyears, count_instances)
# Convert the list of counts to a data frame
p7y <- do.call(rbind, instances_per_sevenyear)
# Transforming into binary matrices
p7y <- as.matrix(p7y); p7y[which(p7y>=1)] = 1; p7y[which(p7y<1)] = 0
# -------------------- 2 sets of 11 year increments----------------------------
# Make a list of 8 years per dataframe
sample_data$ElevenYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 11, by = 11), labels = FALSE)
list_elevenyears <- split(sample_data, sample_data$ElevenYearIncrement)
# Apply the count_instances function to each two years
instances_per_elevenyear <- lapply(list_elevenyears, count_instances)
# Convert the list of counts to a data frame
p11y <- do.call(rbind, instances_per_elevenyear)
# Transforming into binary matrices
p11y <- as.matrix(p11y); p11y[which(p11y>=1)] = 1; p11y[which(p11y<1)] = 0
source("../code/functions.R") # turnover_w function
sample_data <- read.csv("sample_data.csv")
# Make a list of 7 years per dataframe
sample_data$SplitYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_splityears <- split(sample_data, sample_data$SplitYearIncrement)
c(1993:2014)
length(c(1993:2014))
c(1998:2011)
# Now split up data 7 years before and after HAB
sample_data <- sample_data[sample_data$Year >= 1998 & sample_data$Year <= 2011,]
unique(sample_data$Year)
write.csv(sample_data, "sample_data.csv")
# Make a list of 7 years per dataframe
sample_data$SplitYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_splityears <- split(sample_data, sample_data$SplitYearIncrement)
unique(list_splityears[[1]]$Year)
unique(list_splityears[[2]]$Year)
# Make a list of 7 years per dataframe
## Sort the data by Year
sample_data <- sample_data[order(sample_data$Year), ]
## Find the midpoint of the years
mid_year <- median(sample_data$Year)
## Create a column indicating the group (1 or 2) based on the midpoint
sample_data$Group <- ifelse(sample_data$Year <= median(sample_data$Year), 1, 2)
## Split the data into two groups based on the 'Group' column
list_splityears <- split(sample_data, sample_data$Group)
# Display the unique years in each group
unique(list_splityears[[1]]$Year)
unique(list_splityears[[2]]$Year)
# Make a list of 7 years per dataframe
## Sort the data by Year
sample_data <- sample_data[order(sample_data$Year), ]
## Create a column indicating the group (1 or 2) based on the midpoint
sample_data$Group <- ifelse(sample_data$Year < median(sample_data$Year), 1, 2)
## Split the data into two groups based on the 'Group' column
list_splityears <- split(sample_data, sample_data$Group)
# Display the unique years in each group
unique(list_splityears[[1]]$Year)
unique(list_splityears[[2]]$Year)
# Subset only individuals that engage in HI
list_HI_splityears <- lapply(list_splityears, function(df) {subset(df, subset=c(df$ConfHI != "0"))})
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_splityears <- sub_locations(list_splityears)
list_HI_splityears <- sub_locations(list_HI_splityears)
# Save list
saveRDS(list_splityears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi <- create_gbi(list_years)
# Save gbi list
saveRDS(gbi, file = "gbi.RData")
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn <- create_nxn(list_years, gbi)
# Save nxn lists
saveRDS(nxn, file = "nxn.RData")
View(gbi)
