F_m = 0.2
N_small <- rep(NA, numyears)
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_small) + 1)))
(max(N_small) + 50)
F_m = 0.2
N_small <- rep(NA, numyears)
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
N_small
N_small[1] <- N0
F_m = 0.2
N_small <- rep(NA, numyears)
N_small[1] <- N0
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_small) + 50)))
F_m = 0.3
N_med <- rep(NA, numyears)
N_med[1] <- N0
for(t in 1:(numyears-1)){
N_med[t + 1] = N_med[t] + r * N_med[t] * (1 - N_med[t]/K) - F_m * N_med[t]
}
# Plot through time
plot(1:numyears, N_med, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_med) + 1)))
F_m = 0.4
N_large <- rep(NA, numyears)
N_large[1] <- N0
for(t in 1:(numyears-1)){
N_large[t + 1] = N_large[t] + r * N_large[t] * (1 - N_large[t]/K) - F_m * N_large[t]
}
# Plot through time
plot(1:numyears, N_large, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_large) + 1)))
N_small[1] <- N0
F_m = 0.2
N_small <- rep(NA, numyears)
N_small[1] <- N0
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size")
F_m = 0.3
N_med <- rep(NA, numyears)
N_med[1] <- N0
for(t in 1:(numyears-1)){
N_med[t + 1] = N_med[t] + r * N_med[t] * (1 - N_med[t]/K) - F_m * N_med[t]
}
# Plot through time
plot(1:numyears, N_med, xlab = "Years", ylab = "Population Size")
F_m = 0.4
N_large <- rep(NA, numyears)
N_large[1] <- N0
for(t in 1:(numyears-1)){
N_large[t + 1] = N_large[t] + r * N_large[t] * (1 - N_large[t]/K) - F_m * N_large[t]
}
# Plot through time
plot(1:numyears, N_large, xlab = "Years", ylab = "Population Size")
N = 1:500
dndt = r * N * (1 - N/K)
plot(N, dndt, xlab = "Population Size", ylab = "dN/dt")
abline(0, 0.2)
abline(0, 0.3, lty=2)
abline(0, 0.4, lty=3)
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N_20, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 30
H = 30
N_30 <- rep(NA, numyears)
N_30[1] <- N0
for(t in 1:(numyears-1)){
N_30[t + 1] = N_30[t] + r * N_30[t] * (1 - N_30[t]/K) - H
}
# Plot through time
plot(1:numyears, N_30, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H= 40
H = 40
N_40 <- rep(NA, numyears)
N_40[1] <- N0
for(t in 1:(numyears-1)){
N_40[t + 1] = N_40[t] + r * N_40[t] * (1 - N_40[t]/K) - H
}
# Plot through time
plot(1:numyears, N_40, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
gc()
gc()
gc()
dist = matrix(c(0,5,1,8,5,0,6,3,1,6,0,9,8,3,9,0), nrow = 4, ncol = 4)
dist
1-(dist / max(dist))
gc()
citation()
# Set working directory here
setwd("../data")
# Load all necessary packages
## Predictors
if(!require(igraph)){install.packages('igraph', version = '1.6.0'); library(igraph)} # graph_from_adjacency_matrix version = '1.6.0'
if(!require(kinship2)){install.packages('kinship2'); library(kinship2)} # genetic relatedness
if(!require(adehabitatHR)){install.packages('adehabitatHR'); library(adehabitatHR)} # Caluculate MCPs and Kernel density
## Network
if(!require(ggalt)){install.packages('ggalt'); library(ggalt)}
if(!require(network)){install.packages('network'); library(network)} # For assigning coordinates to nodes %v%
if(!require(ggmap)){install.packages('ggmap'); library(ggmap)} # register API key version = '3.0.0'
if(!require(graphlayouts)){install.packages('graphlayouts'); library(graphlayouts)}
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # mapping clusters geom_mark_hull
if(!require(ggraph)){install.packages('ggraph'); library(ggraph)} # For network plotting on map
if(!require(tnet)){install.packages('tnet'); library(tnet)} # For weights
if(!require(asnipe)){install.packages('asnipe'); library(asnipe)} # get_group_by_individual
if(!require(assortnet)){install.packages('assortnet'); library(assortnet)} # associative indices
source("../code/functions.R") # nxn
## Mapping
if(!require(statnet)){install.packages('statnet'); library(statnet)}
if(!require(viridis)){install.packages('viridis'); library(viridis)}
if(!require(ggnetwork)){install.packages('ggnetwork'); library(ggnetwork)} # Get cluster coords
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # for drawing lines around social clusters
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
if(!require(intergraph)){install.packages('intergraph'); library(intergraph)} # To use igraph network in ggnet
if(!require(sna)){install.packages('sna'); library(sna)} # For network
if(!require(GGally)){install.packages('GGally'); library(GGally)} # For mapping networks in ggplot version = '2.2.1'
if(!require(ggplot2)){install.packages('ggplot2'); library(ggplot2)}
if(!require(sf)){install.packages('sf'); library(sf)} # Convert degrees to meters
if(!require(sp)){install.packages('sp'); library(sp)} # Convert degrees to meters
## Bayesian
if(!require(abind)){install.packages('abind'); library(abind)} # array
if(!require(brms)){install.packages('brms'); library(brms)} # For brm model
if(!require(coda)){install.packages('coda'); library(coda)}
if(!require(bayesplot)){install.packages('bayesplot'); library(bayesplot)} # plot parameters in mcmc_area
if(!require(magrittr)){install.packages('magrittr'); library(magrittr)} # For STAN
if(!require(dplyr)){install.packages('dplyr'); library(dplyr)}  # for organizing code
if(!require(rstan)){install.packages('rstan'); library(rstan)} # To make STAN run faster
if(!require(ggrepel)){install.packages('ggrepel'); library(ggrepel)} # for function labs
if(!require(RColorBrewer)){install.packages('RColorBrewer'); library(RColorBrewer)}
if(!require(gganimate)){install.packages('gganimate'); library(gganimate)}
if(!require(posterior)){install.packages('posterior'); library(posterior)} # Find the posterior sample names
if(!require(distributional)){install.packages('distributional'); library(distributional)}
if(!require(doParallel)){install.packages('doParallel'); library(doParallel)}
## Create social network
net <- lapply(nxn, function (df) {
as.network(df, matrix.type='adjacency',
directed = F,
ignore.eval=FALSE,
names.eval='weight')
})
## Read in nxn
nxn <- readRDS("nxn.RData")
## Create social network
net <- lapply(nxn, function (df) {
as.network(df, matrix.type='adjacency',
directed = F,
ignore.eval=FALSE,
names.eval='weight')
})
ig <- readRDS("ig.RData")
# Only show IDs of HI dolphins
HI_list <- readRDS("HI_list.RData")
HI_list <- HI_list[-4] # Get rid of natural foragers
HI_IDs <- unique(as.vector(unlist(HI_list))) # Put them all together
#----Modularity---
# igraph format with weight
el_years <- readRDS("el_years.RData")
dolphin_ig <- lapply(nxn, function (mtx)
graph_from_adjacency_matrix(as.matrix(mtx),
mode="undirected",
weighted=TRUE, diag=FALSE))
# Modularity by the WalkTrap algorithm
dolphin_walk <- lapply(dolphin_ig, function (df)
cluster_walktrap(df, weights = E(df)$weight,
steps = 4, merges = TRUE,
modularity = TRUE, membership = TRUE))
# Create an unweighted network
install.packages('igraph', version = '1.6.0')
newman <- readRDS("newman.RData")
# Generate a vector of colors based on the number of unique memberships
for (i in seq_along(dolp_ig)) {
# Generate a vector of colors based on the number of unique memberships
col <- viridis(min(max(newman[[i]]$membership), length(unique(newman[[i]]$membership))))
# Initialize the color attribute with NA
V(dolp_ig[[i]])$color <- NA
# Loop through each membership value and assign colors to corresponding vertices
for (j in 1:max(newman[[i]]$membership)){
V(dolp_ig[[i]])$color[newman[[i]]$membership == j] <- rep(col[j], sum(newman[[i]]$membership == j))
}
}
# Create an unweighted network
install.packages('igraph', version = '1.6.0')
library(igraph)
dolp_ig <- lapply(el_years, function (el) {
ig <- graph_from_edgelist(el[,1:2])
# Add the edge weights to this network
E(ig)$weight <- as.numeric(el[,3])
# Create undirected network
ig <- as.undirected(ig)
return(ig)
}
)
dolp_ig <- lapply(el_years, function (el) {
ig <- graph_from_edgelist(el[,1:2])
# Add the edge weights to this network
E(ig)$weight <- as.numeric(el[,3])
# Create undirected network
ig <- as_undirected(ig)
return(ig)
}
)
# Generate a vector of colors based on the number of unique memberships
for (i in seq_along(dolp_ig)) {
# Generate a vector of colors based on the number of unique memberships
col <- viridis(min(max(newman[[i]]$membership), length(unique(newman[[i]]$membership))))
# Initialize the color attribute with NA
V(dolp_ig[[i]])$color <- NA
# Loop through each membership value and assign colors to corresponding vertices
for (j in 1:max(newman[[i]]$membership)){
V(dolp_ig[[i]])$color[newman[[i]]$membership == j] <- rep(col[j], sum(newman[[i]]$membership == j))
}
}
if(!require(igraph)){install.packages('igraph', version = '1.6.0'); library(igraph)} # graph_from_adjacency_matrix version = '1.6.0'
if(!require(kinship2)){install.packages('kinship2'); library(kinship2)} # genetic relatedness
if(!require(adehabitatHR)){install.packages('adehabitatHR'); library(adehabitatHR)} # Caluculate MCPs and Kernel density
## Network
if(!require(ggalt)){install.packages('ggalt'); library(ggalt)}
if(!require(network)){install.packages('network'); library(network)} # For assigning coordinates to nodes %v%
if(!require(ggmap)){install.packages('ggmap'); library(ggmap)} # register API key version = '3.0.0'
if(!require(graphlayouts)){install.packages('graphlayouts'); library(graphlayouts)}
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # mapping clusters geom_mark_hull
if(!require(ggraph)){install.packages('ggraph'); library(ggraph)} # For network plotting on map
if(!require(tnet)){install.packages('tnet'); library(tnet)} # For weights
if(!require(asnipe)){install.packages('asnipe'); library(asnipe)} # get_group_by_individual
if(!require(assortnet)){install.packages('assortnet'); library(assortnet)} # associative indices
source("../code/functions.R") # nxn
## Mapping
if(!require(statnet)){install.packages('statnet'); library(statnet)}
if(!require(viridis)){install.packages('viridis'); library(viridis)}
if(!require(ggnetwork)){install.packages('ggnetwork'); library(ggnetwork)} # Get cluster coords
if(!require(ggforce)){install.packages('ggforce'); library(ggforce)} # for drawing lines around social clusters
if(!require(ggOceanMaps)){install.packages('ggOceanMaps'); library(ggOceanMaps)} # To map florida
if(!require(intergraph)){install.packages('intergraph'); library(intergraph)} # To use igraph network in ggnet
if(!require(sna)){install.packages('sna'); library(sna)} # For network
if(!require(GGally)){install.packages('GGally'); library(GGally)} # For mapping networks in ggplot version = '2.2.1'
if(!require(ggplot2)){install.packages('ggplot2'); library(ggplot2)}
if(!require(sf)){install.packages('sf'); library(sf)} # Convert degrees to meters
if(!require(sp)){install.packages('sp'); library(sp)} # Convert degrees to meters
## Bayesian
if(!require(abind)){install.packages('abind'); library(abind)} # array
if(!require(brms)){install.packages('brms'); library(brms)} # For brm model
if(!require(coda)){install.packages('coda'); library(coda)}
if(!require(bayesplot)){install.packages('bayesplot'); library(bayesplot)} # plot parameters in mcmc_area
if(!require(magrittr)){install.packages('magrittr'); library(magrittr)} # For STAN
if(!require(dplyr)){install.packages('dplyr'); library(dplyr)}  # for organizing code
if(!require(rstan)){install.packages('rstan'); library(rstan)} # To make STAN run faster
if(!require(ggrepel)){install.packages('ggrepel'); library(ggrepel)} # for function labs
if(!require(RColorBrewer)){install.packages('RColorBrewer'); library(RColorBrewer)}
if(!require(gganimate)){install.packages('gganimate'); library(gganimate)}
if(!require(posterior)){install.packages('posterior'); library(posterior)} # Find the posterior sample names
if(!require(distributional)){install.packages('distributional'); library(distributional)}
if(!require(doParallel)){install.packages('doParallel'); library(doParallel)}
saveRDS(dolp_ig, "dolp_ig.RData")
# Generate a vector of colors based on the number of unique memberships
for (i in seq_along(dolp_ig)) {
# Generate a vector of colors based on the number of unique memberships
col <- viridis(min(max(newman[[i]]$membership), length(unique(newman[[i]]$membership))))
# Initialize the color attribute with NA
V(dolp_ig[[i]])$color <- NA
# Loop through each membership value and assign colors to corresponding vertices
for (j in 1:max(newman[[i]]$membership)){
V(dolp_ig[[i]])$color[newman[[i]]$membership == j] <- rep(col[j], sum(newman[[i]]$membership == j))
}
}
# Read in homerange for individuals
kernel <- readRDS("kernel.RData")
# Create a for loop to store each period's average coordinates
# Extract 50% home range polygons
homerange50 <- lapply(kernel, function (kud) getverticeshr(kud, percent = 50))
# Initialize an empty list to store individual IDs and their centroids
centroid_list <- list()
# Loop through each individual's home range polygons
for(i in seq_along(homerange50)) {
# Initialize an empty data frame to store individual IDs and their centroids
centroids_df <- data.frame(ID = character(), Latitude = numeric(), Longitude = numeric())
for (id in names(kernel[[1]])) {
# Convert to sf object for further analysis or export
homerange50_sf <- st_as_sf(homerange50[[i]], coords = c("X", "Y"), crs = "+proj=longlat +ellps=WGS84 +datum=WGS84 +no_defs")
# Get the centroid of the geometry
centroid <- st_centroid(homerange50_sf$geometry[homerange50_sf$id == id])
# Add the individual ID and centroid coordinates to the data frame
centroids_df <- rbind(centroids_df, data.frame(ID = id, Latitude = centroid[[1]][2], Longitude = centroid[[1]][1]))
}
# Put this into a list
centroid_list[[i]] <- centroids_df
}
# Order data
order_rows <- rownames(nxn[[1]])
centroid_list <- lapply(centroid_list, function(df) {
df <- df[df$ID %in% order_rows, , drop = FALSE]  # Subsetting rows based on order_rows
df <- df[match(order_rows, df$ID), ]  # Reorder rows based on order_rows
return(df)  # Returning the modified data frame
})
# Define a function to convert UTM coordinates to longitude and latitude
utm_to_lonlat <- function(x, y, zone = 17, northern = TRUE) {
proj <- sprintf("+proj=utm +zone=%d %s", zone, ifelse(northern, "+north", "+south"))
xy <- data.frame(x = x, y = y)
xy <- SpatialPoints(xy, proj4string = CRS(proj))
xy <- spTransform(xy, CRS("+proj=longlat +datum=WGS84"))
return(coordinates(xy))
}
# Convert UTM coordinates to longitude and latitude
centroid_list <- lapply(centroid_list, function(df) {
lonlat <- utm_to_lonlat(df$Longitude, df$Latitude)
centroid_list <- data.frame(ID = df$ID,
X = lonlat[,1],
Y = lonlat[,2])
return(centroid_list)})
# ---Plot network---
# Set up the plotting area with 1 row and 2 columns for side-by-side plots
labeled_nodes <- list()
plot_list <- list()
register_google(key = "AIzaSyAgFfxIJmkL8LAWE7kHCqSqKBQDvqa9umI")
florida_map <- basemap(limits = c(-87.6349, -79.9743, 24.3963, 31.0006)) +
theme(axis.line = element_blank(),
panel.grid.major = element_blank(),
panel.grid.minor = element_blank(),
axis.text = element_blank(),
axis.ticks = element_blank(),
axis.title = element_blank())
# What is the cluster size for each period?
combined_cluster_data <- list()
for (i in 1:3) {
## Get the member data from newman and HI data from labeled_nodes
member_data <- data.frame(Cluster = newman[[i]]$membership, HI = labeled_nodes[[i]])
HI_clusters <- member_data[member_data$HI == T,]
HI_counts <- table(HI_clusters$Cluster)
membership_counts <- table(newman[[i]]$membership)
## Ensure both tables have the same keys
all_keys <- 1:(length(unique(newman[[1]]$membership)))
## Create a named vector for HI_counts with counts of zero for missing keys
HI_counts <- as.table(HI_counts)
HI_counts[as.character(all_keys)] <- HI_counts[as.character(all_keys)]
HI_counts[is.na(HI_counts)] <- 0
## Turn data into data frame
membership_counts_df <- data.frame(Cluster = as.numeric(names(membership_counts)), Total_Cluster_Count = as.numeric(membership_counts))
HI_counts_df <- data.frame(Cluster = as.numeric(names(HI_counts)), HI_Cluster_Count = as.numeric(HI_counts))
combined_cluster_data[[i]] <- merge(HI_counts_df, membership_counts_df, all = T)
combined_cluster_data[[i]]$perc_HI <- (combined_cluster_data[[i]]$HI_Cluster_Count/combined_cluster_data[[i]]$Total_Cluster_Count) *100
}
View(labeled_nodes)
# Create non-mapped graph
labeled_nodes <- list()
plot_list <- list()
for (i in 1:length(ig)) {  # Loop through periods
# Get nodes for each behavior
labeled_nodes[[i]] <- V(ig[[i]])$name %in% HI_IDs  # Fixed index here
# Make net_i
net_i <- ig[[i]]
net_i <- simplify(net_i)
# Set network and attributes
node_color <- V(dolp_ig[[i]])$color
# Map the node colors to their corresponding numbers
color_mapping <- setNames(seq_along(unique(node_color)), unique(node_color))
node_color_numbers <- color_mapping[node_color]
# Update vertex attributes
V(net_i)$grp <- as.character(node_color_numbers)
create_group_layout <- function(graph, node_groups, layout_func = layout_with_fr, group_spacing = 5) {
# Get unique groups
unique_groups <- unique(node_groups)
# Initialize an empty layout
layout <- matrix(0, nrow = vcount(graph), ncol = 2)
# Apply the layout function to each group individually
for (i in seq_along(unique_groups)) {
group_idx <- which(node_groups == unique_groups[i])
subgraph <- induced_subgraph(graph, group_idx)
sub_layout <- layout_func(subgraph)
# Normalize sub_layout to avoid large coordinate ranges
sub_layout <- sub_layout / max(abs(sub_layout))
# Offset sub_layout
offset_x <- (i - 1) * group_spacing
offset_y <- (i - 1) * group_spacing
layout[group_idx, ] <- sub_layout + c(offset_x, offset_y)
}
return(layout)
}
# Generate a layout based on the node groups
bb <- create_group_layout(net_i, V(net_i)$grp)
# Create the plot
plot <- ggraph(net_i, layout = "manual", x = bb[, 1], y = bb[, 2]) +
geom_edge_link0(aes(color = "grey80", width = E(net_i)$weight), alpha = 0.08) +
geom_node_point(aes(fill = grp, size = ifelse(labeled_nodes[[i]], 1.5, 0.5)), shape = 21) +
geom_text(aes(x = bb[, 1], y = bb[, 2], label = ifelse(labeled_nodes[[i]], V(net_i)$name, "")), color = "black", size = 2, vjust = 1.5) +
geom_mark_hull(aes(x = bb[, 1], y = bb[, 2], group = grp, fill = grp),
concavity = 4,
expand = unit(2, "mm"),
alpha = 0.25) +
scale_fill_brewer(palette = "Set1") +
scale_edge_color_manual(values = c(rgb(0, 0, 0, 0.1), rgb(0, 0, 0, 0.3))) +
theme_graph() +
theme(legend.position = "none")
plot_list[[i]] <- plot
}
# What is the cluster size for each period?
combined_cluster_data <- list()
for (i in 1:3) {
## Get the member data from newman and HI data from labeled_nodes
member_data <- data.frame(Cluster = newman[[i]]$membership, HI = labeled_nodes[[i]])
HI_clusters <- member_data[member_data$HI == T,]
HI_counts <- table(HI_clusters$Cluster)
membership_counts <- table(newman[[i]]$membership)
## Ensure both tables have the same keys
all_keys <- 1:(length(unique(newman[[1]]$membership)))
## Create a named vector for HI_counts with counts of zero for missing keys
HI_counts <- as.table(HI_counts)
HI_counts[as.character(all_keys)] <- HI_counts[as.character(all_keys)]
HI_counts[is.na(HI_counts)] <- 0
## Turn data into data frame
membership_counts_df <- data.frame(Cluster = as.numeric(names(membership_counts)), Total_Cluster_Count = as.numeric(membership_counts))
HI_counts_df <- data.frame(Cluster = as.numeric(names(HI_counts)), HI_Cluster_Count = as.numeric(HI_counts))
combined_cluster_data[[i]] <- merge(HI_counts_df, membership_counts_df, all = T)
combined_cluster_data[[i]]$perc_HI <- (combined_cluster_data[[i]]$HI_Cluster_Count/combined_cluster_data[[i]]$Total_Cluster_Count) *100
}
mean(combined_cluster_data[[1]]$Total_Cluster_Count[combined_cluster_data[[1]]$HI_Cluster_Count != 0])
# Save combined cluster list
saveRDS("combined_cluster_data.RData")
View(combined_cluster_data)
# Save combined cluster list
saveRDS(combined_cluster_data, "combined_cluster_data.RData")
mean(na.omit(combined_cluster_data[[2]]$Total_Cluster_Count[combined_cluster_data[[2]]$HI_Cluster_Count != 0]))
# Make the list into a dataframe
combined_cluster_df <- as.data.frame(combined_cluster_data)
View(combined_cluster_df)
# Make the list into a dataframe
periods <- c("Before", "During", "After")
combined_cluster_df <- do.call(rbind, Map(cbind, combined_cluster_data, Period = periods))
View(combined_cluster_df)
# Poisson test for HC clusters
model1 <- glm(Total_Cluster_Count ~ Period, family = poisson, data = combined_cluster_df)
summary(model1)
# Make During the intercept
combined_cluster_df$Period <- relevel(factor(combined_cluster_df$Period),
ref = "During")
# Poisson test for HC clusters
model1 <- glm(Total_Cluster_Count ~ Period, family = poisson, data = combined_cluster_df)
summary(model1)
# Poisson test for proportion of HC
model2 <- glm(cbind(HI_Cluster_Count, Total_Cluster_Count) ~ Period, family = binomial, data = combined_cluster_df)
summary(model2)
# Poisson test for proportion of HC
model2 <- glm(cbind(HI_Cluster_Count, Total_Cluster_Count - HI_Cluster_Count) ~ Period,
family = binomial,
data = combined_cluster_df)
summary(model2)
# Poisson test for HC cluster sizes
model2 <- glm(HI_Cluster_Count ~ Period, family = poisson, data = combined_cluster_df)
summary(model2)
# Poisson test for proportion of HC
model3 <- glm(cbind(HI_Cluster_Count, Total_Cluster_Count - HI_Cluster_Count) ~ Period,
family = binomial,
data = combined_cluster_df)
summary(model3)
# Poisson test for cluster sizes
model1 <- glm(Total_Cluster_Count ~ Period, family = poisson, data = combined_cluster_df)
summary(model1)
# Find the average cluster size for each HAB period
mean(combined_cluster_data[[1]]$Total_Cluster_Count[combined_cluster_data[[1]]$HI_Cluster_Count != 0])
mean(na.omit(combined_cluster_data[[2]]$Total_Cluster_Count[combined_cluster_data[[2]]$HI_Cluster_Count != 0]))
mean(combined_cluster_data[[3]]$Total_Cluster_Count[combined_cluster_data[[3]]$HI_Cluster_Count != 0])
# Find the average cluster size for each HAB period
mean(combined_cluster_data[[1]]$Total_Cluster_Count)
mean(combined_cluster_data[[2]]$Total_Cluster_Count)
## Find avg
mean(combined_cluster_df$Total_Cluster_Count[combined_cluster_df$Period == "Before"])
mean(combined_cluster_df$Total_Cluster_Count[combined_cluster_df$Period == "During"])
na.omit(combined_cluster_df)
combined_cluster_df <- na.omit(combined_cluster_df)
# Make During the intercept
combined_cluster_df$Period <- relevel(factor(combined_cluster_df$Period),
ref = "During")
# Poisson test for cluster sizes
model1 <- glm(Total_Cluster_Count ~ Period, family = poisson, data = combined_cluster_df)
summary(model1)
## Find avg
mean(combined_cluster_df$Total_Cluster_Count[combined_cluster_df$Period == "Before"])
mean(combined_cluster_df$Total_Cluster_Count[combined_cluster_df$Period == "During"])
mean(combined_cluster_df$Total_Cluster_Count[combined_cluster_df$Period == "After"])
# Poisson test for proportion of HC
model2 <- glm(cbind(HI_Cluster_Count, Total_Cluster_Count - HI_Cluster_Count) ~ Period,
family = binomial,
data = combined_cluster_df)
summary(model2)
mean(combined_cluster_data[[1]]$perc_HI[combined_cluster_data[[1]]$HI_Cluster_Count != 0])
mean(na.omit(combined_cluster_data[[2]]$perc_HI[combined_cluster_data[[1]]$HI_Cluster_Count != 0]))
mean(combined_cluster_data[[3]]$perc_HI[combined_cluster_data[[1]]$HI_Cluster_Count != 0])
