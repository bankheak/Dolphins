# Check for model convergence
model <- fit_mcmc.1
# Extract Posteriors
posterior <- model$Sol
# Summary of parameters
summary(fit_mcmc.1)
# Plot the posterior distribution
mcmc_intervals(posterior, pars = c("(Intercept)", "HI_differences", "HI_differences:HAB", "HAB", "age_difference", "sex_similarity", "HRO"))
## Three period data
dist_HI <- readRDS("dist_HI_int.RData") # HI Sim Matrix
## Three period data
dist_HI <- readRDS("../data/dist_HI_int.RData") # HI Sim Matrix
ILV_mat <-readRDS("../data/ILV_mat_int.RData") # Age and Sex Matrices
kov <- readRDS("../data/kov_int.RDS")  # Home range overlap
nxn <- readRDS("../data/nxn_int.RData") # Association Matrix
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
# Format data
period = 3
upper_tri <- lapply(nxn, function(df) upper.tri(df, diag = TRUE))
edge_nxn <- abind(lapply(nxn, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
## Split by 3 for int data
HAB_data <- as.data.frame(cbind(c(edge_nxn[,1], edge_nxn[,2], edge_nxn[,3]), c(rep(1, nrow(edge_nxn)), rep(2, nrow(edge_nxn)), rep(3, nrow(edge_nxn))))) # Three
colnames(HAB_data) <- c("SRI", "HAB")
HAB_data$During <- ifelse(HAB_data$HAB == 2, 1, 0)
HAB_data$After <- ifelse(HAB_data$HAB == 3, 1, 0)
HI <- abind(lapply(dist_HI, function(mat) mat[upper.tri(mat, diag = TRUE)]), along = 2)
one <- lapply(seq_along(node_ids_i), function(i) factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
two <- lapply(seq_along(node_ids_j), function(i) factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri[[i]]]]), levels = node_names[[i]]))
# Put data into a dataframe
df_list = data.frame(edge_weight = HAB_data[, 1],
HAB_During = HAB_data[, 3],
HAB_After = HAB_data[, 4],
HRO = unlist(lapply(kov, function (df) df[upper.tri(df, diag = TRUE)])),
sex_similarity = rep(ILV_mat[[1]][upper.tri(ILV_mat[[1]], diag = TRUE)], period),
age_difference = rep(ILV_mat[[2]][upper.tri(ILV_mat[[2]], diag = TRUE)], period),
HI_differences = c(HI[,c(1:period)]),
node_id_1 = unlist(one),
node_id_2 = unlist(two))
## HI Behavior Combined Three Year Period ##
fit_mcmc.2 <- MCMCglmm(edge_weight ~ HI_differences * HAB_During + HI_differences * HAB_After + HRO + age_difference + sex_similarity,
random=~mm(node_id_1 + node_id_2), data = df_list, nitt = 20000)
## HI Behavior Combined Three Year Period ##
fit_mcmc.2 <- MCMCglmm(edge_weight ~ HI_differences * HAB_During + HI_differences * HAB_After + HRO + age_difference + sex_similarity,
random=~mm(node_id_1 + node_id_2), data = df_list, nitt = 20000)
# Check for model convergence
model <- fit_mcmc.2
plot(model$Sol)
# Check for model convergence
model <- fit_mcmc.2
# Extract Posteriors
posterior <- model$Sol
# Summary of parameters
summary(fit_mcmc.2)
# Plot the posterior distribution
mcmc_intervals(posterior, pars = c("(Intercept)", "HI_differences", "HI_differences:HAB_During", "HI_differences:HAB_After", "HAB_During", "HAB_After", "age_difference", "sex_similarity", "HRO"))
library(ggplot2)
QAP <- function(Y, X) {
n <- dim(Y)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
obs <- .lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
null_dist <- sapply(1:1000, function(i) {
shuffle_rows <- sample(1:n)
X_ <- as.vector(X[shuffle_rows, shuffle_rows][upper.tri(X)])
.lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
})
list(estimate=obs, p_value=mean(abs(null_dist) > abs(obs)))
}
dyadic_regression <- function(Y, X) {
ls_dyadreg <- function(par, X, Y) {
beta <- par[1:2]
r <- par[3:(3 + n - 1)]
n <- dim(X)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
R <- matrix(rep(r, n), n)
R <- R + t(R)
R_ <- R[upper.tri(R)]
Y_pred <- beta[1] + beta[2] * X_ + R_
sum((Y_ - Y_pred)^2)
}
n <- dim(X)[1]
r <- runif(n, min=-1, max=1)
beta <- c(0, 0)
target <- function(par) ls_dyadreg(par, X, Y)
optim_obj <- optim(c(beta, r), target, method="BFGS", hessian=TRUE)
samples <- MASS::mvrnorm(1e5, optim_obj$par[1:3], solve(optim_obj$hessian[1:3, 1:3]))
summary_table <- t(apply(samples, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))))
rownames(summary_table) <- c("Intercept", "Slope", "Sigma")
summary_table <- signif(summary_table, 2)
# summary_table
summary_table <- cbind(summary_table, sapply(1:3, function(i) 2 * min(mean(samples[, i] < 0), mean(samples[, i] > 0))))
colnames(summary_table)[4] <- "P-value"
summary_table
}
mmlm <- function(Y, X) {
num_nodes <- dim(Y)[1]
node_ids_i <- matrix(rep(1:num_nodes, num_nodes), num_nodes, num_nodes)
node_ids_j <- t(node_ids_i)
df <- data.frame(
y=Y[upper.tri(Y)],
x=X[upper.tri(X)],
node_id_1=factor(node_ids_i[upper.tri(node_ids_i)], levels=1:num_nodes),
node_id_2=factor(node_ids_j[upper.tri(node_ids_j)], levels=1:num_nodes)
)
fit_mcmc <- MCMCglmm(y ~ x, random=~mm(node_id_1 + node_id_2), data=df, verbose=FALSE)
summary(fit_mcmc)$solutions
}
n <- 20
b <- 0.2
results <- data.frame(effect_size=numeric(), p_value=numeric(), method=numeric(), effect=numeric())
for (effect in c(TRUE, FALSE)) {
for (iter in 1:100) {
X_ <- matrix(runif(n^2), n, n)
X_ <- X_ * upper.tri(X_)
X_ <- X_ + t(X_)
Y_ <- matrix(runif(n^2), n, n)
Y_ <- Y_ * upper.tri(Y_)
Y_ <- Y_ + t(Y_)
R <- matrix(rep(runif(n), n), n, n)
S <- matrix(rep(runif(n), n), n, n)
X = R + t(R) + X_
Y = S + t(S) + Y_
if (effect) {
Y <- b * X + (1 - b) * Y
}
x <- X[upper.tri(X)]
y <- Y[upper.tri(Y)]
obj_lm <- summary(lm(y ~ x))
obj_perm <- QAP(Y, X)
obj_dyadreg <- mmlm(Y, X)
effect_lm <- obj_lm$coefficients[2, 1]
effect_perm <- obj_perm$estimate
# effect_dyadreg <- obj_dyadreg[2, 2]
effect_dyadreg <- obj_dyadreg[2, 1]
pval_lm <- obj_lm$coefficients[2, 4]
pval_perm <- obj_perm$p_value
pval_dyadreg <- obj_dyadreg[2, 5]
results[nrow(results) + 1, ] <- list(effect_lm, pval_lm, "OLS", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_perm, pval_perm, "QAP", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_dyadreg, pval_dyadreg, "OLS + Control", as.character(effect))
}
}
library(MCMCglmm) # MCMC models
library(ggplot2)
QAP <- function(Y, X) {
n <- dim(Y)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
obs <- .lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
null_dist <- sapply(1:1000, function(i) {
shuffle_rows <- sample(1:n)
X_ <- as.vector(X[shuffle_rows, shuffle_rows][upper.tri(X)])
.lm.fit(cbind(rep(1, length(X_)), X_), Y_)$coefficients[2]
})
list(estimate=obs, p_value=mean(abs(null_dist) > abs(obs)))
}
dyadic_regression <- function(Y, X) {
ls_dyadreg <- function(par, X, Y) {
beta <- par[1:2]
r <- par[3:(3 + n - 1)]
n <- dim(X)[1]
Y_ <- Y[upper.tri(Y)]
X_ <- X[upper.tri(X)]
R <- matrix(rep(r, n), n)
R <- R + t(R)
R_ <- R[upper.tri(R)]
Y_pred <- beta[1] + beta[2] * X_ + R_
sum((Y_ - Y_pred)^2)
}
n <- dim(X)[1]
r <- runif(n, min=-1, max=1)
beta <- c(0, 0)
target <- function(par) ls_dyadreg(par, X, Y)
optim_obj <- optim(c(beta, r), target, method="BFGS", hessian=TRUE)
samples <- MASS::mvrnorm(1e5, optim_obj$par[1:3], solve(optim_obj$hessian[1:3, 1:3]))
summary_table <- t(apply(samples, 2, function(x) quantile(x, probs=c(0.025, 0.5, 0.975))))
rownames(summary_table) <- c("Intercept", "Slope", "Sigma")
summary_table <- signif(summary_table, 2)
# summary_table
summary_table <- cbind(summary_table, sapply(1:3, function(i) 2 * min(mean(samples[, i] < 0), mean(samples[, i] > 0))))
colnames(summary_table)[4] <- "P-value"
summary_table
}
mmlm <- function(Y, X) {
num_nodes <- dim(Y)[1]
node_ids_i <- matrix(rep(1:num_nodes, num_nodes), num_nodes, num_nodes)
node_ids_j <- t(node_ids_i)
df <- data.frame(
y=Y[upper.tri(Y)],
x=X[upper.tri(X)],
node_id_1=factor(node_ids_i[upper.tri(node_ids_i)], levels=1:num_nodes),
node_id_2=factor(node_ids_j[upper.tri(node_ids_j)], levels=1:num_nodes)
)
fit_mcmc <- MCMCglmm(y ~ x, random=~mm(node_id_1 + node_id_2), data=df, verbose=FALSE)
summary(fit_mcmc)$solutions
}
n <- 20
b <- 0.2
results <- data.frame(effect_size=numeric(), p_value=numeric(), method=numeric(), effect=numeric())
for (effect in c(TRUE, FALSE)) {
for (iter in 1:100) {
X_ <- matrix(runif(n^2), n, n)
X_ <- X_ * upper.tri(X_)
X_ <- X_ + t(X_)
Y_ <- matrix(runif(n^2), n, n)
Y_ <- Y_ * upper.tri(Y_)
Y_ <- Y_ + t(Y_)
R <- matrix(rep(runif(n), n), n, n)
S <- matrix(rep(runif(n), n), n, n)
X = R + t(R) + X_
Y = S + t(S) + Y_
if (effect) {
Y <- b * X + (1 - b) * Y
}
x <- X[upper.tri(X)]
y <- Y[upper.tri(Y)]
obj_lm <- summary(lm(y ~ x))
obj_perm <- QAP(Y, X)
obj_dyadreg <- mmlm(Y, X)
effect_lm <- obj_lm$coefficients[2, 1]
effect_perm <- obj_perm$estimate
# effect_dyadreg <- obj_dyadreg[2, 2]
effect_dyadreg <- obj_dyadreg[2, 1]
pval_lm <- obj_lm$coefficients[2, 4]
pval_perm <- obj_perm$p_value
pval_dyadreg <- obj_dyadreg[2, 5]
results[nrow(results) + 1, ] <- list(effect_lm, pval_lm, "OLS", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_perm, pval_perm, "QAP", as.character(effect))
results[nrow(results) + 1, ] <- list(effect_dyadreg, pval_dyadreg, "OLS + Control", as.character(effect))
}
}
write.csv(results, "results/mmlm.test.csv")
results
gc()
list_years <- readRDS("list_years_int.RData") # (1995-2000)/(2001-2006)/(2007-2012)
# Set working directory here
setwd("../data")
list_years <- readRDS("list_years_int.RData") # (1995-2000)/(2001-2006)/(2007-2012)
nxn <- readRDS("nxn_int.RData") # association matrix of list_years_int
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux <- aux_data(list_years)
aux <- aux_data(list_years)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
View(list_years)
View(list_years[[1]])
View(aux)
View(aux[[1]])
IDbehav <- ID_forg(aux)
# Separate HI Behaviors
#' BG = Beg: F, G
#' SD = Scavenge and Depredation: A, B, C, D, E
#' FG = Fixed Gear Interaction: P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(aux)
View(aux)
View(aux[[1]])
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
View(rawHI_diff)
View(rawHI_diff[[1]])
# Clump all the HI behaviors together
clump_behav <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$ConfHI <- ifelse(aux_data[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux_data, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
return(rawHI)
}
rawHI <- clump_behav(aux)
# Get HI Freq
create_IDbehav_HI <- function(IDbehav_data, rawHI_data){
IDbehav_HI <- lapply(IDbehav_data, function(df) {
df$HI <- rawHI_data[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
return(IDbehav_HI)
}
IDbehav_HI <- create_IDbehav_HI(IDbehav, rawHI)
# Get HI Freq
create_IDbehav_HI <- function(IDbehav_data, rawHI_data){
IDbehav_HI <- lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
df$HI <- rawHI_data[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
return(IDbehav_HI)
}
IDbehav_HI <- create_IDbehav_HI(IDbehav, rawHI)
View(IDbehav_HI)
View(IDbehav_HI[[1]])
View(IDbehav)
View(aux)
View(aux[[1]])
df = aux
df = aux[[1]]
c(df$Code, length(unique(df$Code)))
df_test <- data.frame(Code = df$Code, Sightings = length(unique(df$Code)))
df_test <- data.frame(
Code = unique(df$Code),
Sightings = tapply(df$Code, df$Code, length)
)
View(df_test)
# Categorize ID to Foraging
ID_sight <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- data.frame(
Code = unique(df$Code),
Sightings = tapply(df$Code, df$Code, length)
)
df
})
return(IDbehav)
}
IDbehav <- ID_sight(aux)
View(IDbehav)
View(IDbehav[[1]])
View(IDbehav[[2]])
# Separate HI Behaviors
#' BG = Beg: F, G
#' SD = Scavenge and Depredation: A, B, C, D, E
#' FG = Fixed Gear Interaction: P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G"), "BG",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C", "D", "E"), "SD",
ifelse(aux_data[[i]]$ConfHI %in% c("P"), "FG", "None")))
}
return(aux_data)  # Return the modified list of data frames
}
aux <- subset_HI(aux)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff <- diff_raw(aux)
# Clump all the HI behaviors together
clump_behav <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$ConfHI <- ifelse(aux_data[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux_data, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
return(rawHI)
}
rawHI <- clump_behav(aux)
# Get HI Freq
create_IDbehav_HI <- function(IDbehav_data, rawHI_data){
IDbehav_HI <- lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
df$HI <- rawHI_data[[i]]$ConfHI
colnames(df) <- c("Code", "Sightings", "HI")
df
})
return(IDbehav_HI)
}
IDbehav_HI <- create_IDbehav_HI(IDbehav, rawHI)
View(IDbehav_HI)
View(IDbehav_HI[[1]])
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
# Two period data
prob_HI <- Prop_HI(IDbehav_HI)
# Proportion of time Sightings spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Sightings)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
# Two period data
prob_HI <- Prop_HI(IDbehav_HI)
View(prob_HI)
View(prob_HI[[1]])
# Two period data
dist_HI <- dis_matr(prob_HI, nxn)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(Prop_HI, nxn) {
# Order data
order_rows <- rownames(nxn[[1]])
# Apply the order to each matrix in the list
Prop_HI <- lapply(Prop_HI, function (df) {
df$Code <- df$Code[match(order_rows, df$Code)]
return(df)})
# Create matrix
dissimilarity_HI <- list()
for (i in seq_along(Prop_HI)) {
fake_HIprop <- Prop_HI[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
}
return(dissimilarity_HI)
}
# Two period data
dist_HI <- dis_matr(prob_HI, nxn)
saveRDS(dist_HI, "dist_HI.RData")
# normalize distances to [0,1]
normdistance <- dist_HI / max(dist_HI)
# normalize distances to [0,1]
normdistance <- lapply(dist_HI, function (df) df / max(df))
View(normdistance)
# Transform to similarity
# Method 1: Normalize Euclidian distances to make it range [0,1] and then simply apply 1-normalized distance
sim_HI <- lapply(dist_HI, function (df) {
# normalize distances to [0,1]
normdistance <- df / max(df)
# similarity [0,1] = 1 - distance[0,1]
similarity1 = 1-normdistance
})
View(sim_HI)
sim_HI[[3]]
# Transform to similarity
# Method 1: Normalize Euclidian distances to make it range [0,1] and then simply apply 1-normalized distance
sim_HI <- lapply(dist_HI, function (df) {
# normalize distances to [0,1]
normdistance <- df / max(df)
# similarity [0,1] = 1 - distance[0,1]
similarity1 = 1-normdistance
return(similarity1)
})
distance <- dist_HI[[1]]
similarity1 <- sim_HI[[1]]
plot(distance)
plot(similarity1)
# Method 2: using Euler's number (base of the natural log) to rescale and convert distances to [0,1] similarity
sim_HI2 <- lapply(dist_HI, function (df) {
similarity2 = 1/exp(df)
return(similarity2)
})
similarity2 <- sim_HI2[[1]]
plot(distance)
plot(similarity2)
saveRDS(sim_HI, "sim_HI.RData")
