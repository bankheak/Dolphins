ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
sample_data <- subset(orig_data, subset=c(orig_data$StartLat != 999))
# Get rid of data with no sex or age data
sample_sexage_data <- sample_data[!is.na(sample_data$Sex) & !is.na(sample_data$Age),]
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Make a list of three years per dataframe for sex and age data
sample_sexage_data$ThreeYearIncrement <- cut(sample_sexage_data$Year, breaks = seq(min(sample_sexage_data$Year), max(sample_sexage_data$Year) + 3, by = 3), labels = FALSE)
list_sexage_threeyears <- split(sample_sexage_data, sample_sexage_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_threeyears <- sub_locations(list_threeyears)
list_sexage_threeyears <- sub_locations(list_sexage_threeyears)
list_years <- readRDS("list_years.RData")
View(list_years)
# Subset only individuals that engage in HI
list_HI_years <- lapply(list_years, function(df) {subset(df, subset=c(df$ConfHI != "0"))})
View(list_HI_years)
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi_HI <- create_gbi(list_HI_years)
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn_HI <- create_nxn(list_HI_years, gbi_HI)
View(nxn_HI)
saveRDS(nxn_HI, file="nxn_HI.RData")
saveRDS(list_HI_years, file="list_HI_years.RData")
# Make a function that calculates all of the following code with and without sex and age
create_coord_data <- function(list_years) {
# Make a list of years
coord_data_list <- list_years
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
})
return(dolph.sp)
}
dolph.sp_HI <- create_coord_data(list_HI_years)
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(rgdal) # Overla
# Make a function that calculates all of the following code with and without sex and age
create_coord_data <- function(list_years) {
# Make a list of years
coord_data_list <- list_years
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
})
return(dolph.sp)
}
dolph.sp_HI <- create_coord_data(list_HI_years)
# Visualize data extent
vis.sf <- function(dolph.sp) {
dolph.sf <- lapply(dolph.sp, function (df) {st_as_sf(df)})
ggplot(dolph.sf[[3]]) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
return(dolph.sf)
}
dolph.sf_HI <- vis.sf(dolph.sp_HI)
# Calculate kernel values
create_kernel <- function(dolph.sp) {
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 10000)
})
return(kernel)
}
kernel_HI <- create_kernel(dolph.sp_HI)
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Subset only individuals that engage in HI
list_HI_years <- lapply(list_threeyears, function(df) {subset(df, subset=c(df$ConfHI != "0"))})
View(list_HI_years)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
# Subset only individuals that engage in HI
list_HI_threeyears <- lapply(list_threeyears, function(df) {subset(df, subset=c(df$ConfHI != "0"))})
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_HI_threeyears <- sub_locations(list_HI_threeyears)
View(list_HI_threeyears)
View(list_HI_years)
saveRDS(list_HI_threeyears, file="list_HI_years.RData")
list_HI_years <- readRDS("list_HI_years.RData")
# Make a function that calculates all of the following code with and without sex and age
create_coord_data <- function(list_years) {
# Make a list of years
coord_data_list <- list_years
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
})
return(dolph.sp)
}
dolph.sp_HI <- create_coord_data(list_HI_years)
# Calculate kernel values
create_kernel <- function(dolph.sp) {
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 10000)
})
return(kernel)
}
kernel_HI <- create_kernel(dolph.sp_HI)
# Calculate kernel overlap values
create_kov <- function(kernel) {
kov <- lapply(kernel, function(kern) {
kerneloverlaphr(kern, method = "HR", lev = 95)
})
return(kov)
}
kov_HI <- create_kov(kernel_HI)
kernel_HI
View(kernel_HI)
kov_HI <- kerneloverlaphr(kernel_HI[[year]], method = "HR", lev = 95)
# Create area of each polygon
year <- 5
kov_HI <- kerneloverlaphr(kernel_HI[[year]], method = "HR", lev = 95)
View(kov_HI)
# Read file in to retain only HI IDs
kov_HI <- readRDS("kov_HI.RDS")
saveRDS(kov_HI, "kov_HI.RDS")
# Read file in to retain only HI IDs
kov_HI <- readRDS("kov_HI.RDS")
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi_HI <- create_gbi(list_HI_years)
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn_HI <- create_nxn(list_HI_years, gbi_HI)
saveRDS(nxn_HI, file="nxn_HI.RData")
nxn_HI <- readRDS("kov_HI.RDS")
# Read file in to retain only HI IDs
kov_HI <- readRDS("kov_HI.RDS")
nxn_HI <- readRDS("nxn_HI.RDS")
nxn_HI <- create_nxn(list_HI_years, gbi_HI)
saveRDS(nxn_HI, file="nxn_HI.RData")
nxn_HI <- readRDS("nxn_HI.RDS")
nxn_HI <- readRDS("nxn_HI.RData")
View(nxn_HI)
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux_HI <- aux_data(list_HI_years)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav_HI <- ID_forg(aux_HI)
# Clump all the HI behaviors together------------------------------------------
clump_behav <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$ConfHI <- ifelse(aux_data[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux_data, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
return(rawHI)
}
rawHI_HI <- clump_behav(aux_HI)
# Get HI Freq
create_IDbehav_HI <- function(IDbehav_data, rawHI_data){
IDbehav_HI <- lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
df$HI <- rawHI_data[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
return(IDbehav_HI)
}
IDbehav_HI_HI <- create_IDbehav_HI(IDbehav_HI, rawHI_HI)
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prop_HI_HI <- Prop_HI(IDbehav_HI_HI)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(Prop_HI) {
dissimilarity_HI <- list()
for (i in seq_along(Prop_HI)) {
fake_HIprop <- Prop_HI[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
}
return(dissimilarity_HI)
}
dist_HI_HI <- dis_matr(prop_HI_HI)
# Set a number of permutations and year
year <- 5
Nperm <- 1000
## With only HI individuals included
mrqap_HIonly <- mrqap.dsp(nxn_HI[[year]] ~ kov_HI + dist_HI_HI[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
mrqap_HIonly
dist_HI_HI[[year]]
# HI behaviors should be partitioned into 3 different types---------------------
#' B = Beg: F, G, H
#' P = Patrol: A, B, C
#' D = Depredation: D, E, P
# Change the code using ifelse statements
for (i in seq_along(aux)) {
aux[[i]]$DiffHI <- ifelse(aux[[i]]$ConfHI %in% c("F", "G", "H"), "Beg",
ifelse(aux[[i]]$ConfHI %in% c("A", "B", "C"), "Pat",
ifelse(aux[[i]]$ConfHI %in% c("P", "D", "E"), "Dep", "0")))
}
# HI behaviors should be partitioned into 3 different types---------------------
#' B = Beg: F, G, H
#' P = Patrol: A, B, C
#' D = Depredation: D, E, P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G", "H"), "Beg",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C"), "Pat",
ifelse(aux_data[[i]]$ConfHI %in% c("P", "D", "E"), "Dep", "0")))
}}
aux_HI <- subset_HI(aux_HI)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff_HI <- diff_raw(aux_HI)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff_HI <- diff_raw(aux_HI)
# Including zeros
IDbehav_Beg <- get_IDHI("Beg", IDbehav, rawHI_diff)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$HI <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
# Including zeros
IDbehav_Beg <- get_IDHI("Beg", IDbehav, rawHI_diff)
# Not including zeros
IDbehav_Beg <- get_IDHI("Beg", IDbehav_HI, rawHI_diff_HI)
# Categorize DiffHI to IDs
diff_raw <- function(aux_data) {
rawHI_diff <- lapply(aux_data, function(df) {
table_df <- as.data.frame(table(df$Code, df$DiffHI))
colnames(table_df) <- c("Code", "DiffHI", "Freq")
return(table_df)
})}
rawHI_diff_HI <- diff_raw(aux_HI)
# Create a frequency count for each HI behavior
get_IDHI <- function(HI, IDbehav_data, rawHI_diff_data) {
lapply(seq_along(IDbehav_data), function(i) {
df <- IDbehav_data[[i]]
HI_freq <- rawHI_diff_data[[i]]$Freq[rawHI_diff_data[[i]]$DiffHI == HI]
df$HI <- HI_freq[match(df$Code, rawHI_diff_data[[i]]$Code)]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
# Not including zeros
IDbehav_Beg <- get_IDHI("Beg", IDbehav_HI, rawHI_diff_HI)
View(list_sexage_threeyears)
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux_HI <- aux_data(list_HI_years)
View(aux_HI)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav_HI <- ID_forg(aux_HI)
View(IDbehav_HI)
View(IDbehav_HI)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux_data, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav_HI <- ID_forg(aux_HI)
View(IDbehav_HI)
# HI behaviors should be partitioned into 3 different types---------------------
#' B = Beg: F, G, H
#' P = Patrol: A, B, C
#' D = Depredation: D, E, P
# Change the code using ifelse statements
subset_HI <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$DiffHI <- ifelse(aux_data[[i]]$ConfHI %in% c("F", "G", "H"), "Beg",
ifelse(aux_data[[i]]$ConfHI %in% c("A", "B", "C"), "Pat",
ifelse(aux_data[[i]]$ConfHI %in% c("P", "D", "E"), "Dep", "0")))
}}
aux_HI <- subset_HI(aux_HI)
