cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(1,1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Veg dynamics
veg.dynamics_vect<-function(N.suitable,N.unsuit,time,hurricane){
Total.patch= N.suitable + N.unsuit
N.unsuit <- N.unsuit + ifelse(hurricane==1,rbinom(length(N.suitable),N.suitable,0.5),0)
N.suitable <- Total.patch - N.unsuit
time <- ifelse(hurricane==1, 0, time + 1)
wt.slow<-0.5
wt.fast<- 1- wt.slow
p.suitable<- 1/(1+exp(-(-5 + 0.05*time + 0.05*time^2)))*wt.slow +
1/(1+exp(-(-5 + 0.75*time)))*wt.fast
N.suitable <- N.suitable + rbinom(length(N.suitable),N.unsuit,p.suitable)
N.unsuit<- Total.patch - N.suitable
veg.data<- cbind(N.suitable,N.unsuit,time)
return(veg.data)}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years, decision){
#   N.suitable=50
#   N.unsuit=50
#   years=50
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
# Change Persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years,decision){
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
vect <- rnorm(100, 30, 2)
# Stop the cluster
stopCluster(cl)
library(parallel)
library(doParallel)
# Stop the cluster
stopCluster(cl)
stopImplicitCluster()
# Specify the number of nodes/workers in the cluster
num_nodes <- 2
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Stop the cluster
stopCluster(cl)
gc()
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
###########################################################################
# PART 1: Create HI Similarity Matrix  ------------------------------------------------
## load all necessary packages
require(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(ncf) # For weights
require(vegan)
# Read file in to retain ILV
orig_data <- read.csv("sample_data.csv")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
year <- 1
dolp_dist = nxn[[year]] + 0.00001
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
dolp_dist
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
dolp_dist
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)
colnames(IDbehav) <- c("Code", "Foraging", "Freq")
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
rawHI <- data.frame(rawHI)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
# Take out the number of foraging events per ID
IDdata <- data.frame(Foraging = subset(IDbehav, IDbehav$Foraging == "Feed"))[,c(1,3)]
## Add up the # of times each ID was seen in HI
IDdata$HI <- rawHI$Freq[rawHI$ConfHI != "0"]
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
# Only ID to prop
HIprop_ID <- na.omit(IDdata[,c(1, 4)])
# ----Or I could measure HI per obs for each ID-----
IDdata_2 <- as.data.frame(table(aux$Code))
IDdata_2$HI <- rawHI$Freq[rawHI[,2] != "0"]
IDdata_2$HIprop <- IDdata_2$HI/IDdata_2$Freq
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- t(HIprop_ID$HIprop)
dissimilarity <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
dissimilarity
fake_HIprop
IDdata_2
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
###########################################################################
# PART 1: Structure Network ------------------------------------------------
## load all necessary packages
require(igraph) # Look at Dai Shizuka/Jordi Bascompte
require(tnet) # For weights
require(sna)
require(statnet)
require(doParallel)
# Read in social association matrix
nxn <- readRDS("nxn.RData")
sample_data <- read.csv("sample_data.csv")
# Test one year at a time
year <- 1
## Create social network
ig <- graph_from_adjacency_matrix(as.matrix(nxn[[year]]),
mode = c("undirected"),
weighted = TRUE,
diag = F, # No loops
add.colnames = T,
add.rownames = NA)
# Plot network
plot(ig,
layout = layout_with_fr(ig),
# link weight, rescaled for better visualization
edge.width= E(ig)$weight*4,
# node size as degree (rescaled)
vertex.size= sqrt(igraph::strength(ig, vids = V(ig), mode = c("all"), loops = TRUE) *10 ),
vertex.frame.color= NA, #"black",
vertex.label.family = "Helvetica",
vertex.label.color="black",
vertex.label.cex=0.8,
vertex.label.dist=2,
# edge.curved=0,
vertex.frame.width=0.01,
)
el <- readRDS("../data/el_years.RData")
#' Breakdown: connectance = length(which(as.dist(orca_hwi)!=0))/(N*(N-1)/2)
#' Number of nodes (number of rows in the association matrix)
N = nrow(nxn[[year]])
#' Number of possible links:
#' Nodes*(Nodes-1)/2: (-1 removes the node itself; /2 removes repetitions)
total = N*(N-1)/2
# Number of realized links: all non-zero cells in the association matrix
real = length(which(as.dist(nxn[[year]])!=0))
# Connectance: realized/total
real/total
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
### End parallel processing
stopImplicitCluster()
})
# Modularity by the WalkTrap algorithm
system.time({
registerDoParallel(n.cores)
dolphin_walk <- list()
for (k in 1:length(years)) {
dolphin_walk[[k]] <- cluster_walktrap(dolphin_ig[[k]], weights = E(dolphin_ig[[k]])$weight,
steps = 4, merges = TRUE, modularity = TRUE, membership = TRUE)
}
### End parallel processing
stopImplicitCluster()
})
## Modularity Q-value
modularity(dolphin_walk[[year]])
n.cores <- detectCores()
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
### End parallel processing
stopImplicitCluster()
})
## Edgelist for each year
years <- unique(sample_data$Year)
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
### End parallel processing
stopImplicitCluster()
})
# Modularity by the WalkTrap algorithm
system.time({
registerDoParallel(n.cores)
dolphin_walk <- list()
for (k in 1:length(years)) {
dolphin_walk[[k]] <- cluster_walktrap(dolphin_ig[[k]], weights = E(dolphin_ig[[k]])$weight,
steps = 4, merges = TRUE, modularity = TRUE, membership = TRUE)
}
### End parallel processing
stopImplicitCluster()
})
## Modularity Q-value
modularity(dolphin_walk[[year]])
## Number of modules
groups(dolphin_walk[[year]])
## Membership of modules
membership(dolphin_walk[[year]])
IDdata
## Save the edgelist into a new object
auxrand <- as.data.frame(el[[year]])
# Permutate the link weights
sample(auxrand$vw)
## Save in the auxrand object
auxrand[,3] <- sample(auxrand$vw)
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
### Add link weights
E(igrand)$weight <- el[[year]][,3]
### Make undirected graph
igrand <- as.undirected(igrand)
## Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
## Calculate modularity Q-value
rmod <- walktrap.community(igrand)
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig[[l]] <- graph.edgelist(el[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el[[l]][,3])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
### End parallel processing
stopImplicitCluster()
})
# Newman's Q modularity
newman <- cluster_leading_eigen(dolp_ig[[year]], steps = -1, weights = E(dolp_ig[[year]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
# Assign a random color to individuals of each module ('module')
V(dolp_ig[[year]])$color <- NA
# Generate a vector of colors based on the number of unique memberships
col <- rainbow(max(newman$membership))
# Assign individual IDs as labels to the nodes
V(dolp_ig[[year]])$label <- V(dolp_ig[[year]])$name
# Generate a vector of colors based on the number of unique memberships
V(dolp_ig[[year]])$color <- NA
col <- rainbow(max(newman$membership))
# Create a vector of individual IDs
individual_ids <- 1:vcount(dolp_ig[[year]])
# Assign individual IDs as labels to the nodes
V(dolp_ig[[year]])$label <- individual_ids
for (i in 1:max(newman$membership)){
V(dolp_ig[[year]])$color[which(newman$membership==i)] <- col[i]
}
# Plot the graph with individual IDs as labels
plot(dolp_ig[[year]], vertex.label = V(dolp_ig[[year]])$label)
newman
dolp_ig[[year]]
el[[year]]
nxn[[year]]
ig
## Edgelist for each year
years <- unique(sample_data$Year)
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
el_years <- list()
for (i in 1:length(years)) {
el_years[[i]] <- matrix_to_edgelist(nxn[[i]], rawdata = FALSE, idnodes = TRUE)
}
})
# Edgelist: Nodes (i & j) and edge (or link) weight
source("../code/functions.R") # SRI & null permutation
system.time({
registerDoParallel(n.cores)
el_years <- list()
for (i in 1:length(years)) {
el_years[[i]] <- matrix_to_edgelist(nxn[[i]], rawdata = FALSE, idnodes = TRUE)
}
})
saveRDS(el_years, "el_years.RData")
el <- readRDS("../data/el_years.RData")
el <- readRDS("../data/el_years.RData")
saveRDS(el_years, "el_years.RData")
el <- readRDS("../data/el_years.RData")
## Save the edgelist into a new object
auxrand <- as.data.frame(el[[year]])
## Save in the auxrand object
auxrand[,3] <- sample(auxrand$vw)
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
el[[year]]
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
class(el[[year]])
el <- as.matrix(readRDS("../data/el_years.RData"))
class(el[[year]])
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
el[[year]]
el[[year]][,1:2]
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]])
igrand
### Add link weights
E(igrand)$weight <- el[[year]][,3]
el[[year]][,3]
el[[year]]
### Add link weights
E(igrand)$weight <- el[[year]][,2]
### Make undirected graph
igrand <- as.undirected(igrand)
## Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
## Calculate modularity Q-value
rmod <- walktrap.community(igrand)
modularity(rmod)
## Number of modules
groups(rmod)
## Membership of modules
membership(rmod)
# Difference from our empirical data?
modularity(dolphin_walk[[year]])
modularity(rmod)
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig[[l]] <- graph.edgelist(el[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el[[l]][,3])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
### End parallel processing
stopImplicitCluster()
})
# Newman's Q modularity
newman <- cluster_leading_eigen(dolp_ig[[year]], steps = -1, weights = E(dolp_ig[[year]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig[[l]] <- graph.edgelist(el[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el[[l]][,2])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
### End parallel processing
stopImplicitCluster()
})
# igraph format
igrand <- graph.edgelist(auxrand[,1:2]) # Create a network from the list of nodes
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
el[[year]][,1:2]
system.time({
registerDoParallel(n.cores)
el_years <- list()
for (i in 1:length(years)) {
el_years[[i]] <- matrix_to_edgelist(nxn[[i]], rawdata = FALSE, idnodes = FALSE)
}
})
## Edgelist for each year
years <- unique(sample_data$Year)
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
el_years <- list()
for (i in 1:length(years)) {
el_years[[i]] <- matrix_to_edgelist(nxn[[i]], rawdata = FALSE, idnodes = FALSE)
}
### End parallel processing
stopImplicitCluster()
})
