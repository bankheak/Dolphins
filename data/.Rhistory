# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
###########################################################################
# PART 1: Social Association Matrix ---------------------------------------------
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutations
require(assocInd)
# Run multiple cores for faster computing
require(doParallel)
require(microbenchmark)
require(parallel)
require(foreach)
# Read file in
orig_data<- read.csv("secondgen_data.csv")
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data<- subset(orig_data, subset=c(orig_data$Year == 2005))
# Make sure every ID has >10 obs
ID <- unique(sample_data$Code)
length(ID)
obs_vect<- NULL
length(ID[1]) %in% sample_data$Code
ID[1]
ID[1] %in% sample_data$Code
length(ID[1] %in% sample_data$Code)
length(sample_data$Code[ID[1]])
sample_data[Code == ID[1]]
sample_data[sample_data$Code == ID[1]]
sample_data[sample_data$Code == "F209"]
length(sample_data$Code == ID[1])
length(sample_data$Code == "F209")
View(sample_data)
sample_data$Code == "F209"
ID
sum(sample_data$Code == "F209")
ID[1]
sum(sample_data$Code == ID[1])
obs_vect<- NULL
for (i in 1:length(ID)) {
obs_vect<- sum(sample_data$Code == ID[i])
}
obs_vect
sum(sample_data$Code == ID[i])
sum(sample_data$Code == ID[2])
1:length(ID)
obs_vect<- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(sample_data$Code == ID[i])
}
obs_vect
sub< rbind(ID, obs_vect)
sub< cbind(ID, obs_vect)
sub< c(ID, obs_vect)
sub< append(ID, obs_vect)
ID<- as.data.frame(ID)
obs_vect<- as.data.frame(obs_vect)
sub< cbind(ID, obs_vect)
View(ID)
View(obs_vect)
# Make sure every ID has >10 obs
ID <- unique(sample_data$Code)
obs_vect<- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(sample_data$Code == ID[i])
}
sub<- data.frame(ID, obs_vect)
sub<- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(sample_data, subset=c(sample_data$Code == sub$ID))
# Use only one year
sample_data <- subset(orig_data, subset=c(orig_data$Year == 2005))
orig_data$Year == 2005
sample_data$Code == sub$ID
sub$ID
sample_data$Code == c(sub$ID)
sample_data$Code %in% c(sub$ID)
sample_data <- subset(sample_data, sample_data$Code %in% c(sub$ID))
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Gambit of the group index
gbi<- get_group_by_individual(sample_data, data_format = "individuals")
write.csv(gbi, "gbi.csv")
# Create association matrix
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn<- SRI.func(gbi)
})
nxn<-as.matrix(nxn)
# end parallel processing
stopImplicitCluster()
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
#  Create 1000 random group-by-individual binary matrices
nF <- null(gbi, iter=1000)
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_null <- rep(NA,1000)
system.time({
registerDoParallel(n.cores)
for (i in 1:1000) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100}
})
stopImplicitCluster()
for (i in 1:reps) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100
}
reps<- 2
for (i in 1:reps) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100
}
#  Create 1000 random group-by-individual binary matrices
reps<- 2
nF <- null(gbi, iter=reps)
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_null <- rep(NA,reps)
View(nF)
View(nF[[1]])
dim(nF[[1]])
dim(gbi)
registerDoParallel(n.cores)
null <- foreach(i = 1:reps,
.combine = c) %dopar% {
# replace c with rbind to create a dataframe
sri_null = as.matrix(SRI.func(nF[[i]]))
( sd(sri_null) / mean(sri_null) ) * 100}
null
nF[[1]] == nF[[2]]
