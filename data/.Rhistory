}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_small) + 1)))
(max(N_small) + 50)
F_m = 0.2
N_small <- rep(NA, numyears)
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
N_small
N_small[1] <- N0
F_m = 0.2
N_small <- rep(NA, numyears)
N_small[1] <- N0
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_small) + 50)))
F_m = 0.3
N_med <- rep(NA, numyears)
N_med[1] <- N0
for(t in 1:(numyears-1)){
N_med[t + 1] = N_med[t] + r * N_med[t] * (1 - N_med[t]/K) - F_m * N_med[t]
}
# Plot through time
plot(1:numyears, N_med, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_med) + 1)))
F_m = 0.4
N_large <- rep(NA, numyears)
N_large[1] <- N0
for(t in 1:(numyears-1)){
N_large[t + 1] = N_large[t] + r * N_large[t] * (1 - N_large[t]/K) - F_m * N_large[t]
}
# Plot through time
plot(1:numyears, N_large, xlab = "Years", ylab = "Population Size", ylim = c(0, (max(N_large) + 1)))
N_small[1] <- N0
F_m = 0.2
N_small <- rep(NA, numyears)
N_small[1] <- N0
for(t in 1:(numyears-1)){
N_small[t + 1] = N_small[t] + r * N_small[t] * (1 - N_small[t]/K) - F_m * N_small[t]
}
# Plot through time
plot(1:numyears, N_small, xlab = "Years", ylab = "Population Size")
F_m = 0.3
N_med <- rep(NA, numyears)
N_med[1] <- N0
for(t in 1:(numyears-1)){
N_med[t + 1] = N_med[t] + r * N_med[t] * (1 - N_med[t]/K) - F_m * N_med[t]
}
# Plot through time
plot(1:numyears, N_med, xlab = "Years", ylab = "Population Size")
F_m = 0.4
N_large <- rep(NA, numyears)
N_large[1] <- N0
for(t in 1:(numyears-1)){
N_large[t + 1] = N_large[t] + r * N_large[t] * (1 - N_large[t]/K) - F_m * N_large[t]
}
# Plot through time
plot(1:numyears, N_large, xlab = "Years", ylab = "Population Size")
N = 1:500
dndt = r * N * (1 - N/K)
plot(N, dndt, xlab = "Population Size", ylab = "dN/dt")
abline(0, 0.2)
abline(0, 0.3, lty=2)
abline(0, 0.4, lty=3)
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 20
H = 20
numyears <- 50
N_20 <- rep(NA, numyears)
N_20[1] <- N0
for(t in 1:(numyears-1)){
N_20[t + 1] = N_20[t] + r * N_20[t] * (1 - N_20[t]/K) - H
}
# Plot through time
plot(1:numyears, N_20, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H = 30
H = 30
N_30 <- rep(NA, numyears)
N_30[1] <- N0
for(t in 1:(numyears-1)){
N_30[t + 1] = N_30[t] + r * N_30[t] * (1 - N_30[t]/K) - H
}
# Plot through time
plot(1:numyears, N_30, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
# H= 40
H = 40
N_40 <- rep(NA, numyears)
N_40[1] <- N0
for(t in 1:(numyears-1)){
N_40[t + 1] = N_40[t] + r * N_40[t] * (1 - N_40[t]/K) - H
}
# Plot through time
plot(1:numyears, N_40, xlab = "Years", ylab = "Population Size", ylim = c(0,600))
gc()
gc()
gc()
# Set working directory here
setwd("../data")
## Coefficient of Variation ##
# Read in null cv values for one year
cv_null <- readRDS("cv_years.RData")
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs <- lapply(nxn, function (df) {(sd(df) / mean(df)) * 100})  # Very high CV = unexpectedly
nxn <- readRDS("nxn.RData") # association matrix of list_years
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs <- lapply(nxn, function (df) {(sd(df) / mean(df)) * 100})  # Very high CV = unexpectedly
# Calculate 95% confidence interval, in a two-tailed test
cv_ci = lapply(cv_null, function (df) {quantile(df, probs=c(0.025, 0.975), type=2)})
# Check whether pattern of connections is non-random
par(mfrow=c(3, 1))
# Create a list to store the histograms
hist_cvs <- list()
# Create histograms for each element in cv_null
for (i in seq_along(cv_null)) {
hist_cvs[[i]] <- hist(cv_null[[i]],
breaks=50,
xlim = c(min(cv_null[[i]]), max(cv_obs[[i]])),
col='grey70',
main = NULL,
xlab="Null CV SRI")
# Add lines for empirical CV, 2.5% CI, and 97.5% CI
abline(v= cv_obs[[i]], col="red")
abline(v= cv_ci[[i]], col="blue")
abline(v= cv_ci[[i]], col="blue")
}
# Check whether pattern of connections is non-random
par(mfrow=c(1, 1))
# Create a list to store the histograms
hist_cvs <- list()
# Create histograms for each element in cv_null
for (i in seq_along(cv_null)) {
hist_cvs[[i]] <- hist(cv_null[[i]],
breaks=50,
xlim = c(min(cv_null[[i]]), max(cv_obs[[i]])),
col='grey70',
main = NULL,
xlab="Null CV SRI")
# Add lines for empirical CV, 2.5% CI, and 97.5% CI
abline(v= cv_obs[[i]], col="red")
abline(v= cv_ci[[i]], col="blue")
abline(v= cv_ci[[i]], col="blue")
}
i=1
hist(cv_null[[i]],
breaks=50,
xlim = c(min(cv_null[[i]]), max(cv_obs[[i]])),
col='grey70',
main = NULL,
xlab="Null CV SRI")
hist(cv_null[[i]],
breaks=50,
col='grey70',
main = NULL,
xlab="Null CV SRI")
# Check whether pattern of connections is non-random
par(mfrow=c(3, 1))
# Create a list to store the histograms
hist_cvs <- list()
# Create histograms for each element in cv_null
for (i in seq_along(cv_null)) {
hist_cvs[[i]] <- hist(cv_null[[i]],
breaks=50,
col='grey70',
main = NULL,
xlab="Null CV SRI")
# Add lines for empirical CV, 2.5% CI, and 97.5% CI
abline(v= cv_obs[[i]], col="red")
abline(v= cv_ci[[i]], col="blue")
abline(v= cv_ci[[i]], col="blue")
}
i=1
cv_obs[[i]]
## Modularity ##
# Read in data
el <- readRDS("el_years.RData")
library(doParallel) # Faster computing
library(vegan)
## igraph format with weight
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in seq_along(list_years)) {
dolphin_ig[[j]] <- graph_from_adjacency_matrix(as.matrix(nxn[[j]]),
mode="undirected",
weighted=TRUE, diag=TRUE)
}
### End parallel processing
stopImplicitCluster()
})
list_years <- readRDS("list_years.RData") # (1995-2000)/(2001-2006)/(2007-2012)
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in seq_along(list_years)) {
dolphin_ig[[j]] <- graph_from_adjacency_matrix(as.matrix(nxn[[j]]),
mode="undirected",
weighted=TRUE, diag=TRUE)
}
### End parallel processing
stopImplicitCluster()
})
library(assocInd) # Could do permutations
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in seq_along(list_years)) {
dolphin_ig[[j]] <- graph_from_adjacency_matrix(as.matrix(nxn[[j]]),
mode="undirected",
weighted=TRUE, diag=TRUE)
}
### End parallel processing
stopImplicitCluster()
})
library(igraph) # graph_from_adjacency_matrix version = '1.6.0'
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in seq_along(list_years)) {
dolphin_ig[[j]] <- graph_from_adjacency_matrix(as.matrix(nxn[[j]]),
mode="undirected",
weighted=TRUE, diag=TRUE)
}
### End parallel processing
stopImplicitCluster()
})
# Dolphin walk
system.time({
registerDoParallel(n.cores)
dolphin_walk <- list()
for (k in seq_along(dolphin_ig)) {
dolphin_walk[[k]] <- cluster_walktrap(dolphin_ig[[k]],
weights = edge_attr(dolphin_ig[[k]], "weight"),
steps = 4, merges = TRUE,
modularity = TRUE,
membership = TRUE)
}
### End parallel processing
stopImplicitCluster()
})
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
#hist(randmod, xlim = c(0, 0.6), main = "Random Q Distribution", xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
#abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
#abline(v = ci[1], col = "blue")
# 97.5% CI
#abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
hist(randmod, xlim = c(0, 0.6), main = "Random Q Distribution", xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
abline(v = ci[1], col = "blue")
# 97.5% CI
abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
par(mfrow=c(3, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
# Check whether pattern of connections is non-random
par(mfrow=c(3, 1))
# Create a list to store the histograms
hist_cvs <- list()
# Create histograms for each element in cv_null
for (i in seq_along(cv_null)) {
hist_cvs[[i]] <- hist(cv_null[[i]],
breaks=50,
col='grey70',
main = NULL,
xlab="Null CV SRI",
col = "lightblue")
# Add lines for empirical CV, 2.5% CI, and 97.5% CI
abline(v= cv_obs[[i]], col="red")
abline(v= cv_ci[[i]], col="blue")
abline(v= cv_ci[[i]], col="blue")
}
# Create a list to store the histograms
hist_cvs <- list()
# Create histograms for each element in cv_null
for (i in seq_along(cv_null)) {
hist_cvs[[i]] <- hist(cv_null[[i]],
breaks=50,
col= "lightblue",
main = NULL,
xlab="Null CV SRI")
# Add lines for empirical CV, 2.5% CI, and 97.5% CI
abline(v= cv_obs[[i]], col="red")
abline(v= cv_ci[[i]], col="blue")
abline(v= cv_ci[[i]], col="blue")
}
i=3
par(mfrow=c(1, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
hist(randmod, xlim = c(0, 0.4), main = "Random Q Distribution",
xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
abline(v = ci[1], col = "blue")
# 97.5% CI
abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
par(mfrow=c(3, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
hist(randmod, xlim = c(0, 0.4),
xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
abline(v = ci[1], col = "blue")
# 97.5% CI
abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
par(mfrow=c(3, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
hist(randmod, xlim = c(0.05, 0.35),
xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
abline(v = ci[1], col = "blue")
# 97.5% CI
abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
par(mfrow=c(3, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
# Run modularity permutations 1000 times for each matrix
run_mod <- function(el, dolphin_walk_list) {
iter <- 1000
randmod <- numeric(iter)  # Initialize a numeric vector to store Q-values
result <- list()
for (k in 1:3) {
for (i in 1:iter) {
# Save the edgelist into a new object and permutate the link weights
auxrand <- el[[k]]
# transform it into igraph format
igrand <- graph_from_edgelist(auxrand[,1:2])
E(igrand)$weight <- auxrand[,3]
igrand <- as.undirected(igrand)
# Now we can permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity for the permutate copy
rand_walk <- cluster_walktrap(igrand)
# and finally save the modularity Q-value into the empty vector
randmod[i] <- modularity(rand_walk)
}
# Calculate the 95% confidence interval (two-tailed test)
ci <- quantile(randmod, probs = c(0.025, 0.975), type = 2)
# Visualization of the random Q distribution
hist(randmod, xlim = c(0.05, 0.35), main = NA,
xlab = "Q-value", ylab = "Frequency", col = "lightblue")
# Empirical Q-value
abline(v = modularity(dolphin_walk_list[[k]]), col = "red")
# 2.5% CI
abline(v = ci[1], col = "blue")
# 97.5% CI
abline(v = ci[2], col = "blue")
# Return a data frame with Q-value and confidence intervals
result[[k]] <- data.frame(Q = modularity(dolphin_walk_list[[k]]), LowCI = ci[1], HighCI = ci[2])
}
return(result)
}
par(mfrow=c(3, 1))
model <- run_mod(el = el, dolphin_walk_list = dolphin_walk)
