Beg_effort
Pat_effort
Dep_effort
install.packages("assortnet")
library(assortnet)
assortment.continuous(nxn[[1]])
IDbehav
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
rawHI
assortment.continuous(nxn[[1]], rawHI[[1]][,2])
rawHI[[1]][,2]
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
kov <- readRDS("kov.RDS")  # Home range overlap
kov <- as.dist(kov)
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Create a different frequency count for each HI behavior
get_IDHI <- function(confHI) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]]$ConfHI == confHI & rawHI[[i]]$ConfHI != "0"]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
View(IDbehav)
View(rawHI)
IDbehave <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]]$ConfHI == confHI & rawHI[[i]]$ConfHI != "0"]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
IDbehave <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]]$ConfHI != "0"]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
View(rawHI)
df <- IDbehav[[1]]
View(df)
df$HI <- rawHI[[1]]$Freq[rawHI[[1]]$ConfHI != "0"]
View(df)
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Clump all the HI behaviors together
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI != "0", 1, 0)
# Clump all the HI behaviors together
for (i in seq_along(aux)) {
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
df <- IDbehav[[1]]
View(df)
View(df)
df$HI <- rawHI[[1]]$Freq
rawHI[[1]]$Freq
View(rawHI)
rawHI[["1"]]
length(unique(rawHI[[1]][,1]))
# Get HI Freq
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]][,"Code"]]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
df <- IDbehav[[1]]
df$HI <- rawHI[[1]]$Freq[rawHI[[1]][,"Code"]]
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
# Aggregate the data by Code, summing up the frequencies
aggregated_df <- aggregate(Freq ~ Code, data = df, sum)
colnames(aggregated_df)[2] <- "Freq"
return(aggregated_df)
})
View(aux)
View(aux)
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
# Aggregate the data by Code, summing up the frequencies
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
# Create a data frame with unique "Code" values
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
View(rawHI)
# Get HI Freq
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[1]]
df$HI <- rawHI[[1]]$Freq
colnames(df) <- c("Code", "Foraging", "HI")
df
})
df <- IDbehav[[1]]
df
rawHI[[1]]
df$HI <- rawHI[[1]]$ConfHI
# Get HI Freq
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
# Get HI Freq
IDbehav_HI <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
View(prob_HI)
dist_HI <- dist_matr(prob_HI)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dist_matr(prob_HI)
prob_HI[[1]]
r
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dist_matr(prob_HI)
dist_HI <- dis_matr(prob_HI)
nxn[[1]]
rawHI[[1]]
# Match Code with matrix and vector
HI_vector <- lapply(seq_along(nxn), function(i) {
matrix_index <- match(rownames(nxn[[i]]), rawHI[[i]]$Code)
reordered_rawHI <- rawHI[[i]][matrix_index, ]
return(reordered_rawHI)
})
View(HI_vector)
View(HI_vector)
HI_vector[[1]][,"ConfHI"]
# Look at HI assortivity
assortment.continuous(nxn[[1]], HI_vector[[1]][,"ConfHI"]) # split and combine categories
seq_along(nxn)
# Look at HI assortivity coefficient over periods
assort_HI <- for (i in seq_along(nxn)) {
assortment.continuous(nxn[[i]], HI_vector[[i]][,"ConfHI"]) # split and combine categories
}
# Look at HI assortivity coefficient over periods
assort_HI <- NULL
for (i in seq_along(nxn)) {
assort_HI[i] <- assortment.continuous(nxn[[i]], HI_vector[[i]][,"ConfHI"]) # split and combine categories
}
assort_HI
plot(assort_HI)
assort_HI <- unlist(assort_HI)
assort_HI
plot(assort_HI)
?assortment.continuous
assortment.continuous(nxn[[1]], HI_vector[[1]][,"ConfHI"], SE = T)
# Look at HI assortivity coefficient over periods
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
assort_HI <- NULL
se <- NULL
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[i]][,"ConfHI"], SE = T)
assort_HI[i] <- coeff[i]$r
se <- coeff[i]$se}
# End parallel processing
stopImplicitCluster()
})
# Look at HI assortivity coefficient over periods
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
assort_HI <- NULL
se <- NULL
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[i]][,"ConfHI"], SE = T)
assort_HI[i] <- coeff$r
se[i] <- coeff$se}
# End parallel processing
stopImplicitCluster()
})
View(HI_vector)
assort_HI
se
assort_HI <- data.frame(HI_assort = unlist(assort_HI),
Std.Err = unlist(se), Year = c(1:7))
View(assort_HI)
# Whisker plot of HI assortivity over each time period
ggplot(assort_HI, aes(x = Year, y = HI_assort)) +
geom_errorbar(aes(ymin = HI_assort - se, ymax = HI_assort + se), width = 0.2) +
geom_point() +
geom_line() +
labs(x = "Year", y = "HI_assort") +
ggtitle("Whisker Plot of assort_HI with Standard Error") +
theme_minimal()
library(ggplot2)
# Whisker plot of HI assortivity over each time period
ggplot(assort_HI, aes(x = Year, y = HI_assort)) +
geom_errorbar(aes(ymin = HI_assort - se, ymax = HI_assort + se), width = 0.2) +
geom_point() +
geom_line() +
labs(x = "Year", y = "HI_assort") +
ggtitle("Whisker Plot of assort_HI with Standard Error") +
theme_minimal()
HI_vector[[1]][,"ConfHI"]
?assortment.continuous
rnorm(20)
dist_HI
View(prob_HI)
# Match Code with matrix and vector
HI_vector <- lapply(seq_along(nxn), function(i) {
matrix_index <- match(rownames(nxn[[i]]), prob_HI[[i]]$Code)
reordered_rawHI <- prob_HI[[i]][matrix_index, ]
return(reordered_prob_HI)
})
# Match Code with matrix and vector
HI_vector <- lapply(seq_along(nxn), function(i) {
matrix_index <- match(rownames(nxn[[i]]), prob_HI[[i]]$Code)
reordered_prob_HI <- prob_HI[[i]][matrix_index, ]
return(reordered_prob_HI)
})
assort_HI <- NULL
se <- NULL
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[1]][,"ConfHI"], SE = F)
assort_HI[i] <- coeff$r}
View(HI_vector)
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[1]][,"HIprop"], SE = F)
assort_HI[i] <- coeff$r}
assort_HI <- NULL
for (i in seq_along(nxn)) {
coeff <- assortment.continuous(nxn[[i]], HI_vector[[i]][,"HIprop"], SE = F)
assort_HI[i] <- coeff$r}
assort_HI <- data.frame(HI_assort = unlist(assort_HI),
#Std.Err = unlist(se),
Year = c(1:7))
View(assort_HI)
# Whisker plot of HI assortivity over each time period
ggplot(assort_HI, aes(x = Year, y = HI_assort)) +
#geom_errorbar(aes(ymin = HI_assort - se, ymax = HI_assort + se), width = 0.2) +
geom_point() +
geom_line() +
labs(x = "Year", y = "HI_assort") +
ggtitle("Whisker Plot of assort_HI with Standard Error") +
theme_minimal()
HI_vector[[1]][,"HIprop"]
# Set a number of permutations
Nperm <- 1000
# Calculate QAP correlations for the association response matrix
year <- 5
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# Clump all the HI behaviors together
for (i in seq_along(aux)) {
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
# Get HI Freq
IDbehav_HI <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dis_matr(prob_HI)
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
View(dist_HI)
dist_HI[[year]]
dim(dist_HI[[year]])
#dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
#dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dis_matr(prob_HI)
View(dist_HI)
View(nxn)
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
kov <- readRDS("kov.RDS")  # Home range overlap
View(nxn)
mrqap <- mrqap.dsp(nxn[[year]] ~ dist_HI[[year]] + kov,
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
mrqap
summary(mrqap)
mrqap
HI_vector[[1]][,"HIprop"]
