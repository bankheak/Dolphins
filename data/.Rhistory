library(parallel)
library(doParallel)
# Stop the cluster
stopCluster(cl)
stopImplicitCluster()
# Specify the number of nodes/workers in the cluster
num_nodes <- 2
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Stop the cluster
stopCluster(cl)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(parallel)
library(sfsmisc, verbose=F)
# Read in file and add months
sample_data <- read.csv("sample_data.csv")
# Get all unique Code values in the entire sample_data
all_codes <- unique(sample_data$Code)
# Create a function that counts the IDs in each element
count_instances <- function(df) {
code_counts <- table(df$Code)
code_counts <- code_counts[match(all_codes, names(code_counts))]
code_counts[is.na(code_counts)] <- 0
return(code_counts)
}
# -------------------- 22 sets of 1 year increments----------------------------
# Make a list of only 1 year per dataframe
list_years <- split(sample_data, sample_data$Year)
# Apply the count_instances function to each year
instances_per_year <- lapply(list_years, count_instances)
# Convert the list of counts to a data frame
p1y <- do.call(rbind, instances_per_year)
# Transforming into binary matrices
p1y <- as.matrix(p1y); p1y[which(p1y>=1)] = 1; p1y[which(p1y<1)] = 0
# -------------------- 11 sets of 2 year increments----------------------------
# Make a list of 2 years per dataframe
sample_data$TwoYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 2, by = 2), labels = FALSE)
list_twoyears <- split(sample_data, sample_data$TwoYearIncrement)
# Apply the count_instances function to each two years
instances_per_twoyear <- lapply(list_twoyears, count_instances)
# Convert the list of counts to a data frame
p2y <- do.call(rbind, instances_per_twoyear)
# Transforming into binary matrices
p2y <- as.matrix(p2y); p2y[which(p2y>=1)] = 1; p2y[which(p2y<1)] = 0
# -------------------- 7 sets of 3 year increments----------------------------
# Make a list of 3 years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Apply the count_instances function to each two years
instances_per_threeyear <- lapply(list_threeyears, count_instances)
# Convert the list of counts to a data frame
p3y <- do.call(rbind, instances_per_threeyear)
# Transforming into binary matrices
p3y <- as.matrix(p3y); p3y[which(p3y>=1)] = 1; p3y[which(p3y<1)] = 0
# -------------------- 6 sets of 4 year increments----------------------------
# Make a list of 4 years per dataframe
sample_data$FourYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 4, by = 4), labels = FALSE)
list_fouryears <- split(sample_data, sample_data$FourYearIncrement)
# Apply the count_instances function to each two years
instances_per_fouryear <- lapply(list_fouryears, count_instances)
# Convert the list of counts to a data frame
p4y <- do.call(rbind, instances_per_fouryear)
# Transforming into binary matrices
p4y <- as.matrix(p4y); p4y[which(p4y>=1)] = 1; p4y[which(p4y<1)] = 0
# -------------------- 4 sets of 5 year increments----------------------------
# Make a list of 5 years per dataframe
sample_data$FiveYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 5, by = 5), labels = FALSE)
list_fiveyears <- split(sample_data, sample_data$FiveYearIncrement)
# Apply the count_instances function to each two years
instances_per_fiveyear <- lapply(list_fiveyears, count_instances)
# Convert the list of counts to a data frame
p5y <- do.call(rbind, instances_per_fiveyear)
# Transforming into binary matrices
p5y <- as.matrix(p5y); p5y[which(p5y>=1)] = 1; p5y[which(p5y<1)] = 0
# -------------------- 4 sets of 6 year increments----------------------------
# Make a list of 6 years per dataframe
sample_data$SixYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 6, by = 6), labels = FALSE)
list_sixyears <- split(sample_data, sample_data$SixYearIncrement)
# Apply the count_instances function to each two years
instances_per_sixyear <- lapply(list_sixyears, count_instances)
# Convert the list of counts to a data frame
p6y <- do.call(rbind, instances_per_sixyear)
# Transforming into binary matrices
p6y <- as.matrix(p6y); p6y[which(p6y>=1)] = 1; p6y[which(p6y<1)] = 0
# -------------------- 3 sets of 7 year increments----------------------------
# Make a list of 7 years per dataframe
sample_data$SevenYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_sevenyears <- split(sample_data, sample_data$SevenYearIncrement)
# Apply the count_instances function to each two years
instances_per_sevenyear <- lapply(list_sevenyears, count_instances)
# Convert the list of counts to a data frame
p7y <- do.call(rbind, instances_per_sevenyear)
# Transforming into binary matrices
p7y <- as.matrix(p7y); p7y[which(p7y>=1)] = 1; p7y[which(p7y<1)] = 0
# -------------------- 3 sets of 8 year increments----------------------------
# Make a list of 8 years per dataframe
sample_data$EightYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 8, by = 8), labels = FALSE)
list_eightyears <- split(sample_data, sample_data$EightYearIncrement)
# Apply the count_instances function to each two years
instances_per_eightyear <- lapply(list_eightyears, count_instances)
# Convert the list of counts to a data frame
p8y <- do.call(rbind, instances_per_eightyear)
# Transforming into binary matrices
p8y <- as.matrix(p8y); p8y[which(p8y>=1)] = 1; p8y[which(p8y<1)] = 0
source("../code/functions.R") # WDI & WDI permutation
# Turn over results
t1 = turnover_w(data = p1y, iter = 1000, subseq=F, plot=FALSE)
t2 = turnover_w(data = p2y, iter = 1000, subseq=F, plot=FALSE)
t3 = turnover_w(data = p3y, iter = 1000, subseq=F, plot=FALSE)
t4 = turnover_w(data = p4y, iter = 1000, subseq=F, plot=FALSE)
t5 = turnover_w(data = p5y, iter = 1000, subseq=F, plot=FALSE)
t6 = turnover_w(data = p6y, iter = 1000, subseq=F, plot=FALSE)
t7 = turnover_w(data = p7y, iter = 1000, subseq=F, plot=FALSE)
t8 = turnover_w(data = p8y, iter = 1000, subseq=F, plot=FALSE)
all = rbind(t1, t2, t3, t4, t5, t6, t7, t8)
all = cbind(c(1, 2, 3, 4, 5, 6, 7, 8), all)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.34,0.43), xlim=c(1,8), cex.axis=0.8)
axis(1, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, cex.axis=0.7)
mtext(side = 1, "Length of periods (years)", line = 2, font = 1)
axis(3, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, labels=c(22, 11, 7, 6, 5, 4, 3, 3), cex.axis=0.7)
mtext(side = 3, "Number of periods", line = 2, font = 1)
# Print final results
all
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.30,0.7), xlim=c(1,8), cex.axis=0.8)
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.5,0.6), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.56,0.61), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.565,0.61), xlim=c(1,8), cex.axis=0.8)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(adehabitatHR) # Kernel density
library(rgdal) # Overlap
# Read in file
sample_data <- read.csv("sample_data.csv")
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'subYear', 'ConfHI')]) # Subset Date and Coordinates #
# Read in file
sample_data <- read.csv("sample_data.csv")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
gc()
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
year <- 5
dolp_dist = nxn[[year]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
# Sex similarity matrix
sexvec <- ifelse(is.na(list_years[[year]]$Sex) | list_years[[year]]$Sex == "Female", 2, 1)
sex <- dist(sexvec)
# Age similarity matrix
agevec <- list_years[[year]]$Age
age <- dist(agevec)
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# HI behaviors should be partitioned into 3 different types
#' B = Begging (direct provisioning): F, G, H
#' P = patrolling/scavenging (indirect): A, B, C
#' D = foraging around fixed gear (humans not present):D, E, P
# Fix the code using ifelse statements
for (i in seq_along(aux)) {
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI %in% c("F", "G", "H"), "B",
ifelse(aux[[1]]$ConfHI %in% c("A", "B", "C", "D", "E"), "S",
ifelse(aux[[i]]$ConfHI %in% c("P"), "D", "0")))
}
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Create a different frequency count for each HI behavior
get_IDHI <- function(confHI) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]]$ConfHI == confHI & rawHI[[i]]$ConfHI != "0"]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
IDbehav_Beg <- get_IDHI("B")
IDbehav_Pat <- get_IDHI("S")
IDbehav_Dep <- get_IDHI("D")
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_Pat <- Prop_HI(IDbehav_Pat)
prob_Dep <- Prop_HI(IDbehav_Dep)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(IDbehav) {
dissimilarity_HI <- list()
for (i in seq_along(IDbehav)) {
fake_HIprop <- IDbehav[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_Pat <- dis_matr(prob_Pat)
dist_Dep <- dis_matr(prob_Dep)
# Dissimilarity Mantel Test
year <- 5
HI_test <- mantel.rtest(dolp_dist, as.dist(dist_Pat[[year]]), nrepet = 1000)
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
library(ncf) # For weights
library(vegan)
library(igraph) # graph_adj
require(asnipe) # mrqap.dsp
HI_test <- mantel.rtest(dolp_dist, as.dist(dist_Pat[[year]]), nrepet = 1000)
View(dist_Pat)
dist_Pat[[5]]
as.dist(dist_Pat[[year]])
dist_Pat <- as.dist(dis_matr(prob_Pat))
dist_Pat <- dis_matr(prob_Pat)
class(dist_Pat)
class(dist_Pat[[year]])
?lapply
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
View(dolp_dist)
dolp_dist[[1]]
# Sex similarity matrix
sex <- lapply(list_years, function(df) {
sexvec <- ifelse(is.na(df$Sex) | df$Sex == "Female", 2, 1)
dist(sexvec)})
View(sex)
# Age similarity matrix
age <- lapply(list_years, function(df) {
agevec <- df$Age
age <- dist(agevec)})
View(age)
dim(dolp_dist[[year]])
# Dissimilarity Mantel Test
year <- 5
HI_test <- mantel.rtest(dolp_dist[[year]], dist_Pat[[year]], nrepet = 1000)
dolp_dist[[year]]
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
#as.dist(df)
})
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Transforming SRI similarity into distance
nrow(nxn[[year]])
View(list_years)
# Sex similarity matrix
length(unique(list_years[[year]]$Code))
# Age similarity matrix
age <- lapply(list_years, function(df) {
agevec <- df$Age
age <- dist(agevec)})
dim(age)
dim(age[[year]])
View(age)
# Sex similarity matrix
sex <- lapply(list_years, function(df) {
sexvec <- ifelse(is.na(df$Sex) | df$Sex == "Female", 2, 1)
dist(sexvec)})
dim(sex[[year]])
View(sex)
# Sex similarity matrix
sex <- lapply(list_years, function(df) {
sexvec <- ifelse(is.na(df$Sex) | df$Sex == "Female", 2, 1)
#dist(sexvec)
})
# Sex similarity matrix
sex <- lapply(list_years, function(df) {
ifelse(is.na(df$Sex) | df$Sex == "Female", 2, 1)
#dist(sexvec)
})
View(sample_data)
View(sex)
sex[["1"]]
# Sex similarity matrix
## Empty matrix to store sex similarity
num_ID <- length(unique(list_years[[year]]))
num_ID
# Sex similarity matrix
sex <- lapply(list_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID)
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
})
View(sample_data)
View(sex)
# Sex similarity matrix
df <- list_years[[year]]
View(df)
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID)
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
View(sex_matrix)
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID)
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
# Add ID names
as.data.frame(sex_matrix, row.names = df$Code, col.names = df$Code)
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
# Sex similarity matrix
sex_list <- lapply(list_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
return(sex_matrix)
})
View(sex_list)
# Age similarity matrix
age_list <- lapply(list_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
age_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
df$Age[i] - df$Age[j]
}
}
return(age_matrix)
})
View(age_list)
# Age similarity matrix
age_list <- lapply(list_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
age_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
age_matrix[i, j] <- abs(df$Age[i] - df$Age[j])
}
}
return(age_matrix)
})
View(age_list)
dim(sex_list[[year]])
dim(dolp_dist[[year]])
View(dolp_dist)
# Extract specific columns from each data frame in list_years
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI
)
})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
# Categorize ID to Foraging
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
# HI behaviors should be partitioned into 3 different types
#' B = Begging (direct provisioning): F, G, H
#' P = patrolling/scavenging (indirect): A, B, C
#' D = foraging around fixed gear (humans not present):D, E, P
# Fix the code using ifelse statements
for (i in seq_along(aux)) {
aux[[i]]$ConfHI <- ifelse(aux[[i]]$ConfHI %in% c("F", "G", "H"), "B",
ifelse(aux[[i]]$ConfHI %in% c("A", "B", "C", "D", "E"), "S",
ifelse(aux[[i]]$ConfHI %in% c("P"), "D", "0")))
}
# Categorize ConfHI to IDs
rawHI <- lapply(aux, function(df) {
df <- as.matrix(table(df$Code, df$ConfHI))
df <- as.data.frame(df, stringsAsFactors = FALSE)
colnames(df) <- c("Code", "ConfHI", "Freq")
df
})
# Create a different frequency count for each HI behavior
get_IDHI <- function(confHI) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$Freq[rawHI[[i]]$ConfHI == confHI & rawHI[[i]]$ConfHI != "0"]
colnames(df) <- c("Code", "Foraging", "HI")
df
})
}
IDbehav_Beg <- get_IDHI("B")
IDbehav_Pat <- get_IDHI("S")
IDbehav_Dep <- get_IDHI("D")
