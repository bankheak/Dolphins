# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutations
require(assocInd)
source("../code/functions.R") # SRI & null permutation
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read file in
orig_data<- read.csv("secondgen_data.csv")
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data<- subset(orig_data, subset=c(orig_data$Year == 2005))
ID <- unique(sample_data$Code)
ID %in% sample_data$Code
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Gambit of the group index
gbi<- get_group_by_individual(sample_data, data_format = "individuals")
write.csv(gbi, "gbi.csv")
# Create association matrix
nxn<- SRI.func(gbi)
# Create association matrix
system.time({nxn<- SRI.func(gbi)})
# Run multiple cores
require(doParallel)
detectCores()
# Create association matrix
system.time({nxn<- SRI.func(gbi)})
# Create association matrix
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn<- SRI.func(gbi)
})
# end parallel processing
stopImplicitCluster()
nxn<-as.matrix(nxn)
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
#  Create 1000 random group-by-individual binary matrices
nF <- null(gbi, iter=1000)
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_null <- rep(NA,1000)
for (i in 1:1000) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100
}
total <- 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
for (i in 1:1000) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100
}
```{r}
knitr::opts_chunk$set(echo = TRUE)
total <- 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
close(pb)
system.time({
registerDoParallel(n.cores)
for (i in 1:1000) {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100
}
})
sri_null = as.matrix(SRI.func(nF[[i]]))
View(sri_null)
sri_null = as.matrix(SRI.func(nF))
View(nF)
# end parallel processing
stopImplicitCluster()
lapply(SRI.func, nF)
lapply(nF, SRI.func)
trial<- lapply(nF, SRI.func)
system.time({
registerDoParallel(n.cores)
trial<- lapply(nF, SRI.func)
sri_null = as.matrix(trial)
})
# end parallel processing
stopImplicitCluster()
total <- 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
close(pb)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutations
require(assocInd)
# Read file in
orig_data <-read.csv("../data/secondgen_data.csv")
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data<- subset(orig_data, subset=c(orig_data$Year == 2005))
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
sample_data <- cbind(group_data[,3:4]) # Subset ID and group
# Gambit of the group index
gbi<- get_group_by_individual(sample_data, data_format = "individuals")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
source("functions.R") # SRI & null permutation
# Create association matrix
nxn<- SRI.func(gbi)
total <- 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
close(pb)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
source("functions.R") # SRI & null permutation
# Create association matrix
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
nxn<- SRI.func(gbi)
nxn<-as.matrix(nxn)
# Calculate the CV of the observation association data
# CV = (SD/mean)*100
cv_obs=(sd(nxn) / mean(nxn)) * 100  # Very high CV = unexpectedly high or low association indices in the empirical distribution
cv_obs
total <- 10
pb <- txtProgressBar(min = 0, max = total, style = 3)
for(i in 1:total){
Sys.sleep(0.1)
setTxtProgressBar(pb, i) # update progress bar
}
close(pb)
#  Create 1000 random group-by-individual binary matrices
nF <- null(gbi, iter=1000)
#  Create 1000 random group-by-individual binary matrices
nF <- null(gbi, iter=100)
