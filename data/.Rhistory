setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(adehabitatHR) # Kernel density
library(rgdal) # Overlap
# Read in file
sample_data <- read.csv("sample_data.csv")
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'subYear', 'ConfHI')]) # Subset Date and Coordinates #
# Read in file
sample_data <- read.csv("sample_data.csv")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
gc()
# Set working directory here
setwd("C:/Users/bankh/My_Repos/DolphiNP/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphin/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutatioNP
require(assocInd)
require(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(foreach)
sample_data <- read.csv("sample_data.csv")
length(unique(sample_data$Code))
list_years <- readRDS("list_years.RData")
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- tapply(sample_data$Date, sample_data$Year, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years$Date, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
effort
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
unique_ID_year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- cbind(effort, pop) # Days per year and pop size per year
pop_effort
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
pop_effort
rownames(pop_effort) <- c('Days Surveyed', 'Number of Indivduals')
pop_effort
plot(pop_effort$effort ~ pop_effort$unique_ID_year)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,] ~ pop_effort[2,])
pop_effort[1,]
pop_effort[2,]
pop_effort[1,c(1:7)]
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
## Compare effort to population size
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
effort
pop_effort
# Read in different behavior's data frames
IDbehav_Beg <- readRDS("IDbehav_Beg.RData")
IDbehav_Pat <- readRDS("IDbehav_Pat.RData")
IDbehav_Dep <- readRDS("IDbehav_Dep.RData")
Beg_effort <- lapply(IDbehav_Beg, function(df) length(unique(df$Code)))
View(IDbehav_Beg)
length(unique(IDbehav_Beg[[1]]$Code=='B'))
IDbehav_Beg[[1]][["HI"]]
length(unique(IDbehav_Beg[[1]]$Code[IDbehav_Beg[[1]]$HI > 0]))
Beg_effort <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0])))
Beg_effort
Pat_effort <- as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI > 0]))))
Pat_effort
Beg_effort <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0]))))
Beg_effort
Dep_effort <- as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI > 0]))))
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
colnames(Beg_effort) <- c(1:7)
colnames(Pat_effort) <- c(1:7)
colnames(Dep_effort) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
effort
colnames(effort) <- c(1:7)
colnames(unique_ID_year) <- c(1:7)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
unique_ID_year
colnames(unique_ID_year) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
pop_effort
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals', 'Beggars', 'Patrollers', 'Depredators')
pop_effort
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
length(unique(orig_data$Code))
length(unique(sample_data$Code))
sum(pop_effort[2,])
sum(pop_effort[c(3:5),])
sum(unique(sample_data$Code[sample_data$ConfHI != 0]))
unique(sample_data$Code[sample_data$ConfHI != 0])
length(unique(sample_data$Code[sample_data$ConfHI != 0]))
sum(pop_effort[c(3),])
sum(pop_effort[c(4),])
sum(pop_effort[c(5),])
lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
sum(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
nb <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
nb <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
sum(nb)
sum(as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI == 0])))))
sum(as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI == 0])))))
all <- subset(IDbehav_Beg, IDbehav_Beg$Code %in% IDbehav_Pat$Code)
all
Beg <- as.data.frame(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0]))
View(Beg)
?append
Beg <- append(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0])))
Beg
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI = 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0]))))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0])))
Beg %in% Pat %in% Dep
sum(Beg %in% Pat %in% Dep)
sum(Beg %in% Dep)
sum(Pat %in% Dep)
Reduce(intersect, list(Beg, Pat, Dep))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI != 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI != 0])))
Reduce(intersect, list(Beg, Pat, Dep))
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
gc()
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
# Set working directory here
setwd("../data")
# Read in social association matrix and listed data
dist_BG <- readRDS("dist_BG.RData") # BG Sim Matrix
dist_FG <- readRDS("dist_FG.RData") # FG Sim Matrix
dist_SD <- readRDS("dist_SD.RData") # SD Sim Matrix
kov <- readRDS("kov.RDS")  # Home range overlap
nxn <- readRDS("nxn_ovrlap.RData") # Association Matrix
list_years <- readRDS("list_years_ovrlap.RData") # Data listed into periods
SE_array <- readRDS("SE_array.RData")
# Now make a sex and age data frame
ILV_df <- list_years[[2]][!duplicated(list_years[[2]][, "Code"]), c("Code", "Sex", "Age")]
ILV_df$Sex <- ifelse(ILV_df$Sex == "Female", 0,
ifelse(ILV_df$Sex == "Male", 1, NA))
## Package for Bayes
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
df_list <- vector("list", length = length(nxn))
for (i in seq_along(nxn)) {
upper_tri <- upper.tri(nxn[[i]], diag = TRUE)
df_dolp <- data.frame(
edge_weight = as.vector(nxn[[i]][upper_tri]),
node_id_1 = factor(as.vector(node_ids_i[[i]][upper_tri]), levels = 1:num_nodes[[i]]),
node_id_2 = factor(as.vector(node_ids_j[[i]][upper_tri]), levels = 1:num_nodes[[i]])
)
df_list[[i]] <- df_dolp
}
View(df_list)
df_list[[1]]
View(node_ids_i)
View(num_nodes)
node_names <- lapply(nxn, function(df) col.names(df))
node_names <- lapply(nxn, function(df) colnames(df))
View(node_names)
## Package for Bayes
# Prepare random effect for MCMC
num_nodes <- lapply(nxn, function(df) dim(df)[1])
node_names <- lapply(nxn, function(df) colnames(df))
# Separate IDs into i and j
node_ids_i <- lapply(num_nodes, function(df) matrix(rep(1:df, each = df), nrow = df, ncol = df))
node_ids_j <- lapply(node_ids_i, function(df) t(df))
df_list <- vector("list", length = length(nxn))
for (i in seq_along(nxn)) {
upper_tri <- upper.tri(nxn[[i]], diag = TRUE)
df_dolp <- data.frame(
edge_weight = as.vector(nxn[[i]][upper_tri]),
node_id_1 = factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri]]), levels = node_names[[i]]),
node_id_2 = factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri]]), levels = node_names[[i]])
)
df_list[[i]] <- df_dolp
}
View(df_list)
df_list[[1]]
# Combine all data frames into a single data frame
combined_df <- do.call(rbind, df_list)
View(combined_df)
library(abind) # array
df_list <- vector("list", length = length(nxn))
for (i in seq_along(nxn)) {
upper_tri <- upper.tri(nxn[[i]], diag = TRUE)
df_dolp <- data.frame(
edge_weight = as.vector(nxn[[i]][upper_tri]),
node_id_1 = factor(as.vector(node_names[[i]][node_ids_i[[i]][upper_tri]]), levels = node_names[[i]]),
node_id_2 = factor(as.vector(node_names[[i]][node_ids_j[[i]][upper_tri]]), levels = node_names[[i]])
)
df_list[[i]] <- df_dolp
}
# Combine all data frames into a single data frame
combined_array <- abind(df_list, along=3)
# Combine all data frames into a single data frame
combined_array <- do.call(rbind, df_list)
1:length(df_list)
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
df_list[[i]]$SE <- SE_array[,,i]
return(df_list[[i]])
})
upper.tri(SE_array[,,1], diag = TRUE)
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
df_list[[i]]$SE <- upper.tri(SE_array[,,1], diag = TRUE)
return(df_list[[i]])
})
SE_array[upper.tri(SE_array[,,1], diag = TRUE)]
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
df_list[[i]]$SE <- SE_array[upper.tri(SE_array[,,i], diag = TRUE)]
return(df_list[[i]])
})
1<-SE_array[upper.tri(SE_array[,,1], diag = TRUE)]
1.test<-SE_array[upper.tri(SE_array[,,1], diag = TRUE)]
test_one<-SE_array[upper.tri(SE_array[,,1], diag = TRUE)]
test_two<-SE_array[upper.tri(SE_array[,,2], diag = TRUE)]
dim(df_list[[1]])
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
df_list[[i]]$SE <- SE_array[upper.tri(SE_array[,,i], diag = TRUE, upper = TRUE)]
return(df_list[[i]])
})
SE_array[upper.tri(SE_array[,,i], diag = TRUE)]
SE_array[upper.tri(SE_array[,,1], diag = TRUE, upper = TRUE, byrow = TRUE)]
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
upper_tri <- upper.tri(SE_array[,,i], diag = TRUE)
df_list[[i]]$SE <- upper_tri
return(df_list[[i]])
})
upper_tri <- upper.tri(SE_array[,,1], diag = TRUE)
upper_tri
SE_array[,,1][upper_tri]
# Calculate the mean value for each individual
combined_array <- lapply(seq_along(df_list), function(i) {
upper_tri <- upper.tri(SE_array[,,i], diag = TRUE)
df_list[[i]]$SE <- SE_array[,,i][upper_tri]
return(df_list[[i]])
})
View(combined_array)
combined_array[[1]]
# load all necessary packages
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
library(asnipe) # mrqap.dsp
library(assortnet) # associative indices
library(kinship2) # genetic relatedness
library(ggplot2) # Visualization
library(doParallel) # For faster coding
library(abind) # array
library(statip) # dbern
library(MCMCglmm) # MCMC models
library(brms) # Baysian
library(nimble) # For MCMC
library(mcmcplots) # For MCMC plots
library(MCMCvis)
library(tidyverse)
View(df_list)
# Make final data frame for model
data_list <- list()
knitr::opts_chunk$set(echo = TRUE)
library(nimble)
library(ggridges)
library(ggplot2)
library(tidyverse)
library(loo)
library(mcmcplots)
source('attach.nimble_v2.R')
# Read in social association matrix and listed data
nimble.data <- readRDS("nimble.data.RData")
nimble.constants <- readRDS("nimble.constants.RData")
nimble.data2 <- list(SRI = unname(nimble.data$SRI),
HRO = unname(nimble.data$HRO),
SEX = unname(nimble.data$SEX),
AGE = unname(nimble.data$AGE),
BP = unname(nimble.data$BP),
FG = unname(nimble.data$FG),
Obs.Err = unname(nimble.data$Obs.Err))
# Write a Nimble model: SRI ~ HRO + SEX + AGE + GR + HAB(BP + FG + SD)
model1 <- nimbleCode({
#Priors
## ILV Effects
HRO_Effect ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
SEX_Effect ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
AGE_Effect ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
#GR_Effect ~ dt(mu=0, sigma=1, df=1)
Rand.Err ~ dunif(0,5) #T(dt(mu=0, sigma=1, df=1), 0, )
## HI Effects
for (p in 1:n.per) {
BP_Effect[p] ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
FG_Effect[p] ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
#SD_Effect[p] ~ dnorm(mean=0,sd=1) #dt(mu=0, sigma=1, df=1)
} #p
# Run through matrix
for(i in 1:n.ind){
# Estimate unknowns
SEX[i] ~ dbern(prob = 0.5) # Same probability for female or male
AGE[i] ~ dunif(0, 56) # uniform probability for all ages
AGE.Est[i] <- floor(AGE[i]) # Get a whole number
} #i
#AGE.Est <- scale.func(X = AGE.Est)
for(i in 1:n.ind){
# Random effect term
u[i] ~ dnorm(mean = 0, sd = Rand.Err)
}#i
for(i in 1:(n.ind-1)){
for (j in (1+i):n.ind) {
# Intercept Prior for each ID
IJ_Random[i, j] <- (u[i]+ u[j])
#Make similarity matrices for sexes & ages:
SEX.SIM[i, j] <- (SEX[i] == SEX[j])
AGE_Diff_Scaled[i, j] <- abs(AGE.Est[i] - AGE.Est[j])
## HI Effects
for (p in 1:n.per) {
# Process Model
logit(SRI.Exp[i, j, p]) <- IJ_Random[i, j] +
HRO[i, j]*HRO_Effect + SEX.SIM[i, j]*SEX_Effect + AGE_Diff_Scaled[i, j]*AGE_Effect + #GR[i, j]*GR_Effect +
BP[i, j, p]*BP_Effect[p] + FG[i, j, p]*FG_Effect[p] #+ SD[i, j, p]*SD_Effect[p]
# Observation Model (Likelihood)
# SRI[i, j, p] ~ dbeta(shape1 = SRI.Exp[i, j, p] * (1 - SRI.Exp[i, j, p]) / Obs.Err[p]^2 - SRI.Exp[i, j, p],
#                      shape2 = (1 - SRI.Exp[i, j, p]) * SRI.Exp[i, j, p]^2 / Obs.Err[p]^2 + SRI.Exp[i, j, p] - 1)
#
SRI[i, j, p] ~ dnorm(mean=SRI.Exp[i, j, p], sd=Obs.Err[i, j, p])
}#p
}#j
}#i
})#model1
# Parameters monitored (are there any new parameters to include?)
parameters <- c("AGE_Diff_Scaled", "SEX.SIM")
# MCMC Settings
ni <- 10000
nt <- 50
nb <- 1000
nc <- 3
mcmc.output <- nimbleMCMC(code = model1,
data = nimble.data2,
constants=nimble.constants,
monitors = parameters,
niter = ni,
nburnin = nb,
nchains = nc,
thin=nt,
summary=TRUE,
samplesAsCodaMCMC = TRUE)
attach.nimble(mcmc.output$samples)
SEX.SIM
1:length(ILV_df)
ILV_df
1:nrow(ILV_df)
dbern(prob = 0.5)
# Estimate unknowns
ILV_df$Sex <- ifelse(is.na(ILV_df$Sex), rbinom(n = nrow(ILV_df), size = 1, prob = 0.5), ILV_df$Sex)
ILV_df
ILV_df$Age ~ ifelse(is.na(ILV_df$Sex), runif(n = nrow(ILV_df), min = 1, max = 56), ILV_df$Age) # uniform probability for all ages
ILV_df$Age <- ifelse(is.na(ILV_df$Age), runif(n = nrow(ILV_df), min = 1, max = 56), ILV_df$Age) # uniform probability for all ages
ILV_df
# Now make a sex and age data frame
ILV_df <- list_years[[2]][!duplicated(list_years[[2]][, "Code"]), c("Code", "Sex", "Age")]
ILV_df$Sex <- ifelse(ILV_df$Sex == "Female", 0,
ifelse(ILV_df$Sex == "Male", 1, NA))
# Estimate unknowns
ILV_df$Sex <- ifelse(is.na(ILV_df$Sex), rbinom(n = nrow(ILV_df), size = 1, prob = 0.5), ILV_df$Sex)
ILV_df
ILV_df$Sex <- ifelse(ILV_df$Sex == "Female", 0,
ifelse(ILV_df$Sex == "Male", 1, NA))
# Now make a sex and age data frame
ILV_df <- list_years[[2]][!duplicated(list_years[[2]][, "Code"]), c("Code", "Sex", "Age")]
ILV_df$Sex <- ifelse(ILV_df$Sex == "Female", 0,
ifelse(ILV_df$Sex == "Male", 1, NA))
# Estimate unknowns
ILV_df$Sex <- ifelse(is.na(ILV_df$Sex), rbinom(n = nrow(ILV_df), size = 1, prob = 0.5), ILV_df$Sex)
ILV_df$Age <- ifelse(is.na(ILV_df$Age), floor(runif(n = nrow(ILV_df), min = 1, max = 56)), ILV_df$Age) # uniform probability for all ages
ILV_df
# Make sex sim matrix
sex_sim <- matrix(nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
# Make sex sim matrix
sex_sim <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
sex_sim
age_diff <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
# Make sex sim and age diff matrix
sex_sim <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
age_diff <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
for (i in 1:nrow(sex_sim)) {
for (j in (1+i):ncol(sex_sim)) {
sex.sim[i, j] <- (ILV_df$Sex[i] == ILV_df$Sex[j])
age_diff[i, j] <- abs(ILV_df$Age[i] - ILV_df$Age[j])
} #i
} #j
# Make sex sim and age diff matrix
sex_sim <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
age_diff <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
for (i in 1:nrow(sex_sim)) {
for (j in (1+i):ncol(sex_sim)) {
sex_sim[i, j] <- (ILV_df$Sex[i] == ILV_df$Sex[j])
age_diff[i, j] <- abs(ILV_df$Age[i] - ILV_df$Age[j])
} #i
} #j
ILV_df$Sex[1]
ILV_df$Sex[1] == ILV_df$Sex[2]
# Make sex sim and age diff matrix
sex_sim <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
age_diff <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
for (i in 1:nrow(sex_sim)) {
for (j in (1+i):ncol(sex_sim)) {
sex_sim[i, j] <- (ILV_df$Sex[i] == ILV_df$Sex[j])
age_diff[i, j] <- abs(ILV_df$Age[i] - ILV_df$Age[j])
sex_sim[j, i] <- sex_sim[i, j]
age_diff[j, i] <- age_diff[i, j]
} #i
} #j
for (i in 1:nrow(sex_sim)) {
for (j in (1+i):ncol(sex_sim)) {
sex_sim[i, j] <- as.numeric(ILV_df$Sex[i] == ILV_df$Sex[j])
age_diff[i, j] <- abs(ILV_df$Age[i] - ILV_df$Age[j])
sex_sim[j, i] <- sex_sim[i, j]
age_diff[j, i] <- age_diff[i, j]
} #i
} #j
sim(ILV_df$Sex)
dim(ILV_df$Sex)
length(ILV_df$Sex)
dim(sex_sim)
# Make sex sim and age diff matrix
sex_sim <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
age_diff <- matrix(0, nrow = nrow(nxn[[1]]), ncol = ncol(nxn[[1]]))
for (i in 1:(nrow(sex_sim) - 1)) {
for (j in (i + 1):ncol(sex_sim)) {
sex_sim[i, j] <- as.numeric(ILV_df$Sex[i] == ILV_df$Sex[j])
age_diff[i, j] <- abs(ILV_df$Age[i] - ILV_df$Age[j])
# Since sex_sim and age_diff are symmetric matrices, update the corresponding values
sex_sim[j, i] <- sex_sim[i, j]
age_diff[j, i] <- age_diff[i, j]
}
}
