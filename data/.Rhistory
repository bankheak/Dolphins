# Stop the cluster
stopCluster(cl)
View(cv_years)
View(cv_null)
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_years <- list()
# Load the parallel package
library(doParallel)
library(foreach)
# Read file in
gbi <-  readRDS("../data/gbi.RData")
# Create an empty list to store the updated matrices
gbi_updated <- vector("list", length(gbi))
# Loop through each matrix in the gbi list
for (i in seq_along(gbi)) {
# Subset the matrix to the desired dimensions
gbi_updated[[i]] <- gbi[[i]][1:20, 1:20]
}
gbi = gbi_updated
# Specify the number of nodes/workers in the cluster
num_nodes <- 4
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Register the cluster to enable parallel processing
registerDoParallel(cl)
# Load Null function
null <- function (mat, iter, ...){
library(vegan)
aux <- permatswap(mat, times=iter, method="quasiswap", fixedmar="both",
shuffle="both", mtype="prab")
return(aux$perm)
}
# Load SRI function
SRI.func<-  function (matr) {
if (any(is.na(matr))) {
matr <- na.omit(matr)
cat("The data matrix contains NA, and have been removed.\n")
}
matr1 = matr
N <- nrow(matr1)
matr1[matr1 > 1] <- 1
n <- apply(matr1, 2, sum)
tmatr <- t(matr1)
df <- as.matrix(t(matr))
a <- df %*% t(df) # Dyad in same group
b <- df %*% (1 - t(df)) # A present, B absent
c <- (1 - df) %*% t(df) # A absent, B present
d <- ncol(df) - a - b - c # Double absent
Dice <- data.frame()
for (i in 1:nrow(a)) {
for (j in 1:ncol(a)) {
Dice[i, j] <- a[i, j]/(a[i, j] + b[i, j] + c[i, j])
}
}
rownames(Dice)=colnames(Dice)=colnames(matr)
Dice
}
# Parallel processing
reps <- 1000
nF <- list()
for (i in 1:22) {
nF[[i]] <- null(gbi[[i]], iter=reps)
}
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_years <- list()
for (j in 1:22) {
cv_null <- rep(NA,reps)
cv_null <- foreach(i = 1:reps,
.combine = c) %dopar% {
sri_null = as.matrix(SRI.func(nF[[j]][[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100}
cv_years[[j]] <- cv_null
}
View(cv_years)
# Stop the cluster
stopCluster(cl)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
###########################################################################
# PART 1: Structure Network ------------------------------------------------
## load all necessary packages
require(igraph) # Look at Dai Shizuka/Jordi Bascompte
require(tnet) # For weights
require(sna)
require(statnet)
require(doParallel)
# Read in social association matrix
nxn <- readRDS("nxn.RData")
sample_data <- read.csv("sample_data.csv")
# Test one year at a time
year <- 1
# Edgelist: Nodes (i & j) and edge (or link) weight
source("../code/functions.R") # SRI & null permutation
el <- readRDS("../data/el_years.RData")
# Centrality measures
# Weighted clustering coefficients
clustering_local_w(el[[year]], measure=c("am", "gm", "mi", "ma", "bi"))
#' Breakdown: connectance = length(which(as.dist(orca_hwi)!=0))/(N*(N-1)/2)
#' Number of nodes (number of rows in the association matrix)
N = nrow(nxn[[year]])
#' Number of possible links:
#' Nodes*(Nodes-1)/2: (-1 removes the node itself; /2 removes repetitions)
total = N*(N-1)/2
# Number of realized links: all non-zero cells in the association matrix
real = length(which(as.dist(nxn[[year]])!=0))
# Connectance: realized/total
real/total
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
})
n.cores <- detectCores()
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
})
## Edgelist for each year
years <- unique(sample_data$Year)
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
### End parallel processing
stopImplicitCluster()
})
# Modularity by the WalkTrap algorithm
system.time({
registerDoParallel(n.cores)
dolphin_walk <- list()
for (k in 1:length(years)) {
dolphin_walk[[k]] <- cluster_walktrap(dolphin_ig[[k]], weights = E(dolphin_ig[[k]])$weight,
steps = 4, merges = TRUE, modularity = TRUE, membership = TRUE)
}
### End parallel processing
stopImplicitCluster()
})
## Modularity Q-value
modularity(dolphin_walk[[year]])
## Number of modules
groups(dolphin_walk[[year]])
## Membership of modules
membership(dolphin_walk[[year]])
## Save the edgelist into a new object
auxrand <- as.data.frame(el_years[[year]])
## Link weight distribution
auxrand$vw
## Save the edgelist into a new object
auxrand <- as.data.frame(el[[year]])
## Link weight distribution
auxrand$vw
## Save in the auxrand object
auxrand[,3] <- sample(auxrand$vw)
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
### Add link weights
E(igrand)$weight <- el[[year]][,3]
### Make undirected graph
igrand <- as.undirected(igrand)
## Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
## Calculate modularity Q-value
rmod <- walktrap.community(igrand)
modularity(rmod)
## Number of modules
groups(rmod)
## Membership of modules
membership(rmod)
# Difference from our empirical data?
modularity(dolphin_walk[[year]])
modularity(rmod)
# Run modularity permutations 1000 times
iter = 1000
randmod = numeric()
for(i in 1:iter){
# Save the edgelist into a new object
auxrand <- el[[year]]
# igraph format
igrand <- graph.edgelist(auxrand[,1:2]) # Create a network from the list of nodes
E(igrand)$weight <- auxrand[,3] # Add link weights
igrand <- as.undirected(igrand) # Make undirected graph
# Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity Q-value
rand_walk <- walktrap.community(igrand)
randmod[i] <- modularity(rand_walk) # Save Q-value into a vector
}
## Calculate the 95% confidence interval (two-tailed test)
ci = quantile(randmod, probs=c(0.025, 0.975), type=2)
## Compare with the empirical Q-value
data.frame(Q=modularity(dolphin_walk[[year]]), LowCI=ci[1], HighCI=ci[2])
## Visualization random Q distribution
hist(randmod, xlim=c(0.2,0.6))
### Empirical Q-value
abline(v= modularity(dolphin_walk[[year]]), col="red")
### 2.5% CI
abline(v= ci[1], col="blue")
### 97.5% CI
abline(v= ci[2], col="blue")
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig <- graph.edgelist(el_years[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el_years[[l]][,3])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
})
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig <- graph.edgelist(el[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el[[l]][,3])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
})
# Create an unweighted network
system.time({
registerDoParallel(n.cores)
dolp_ig <- list()
for (l in 1:length(years)) {
dolp_ig[[l]] <- graph.edgelist(el[[l]][,1:2])
# Add the edge weights to this network
E(dolp_ig[[l]])$weight <- as.numeric(el[[l]][,3])
# Create undirected network
dolp_ig[[l]] <- as.undirected(dolp_ig[[l]])
}
})
View(dolphin_ig)
# Newman's Q modularity
system.time({
registerDoParallel(n.cores)
newman <- list()
for (p in 1:length(years)) {
newman[[p]] <- cluster_leading_eigen(dolp_ig[[p]], steps = -1, weights = E(dolp_ig[[p]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
}
})
stopImplicitCluster()
newman <- cluster_leading_eigen(dolp_ig[[1]], steps = -1, weights = E(dolp_ig[[1]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
View(newman)
# Random color scheme
col <- rgb(runif(max(newman$membership)))
rgb(runif(max(newman$membership)))
max(newman$membership)
rgb(runif(max(newman$membership)))
# Random color scheme
col <- rgb(runif(max(newman$membership)), runif(max(newman$membership)), runif(max(newman$membership)))
col
# Assign a random color to individuals of each module ('module')
V(dolp_ig)$color <- NA
# Assign a random color to individuals of each module ('module')
V(dolp_ig[[year]])$color <- NA
for (i in 1:max(newman$membership)){
sample(col)
V(dolp_ig[[year]])$color[which(newman$membership==i)] = col[i]
}
plot(dolp_ig)
plot(dolp_ig[[year]])
# Test one year at a time
year <- 2
# Newman's Q modularity
newman <- cluster_leading_eigen(dolp_ig[[year]], steps = -1, weights = E(dolp_ig[[year]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
# Random color scheme
col <- rgb(runif(max(newman$membership)), runif(max(newman$membership)), runif(max(newman$membership)))
# Assign a random color to individuals of each module ('module')
V(dolp_ig[[year]])$color <- NA
for (i in 1:max(newman$membership)){
sample(col)
V(dolp_ig[[year]])$color[which(newman$membership==i)] = col[i]
}
plot(dolp_ig[[year]])
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## igraph format with weight
system.time({
registerDoParallel(n.cores)
dolphin_ig <- list()
for (j in 1:length(years)) {
dolphin_ig[[j]] <- graph.adjacency(as.matrix(nxn[[j]]),mode="undirected",weighted=TRUE,diag=FALSE)
}
### End parallel processing
stopImplicitCluster()
})
# Modularity by the WalkTrap algorithm
system.time({
registerDoParallel(n.cores)
dolphin_walk <- list()
for (k in 1:length(years)) {
dolphin_walk[[k]] <- cluster_walktrap(dolphin_ig[[k]], weights = E(dolphin_ig[[k]])$weight,
steps = 4, merges = TRUE, modularity = TRUE, membership = TRUE)
}
### End parallel processing
stopImplicitCluster()
})
## Save the edgelist into a new object
auxrand <- as.data.frame(el[[year]])
## Save in the auxrand object
auxrand[,3] <- sample(auxrand$vw)
# Calculate the modularity Q-value for a new permutated edge list
## Create a network from the list of nodes
igrand <- graph.edgelist(el[[year]][,1:2])
### Add link weights
E(igrand)$weight <- el[[year]][,3]
### Make undirected graph
igrand <- as.undirected(igrand)
## Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
## Calculate modularity Q-value
rmod <- walktrap.community(igrand)
# Difference from our empirical data?
modularity(dolphin_walk[[year]])
modularity(rmod)
# Run modularity permutations 1000 times
iter = 1000
randmod = numeric()
for(i in 1:iter){
# Save the edgelist into a new object
auxrand <- el[[year]]
# igraph format
igrand <- graph.edgelist(auxrand[,1:2]) # Create a network from the list of nodes
E(igrand)$weight <- auxrand[,3] # Add link weights
igrand <- as.undirected(igrand) # Make undirected graph
# Permutate the link weights
E(igrand)$weight <- sample(E(igrand)$weight)
# calculate the modularity Q-value
rand_walk <- walktrap.community(igrand)
randmod[i] <- modularity(rand_walk) # Save Q-value into a vector
}
## Calculate the 95% confidence interval (two-tailed test)
ci = quantile(randmod, probs=c(0.025, 0.975), type=2)
## Compare with the empirical Q-value
data.frame(Q=modularity(dolphin_walk[[year]]), LowCI=ci[1], HighCI=ci[2])
## Visualization random Q distribution
hist(randmod, xlim=c(0.2,0.6))
### Empirical Q-value
abline(v= modularity(dolphin_walk[[year]]), col="red")
### 2.5% CI
abline(v= ci[1], col="blue")
### 97.5% CI
abline(v= ci[2], col="blue")
# Test one year at a time
year <- 3
# Newman's Q modularity
newman <- cluster_leading_eigen(dolp_ig[[year]], steps = -1, weights = E(dolp_ig[[year]])$weight,
start = NULL, options = arpack_defaults, callback = NULL,
extra = NULL, env = parent.frame())
# Random color scheme
col <- rgb(runif(max(newman$membership)),
runif(max(newman$membership)),
runif(max(newman$membership)))
# Assign a random color to individuals of each module ('module')
V(dolp_ig[[year]])$color <- NA
for (i in 1:max(newman$membership)){
sample(col)
V(dolp_ig[[year]])$color[which(newman$membership==i)] = col[i]
}
plot(dolp_ig[[year]])
# Stop the cluster
stopCluster(cl)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
require(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(ncf) # For weights
require(vegan)
# Read file in to retain ILV
orig_data <- read.csv("sample_data.csv")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c("Date","Sighting","Code","Year")])
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c("Date","Sighting","Code","Year")])
group_data <- subset(group_data, subset=c(group_data$Code != "None"))
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
# Save nxn list
saveRDS(list_years_test, file="list_years.RData")
# Save nxn list
saveRDS(list_years, file="list_years.RData")
# Transforming SRI similarity into distance
year <- 1
dolp_dist = nxn[[year]] + 0.00001
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
year <- 1
dolp_dist = nxn[[year]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
dolp_dist
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Read file in to retain ILV
orig_data <- read.csv("sample_data.csv")
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
rawHI <- data.frame(rawHI)
# Take out the number of foraging events per ID
IDdata <- data.frame(Foraging = subset(IDbehav, IDbehav[,2] == "Feed"))[,c(1,3)]
## Add up the # of times each ID was seen in HI
IDdata$HI <- rawHI$Freq[rawHI[,2] != "0"]
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata[,3])/as.numeric(IDdata[,2])
# Only ID to prop
HIprop_ID <- na.omit(IDdata[,c(1, 4)])
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- t(HIprop_ID[,2])
as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
aux
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
aux
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav
IDbehav <- data.frame(IDbehav)
IDbehav
colnames(IDbehav) <- c("Code", "Foraging", "Freq")
IDbehav
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
rawHI
rawHI <- data.frame(rawHI)
rawHI
colnames(IDbehav) <- c("Code", "ConfHI", "Freq")
IDbehav
colnames(IDbehav) <- c("Code", "Foraging", "Freq")
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
# Take out the number of foraging events per ID
IDdata <- data.frame(Foraging = subset(IDbehav, IDbehav$Foraging == "Feed"))[,c(1,3)]
IDdata
rawHI
## Add up the # of times each ID was seen in HI
IDdata$HI <- rawHI$Freq[rawHI$ConfHI != "0"]
IDdata
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
# Only ID to prop
HIprop_ID <- na.omit(IDdata[,c(1, 4)])
HIprop_ID
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- t(HIprop_ID$HIprop)
as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- t(HIprop_ID$HIprop)
dissimilarity <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
dissimilarity
HIprop_ID$HIprop
t(HIprop_ID$HIprop)
dim(fake_HIprop)
HIprop_ID
dissimilarity <- as.matrix(vegdist(HIprop_ID, method = 'euclidean'))
