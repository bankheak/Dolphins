# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
aux <- subset(aux, aux$Foraging == "Feed")
aux$ConfHI <- ifelse(aux$ConfHI == "0", 0, 1)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)[,c(1,3)]
colnames(IDbehav) <- c("Code", "Forg_Freq")
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI))
rawHI <- data.frame(rawHI)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
rawHI
## Add up the # of times each ID was seen in HI
IDbehav$HI <- rawHI$Freq[rawHI$ConfHI != 0]
IDdata <- IDbehav
colnames(IDdata) <- c("Code", "Foraging", "HI")
IDdata
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
# Only ID to prop
HIprop_ID <- na.omit(IDdata[,c(1, 4)])
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
View(dissimilarity_HI)
?mantel.rtest
class(dissimilarity_HI)
dissimilarity_HI <- as.dist(dissimilarity_HI)
class(dissimilarity_HI)
kov <- readRDS("kov.RDS")
class(kov)
kov <- as.dist(kov)
class(kov)
class(dolp_dist)
# Dissimilarity matrix
mantel.rtest(kov, dolp_dist, nrepet=999)
nrow(list_years[[year]])
length(unique(list_years[[year]]$Code)))
length(unique(list_years[[year]]$Code))
HIprop_ID
# Select variables from the raw data
aux <- sample_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
aux <- subset(aux, aux$Foraging == "Feed")
aux$ConfHI <- ifelse(aux$ConfHI == "0", 0, 1)
nrow(list_years[[year]])
# Select variables from the raw data
aux <- sample_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
aux$ConfHI <- ifelse(aux$ConfHI == "0", 0, 1)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)[,c(1,3)]
colnames(IDbehav) <- c("Code", "Forg_Freq")
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI))
rawHI <- data.frame(rawHI)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
rawHI
## Add up the # of times each ID was seen in HI
IDbehav$HI <- rawHI$Freq[rawHI$ConfHI != 0]
IDdata <- IDbehav
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
# Only ID to prop
HIprop_ID <- na.omit(IDdata[,c(1, 4)])
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
mantel.rtest(dissimilarity_HI, dolp_dist, nrepet=999)
View(IDdata)
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
HIprop_ID[is.na(HIprop_ID)] <- 0
HIprop_ID[is.na(HIprop_ID$HIprop)] <- 0
View(HIprop_ID)
is.na(HIprop_ID$HIprop)
sum(is.na(HIprop_ID))
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
mantel.rtest(dissimilarity_HI, dolp_dist, nrepet=999)
View(list_years)
# Make sure all matrices have the same dimensions
length(unique(list_years[[year]]$Code))
length(unique(HIprop_ID$Code))
# Select variables from the raw data
aux <- sample_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
length(unique(aux$Code))
# Select variables from the raw data
aux <- sample_data[, c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
length(unique(aux$Code))
# Select variables from the raw data
aux <- list_years[[year]][, c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
list_years[[year]]
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Save list
saveRDS(list_threeyears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
year <- 5
list_years[[year]]
# Select variables from the raw data
data <- list_years[[year]]
aux <- data[, c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
length(unique(aux$Code))
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
#aux <- subset(aux, aux$Foraging == "Feed")
aux$ConfHI <- ifelse(aux$ConfHI == "0", 0, 1)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)[,c(1,3)]
colnames(IDbehav) <- c("Code", "Forg_Freq")
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI))
rawHI <- data.frame(rawHI)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
## Add up the # of times each ID was seen in HI
IDbehav$HI <- rawHI$Freq[rawHI$ConfHI != 0]
IDdata <- IDbehav
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time FOraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
HIprop_ID[is.na(HIprop_ID$HIprop)] <- 0
View(HIprop_ID)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
is.na(HIprop_ID$HIprop
is.na(HIprop_ID$HIprop)
is.na(IDdata)
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
IDdata[is.na(IDdata$HIprop)] <- 0
is.na(IDdata$HIprop)
IDdata[is.na(IDdata)] <- 0
is.na(IDdata)
sum(is.na(IDdata$HIprop))
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
sum(is.na(HIprop_ID$HIprop))
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
sum(is.na(dissimilarity_HI))
is.na(dissimilarity_HI)==1
is.na(dissimilarity_HI)==TRUE
sum(is.na(fake_HIprop))
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
View(dissimilarity_HI)
dissimilarity_HI[is.na(dissimilarity_HI)] <- 0
sum(is.na(dissimilarity_HI))
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
dissimilarity_HI <- as.matrix(vegdist(fake_HIprop, method = 'euclidean'))
dissimilarity_HI[is.na(dissimilarity_HI)] <- 0
View(dissimilarity_HI)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(dist(as.matrix(fake_HIprop), method = "euclidean"))
is.na(dissimilarity_HI)
sum(is.na(dissimilarity_HI))
dissimilarity_HI[is.na(dissimilarity_HI)] <- 0
class(dissimilarity_HI)
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
dissimilarity_HI <- as.matrix(dist(as.matrix(HIprop_ID), method = "euclidean"))
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(dist(as.matrix(fake_HIprop), method = "euclidean"))
View(IDdata)
length(unique(IDdata$Code))
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- data.frame(IDbehav)[,c(1,3)]
View(IDbehav)
View(IDbehav)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- as.data.frame(IDbehav[,c(1,3)], stringsAsFactors = FALSE)
IDbehav[,c(1,3)]
IDbehav
IDbehav <- as.data.frame(IDbehav, stringsAsFactors = FALSE)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- as.data.frame(IDbehav, stringsAsFactors = FALSE)
IDbehav <- IDbehav[,c(1,3)]
colnames(IDbehav) <- c("Code", "Forg_Freq")
# Group by the 'Code' column and sum the frequencies
IDbehav <- aggregate(. ~ Code, data = IDbehav, sum)
View(IDbehav)
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI))
rawHI <- as.data.frame(rawHI, stringsAsFactors = FALSE)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
# Group by the 'Code' column and sum the frequencies
rawHI <- aggregate(. ~ Code, data = rawHI, sum)
View(rawHI)
View(rawHI)
## Add up the # of times each ID was seen in HI
IDbehav$HI <- rawHI$Freq[rawHI$ConfHI != 0]
IDdata <- IDbehav
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time Foraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
IDdata[is.na(IDdata)] <- 0
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
IDdata[,c(1, 4)]
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
HIprop_ID
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(dist(as.matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[is.na(dissimilarity_HI)] <- 0
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
mantel.rtest(dissimilarity_HI, dolp_dist, nrepet=999)
# Dissimilarity matrix
mantel.rtest(kov, dolp_dist, nrepet=999)
kov
kov <- readRDS("kov.RDS")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
## Format date and year
coord_data$Date <- as.Date(as.character(coord_data$Date), format="%Y-%m-%d")
## Give descriptive names
colnames(coord_data) <- c("date", "y", "x", "id", "year", "HI")
# Seperate map per 3 years
coord_data$ThreeYearIncrement <- cut(coord_data$year, breaks = seq(min(coord_data$year), max(coord_data$year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(coord_data, coord_data$ThreeYearIncrement)
# Test one year at a time
coord_data <- list_threeyears[[5]]
list_years <- readRDS("list_years.RData")
# # Test one year at a time
year <- 5
coord_data <- list_years[[year]]
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
## Format date and year
coord_data$Date <- as.Date(as.character(coord_data$Date), format="%Y-%m-%d")
## Give descriptive names
colnames(coord_data) <- c("date", "y", "x", "id", "year", "HI")
is.na(coord_data$x)
sum(is.na(coord_data$x))
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
sum(is.na(coord_data$StartLat))
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
sum(is.na(orig_data$StartLat))
# Make sure every ID has >10 obs
ID <- unique(orig_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(orig_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(orig_data, orig_data$Code %in% c(sub$ID))
write.csv(sample_data, "sample_data.csv")
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Save list
saveRDS(list_threeyears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c("Date","Sighting","Code","Year")])
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Gambit of the group index
gbi <- list()
for (y in seq_along(list_years)) {
gbi[[y]] <- get_group_by_individual(list_years[[y]][,c("Code", "Group")], data_format = "individuals")
}
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Gambit of the group index
gbi <- list()
for (y in seq_along(list_years)) {
gbi[[y]] <- get_group_by_individual(list_years[[y]][,c("Code", "Group")], data_format = "individuals")
}
View(list_years)
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c("Date","Sighting","Code","Year")])
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Gambit of the group index
gbi <- list()
for (y in seq_along(list_years)) {
gbi[[y]] <- get_group_by_individual(list_years[[y]][,c("Code", "Group")], data_format = "individuals")
}
seq_along(list_years)
# Make a list of three years per dataframe
group_data$ThreeYearIncrement <- cut(group_data$Year, breaks = seq(min(group_data$Year), max(group_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(group_data, group_data$ThreeYearIncrement)
# Gambit of the group index
gbi <- list()
for (y in seq_along(list_threeyears)) {
gbi[[y]] <- get_group_by_individual(list_threeyears[[y]][,c("Code", "Group")], data_format = "individuals")
}
saveRDS(gbi, file="gbi.RData")
# SIMPLE-RATIO INDEX ------------------------------------------------------
#' @description This function creates an association matrix using the simple-ratio index (SRI).
#' @param matr A binary matrix depicting individuals in the columns and groups in the rows
#' @return A square matrix in which each cell is an estimate of a dyadic social relationship, from 0 (never seen in the same group) to 1 (always seen in the same group)
SRI.func <-  function (matr) {
if (any(is.na(matr))) {
matr <- na.omit(matr)
cat("The data matrix contains NA, and have been removed.\n")
}
matr1 = matr
N <- nrow(matr1)
matr1[matr1 > 1] <- 1
n <- apply(matr1, 2, sum)
tmatr <- t(matr1)
df <- as.matrix(t(matr))
a <- df %*% t(df) # Dyad in same group
b <- df %*% (1 - t(df)) # A present, B absent
c <- (1 - df) %*% t(df) # A absent, B present
d <- ncol(df) - a - b - c # Double absent
Dice <- data.frame()
for (i in 1:nrow(a)) {
for (j in 1:ncol(a)) {
Dice[i, j] <- a[i, j]/(a[i, j] + b[i, j] + c[i, j])
}
}
rownames(Dice)=colnames(Dice)=colnames(matr)
Dice
}
# NULL PERMUTATIONS -------------------------------------------------------
#' @description shuffles binary matrices under different restrictions.
#' @param mat A quantitative matrix
#' @param iter Number of random matrices to be created
#' @param model Function to be chosen.
#' @param ... Further arguments from \code{permatswap} or \code{permatfull}
#' @return a list with \code{iter} random matrices
#' @details Totally restricted null model is called. Cell values are permuted restricting all features of the original matrix: column sums, row sums, matrix fill and total sum.
#' @references \code{citation("vegan")}
null <- function (mat, iter, ...){
require(vegan)
aux <- permatswap(mat, times=iter, method="quasiswap", fixedmar="both", shuffle="both", mtype="prab")
return(aux$perm)
}
n.cores <- detectCores()
# Run multiple cores for faster computing
require(doParallel)
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
# Eliminate IDs with less than 5 locations
ID <- list()
for (i in seq_along(list_threeyears)) {
ID[[i]] <- unique(list_threeyears[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect [j] <- sum(coord_data$id == ID[[i]][j])
}
sub <- data.frame(ID[[i]], obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_threeyears[[i]] <- subset(list_threeyears[[i]], list_threeyears[[i]]$Code %in% c(sub$ID))}
View(list_threeyears)
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
ID <- list()
for (i in seq_along(list_threeyears)) {
ID[[i]] <- unique(list_threeyears[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect [j] <- sum(coord_data$id == ID[[i]][j])
}
sub <- data.frame(ID[[i]], obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_threeyears[[i]] <- subset(list_threeyears[[i]], list_threeyears[[i]]$Code %in% c(sub$ID))}
View(list_threeyears)
# Eliminate IDs with less than 5 locations
ID <- list()
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
seq_along(list_threeyears)
unique(list_threeyears[[1]]$Code)
# Eliminate IDs with less than 5 locations
ID <- list()
ID[[1]] <- unique(list_threeyears[[1]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect [j] <- sum(coord_data$id == ID[[i]][j])
}
seq_along(ID)
length(ID[[i]])
length(ID[[1]])
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
ID <- list()
for (i in seq_along(list_threeyears)) {
ID[[i]] <- unique(list_threeyears[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect[j] <- sum(list_threeyears[[i]]$Code == ID[[i]][j])
}
sub <- data.frame(ID = ID[[i]], obs_vect = obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_threeyears[[i]] <- subset(list_threeyears[[i]], list_threeyears[[i]]$Code %in% c(sub$ID))}
View(list_threeyears)
# Save list
saveRDS(list_threeyears, file="list_years.RData")
list_years <- readRDS("list_years.RData")
# Calculate Gambit of the group
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data <- list()
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi <- list()
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
cbind(list_years[[1]][,c("Date","Sighting","Code","Year")])
group_data <- cbind(list_years[[1]][,c("Date","Sighting","Code","Year")])
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Calculate Gambit of the group
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data <- list()
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi <- list()
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
View(gbi)
# Calculate Gambit of the group
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
View(gbi)
saveRDS(gbi, file="gbi.RData")
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
# Save nxn list
saveRDS(nxn, file="nxn.RData")
gc()
