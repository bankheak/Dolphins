list_sexage_threeyears <- split(sample_sexage_data, sample_sexage_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
ID <- list()
for (i in seq_along(list_years)) {
ID[[i]] <- unique(list_years[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[[i]][j])
}
sub <- data.frame(ID = ID[[i]], obs_vect = obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_years[[i]] <- subset(list_years[[i]], list_years[[i]]$Code %in% c(sub$ID))}}
list_threeyears <- sub_locations(list_threeyears)
list_sexage_threeyears <- sub_locations(list_sexage_threeyears)
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Match ID to sex and age data
ILV <- read.csv("Individuals_Residency_Analysis.csv")
## Sex
ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
## Sex
ID_sex <- setNames(ILV$Sex, ILV$Alias)
orig_data$Sex <- ID_sex[orig_data$Code]
## Age
ID_birth <- setNames(ILV$BirthYear, ILV$Alias)
orig_data$Birth <- ID_birth[orig_data$Code]
orig_data$Age <- as.numeric(orig_data$Year) - as.numeric(orig_data$Birth)
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
sample_data <- subset(orig_data, subset=c(orig_data$StartLat != 999))
# Get rid of data with no sex or age data
sample_sexage_data <- sample_data[!is.na(sample_data$Sex) & !is.na(sample_data$Age),]
write.csv(sample_data, "sample_data.csv")
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Make a list of three years per dataframe for sex and age data
sample_sexage_data$ThreeYearIncrement <- cut(sample_sexage_data$Year, breaks = seq(min(sample_sexage_data$Year), max(sample_sexage_data$Year) + 3, by = 3), labels = FALSE)
list_sexage_threeyears <- split(sample_sexage_data, sample_sexage_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
ID <- list()
for (i in seq_along(list_years)) {
ID[[i]] <- unique(list_years[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[[i]][j])
}
sub <- data.frame(ID = ID[[i]], obs_vect = obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_years[[i]] <- subset(list_years[[i]], list_years[[i]]$Code %in% c(sub$ID))}}
list_threeyears <- sub_locations(list_threeyears)
list_sexage_threeyears <- sub_locations(list_sexage_threeyears)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(list_years)
}
list_threeyears <- sub_locations(list_threeyears)
list_sexage_threeyears <- sub_locations(list_sexage_threeyears)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Make a list of three years per dataframe for sex and age data
sample_sexage_data$ThreeYearIncrement <- cut(sample_sexage_data$Year, breaks = seq(min(sample_sexage_data$Year), max(sample_sexage_data$Year) + 3, by = 3), labels = FALSE)
list_sexage_threeyears <- split(sample_sexage_data, sample_sexage_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
sub_locations <- function(list_years) {
updated_list_years <- list()  # Initialize an empty list to store the updated datasets
for (i in seq_along(list_years)) {
ID <- unique(list_years[[i]]$Code)
obs_vect <- numeric(length(ID))
for (j in seq_along(ID)) {
obs_vect[j] <- sum(list_years[[i]]$Code == ID[j])
}
sub <- data.frame(ID = ID, obs_vect = obs_vect)
sub <- subset(sub, subset = obs_vect > 10)
updated_list_years[[i]] <- subset(list_years[[i]], Code %in% sub$ID)
}
return(updated_list_years)
}
list_threeyears <- sub_locations(list_threeyears)
list_sexage_threeyears <- sub_locations(list_sexage_threeyears)
# Save list
saveRDS(list_sexage_threeyears, file="list_sexage_years.RData")
saveRDS(list_threeyears, file="list_years.RData")
list_sexage_years <- readRDS("list_sexage_years.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
kov <- readRDS("kov.RDS")  # Home range overlap
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_sexage_years <- readRDS("list_sexage_years.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Calculate Gambit of the group
create_gbi <- function(list_years) {
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
return(gbi)
}
gbi <- create_gbi(list_sexage_years)
gbi_sexage <- create_gbi(list_sexage_years)
# Create association matrix
create_nxn <- function(list_years) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn_sexage <- create_nxn(list_sexage_years)
# Create association matrix
create_nxn <- function(list_years, gbi) {
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
return(nxn)
}
nxn_sexage <- create_nxn(list_sexage_years, gbi_sexage)
saveRDS(nxn_sexage, file="nxn_sexage.RData")
# Sex similarity matrix
sex_list <- lapply(list_sexage_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
return(sex_matrix)
})
# Read file in to retain ILV
kov <- readRDS("kov.RDS")  # Home range overlap
nxn_sexage <- readRDS("nxn_sexage.RData")
# Read in social association matrix and listed data
nxn <- readRDS("nxn.RData")
list_sexage_years <- readRDS("list_sexage_years.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Sex similarity matrix
sex_list <- lapply(list_sexage_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
return(sex_matrix)
})
# Age similarity matrix
age_list <- lapply(list_sexage_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
age_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
age_matrix[i, j] <- abs(df$Age[i] - df$Age[j])
}
}
return(age_matrix)
})
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
library(ncf) # For weights
library(vegan)
library(igraph) # graph_adj
require(asnipe) # mrqap.dsp
library(assortnet) # associative indices
library(ggplot2)
library(doParallel)
# Read file in to retain ILV
kov <- readRDS("kov.RDS")  # Home range overlap
nxn_sexage <- readRDS("nxn_sexage.RData")
list_sexage_years <- readRDS("list_sexage_years.RData")
# Read in social association matrix and listed data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
dolp_dist <- lapply(nxn, function(df) {
df + 0.00001
1 - df
## Remove the redundant cells and the diagonal
as.dist(df)
})
# Sex similarity matrix
sex_list <- lapply(list_sexage_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
sex_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
if (df$Sex[i] == df$Sex[j]) {
sex_matrix[i, j] <- 1  # Same sex
} else {
sex_matrix[i, j] <- 0  # Different sex
}
}
}
return(sex_matrix)
})
# Age similarity matrix
age_list <- lapply(list_sexage_years, function(df) {
## Empty matrix to store sex similarity
num_ID <- length(unique(df$Code))
age_matrix <- matrix(NA, nrow = num_ID, ncol = num_ID, dimnames = list(unique(df$Code), unique(df$Code)))
# Fill in similarity of sex
for (i in 1:num_ID) {
for (j in 1:num_ID) {
age_matrix[i, j] <- abs(df$Age[i] - df$Age[j])
}
}
return(age_matrix)
})
# Extract specific columns from each data frame in list_years
aux_data <- function(list_years) {
aux <- lapply(list_years, function(df) {
data.frame(
Code = df$Code,
Behaviors = df$Behaviors,
HumanInteraction = df$HumanInteraction,
ConfHI = df$ConfHI)})
# Add the 'Foraging' variable to each data frame in the 'aux' list
aux <- lapply(aux, function(df) {
df$Foraging <- "Other"
df$Foraging[grepl(pattern = 'Feed', x = df$Behaviors, ignore.case = FALSE)] <- "Feed"
df
})
return(aux)
}
aux <- aux_data(list_years)
aux_sexage <- aux_data(list_sexage_years)
# Categorize ID to Foraging
ID_forg <- function(aux_data) {
IDbehav <- lapply(aux, function(df) {
df <- table(df$Code, df$Foraging)
df <- as.data.frame(df, stringsAsFactors = FALSE)
df <- df[, c(1, 3)]
colnames(df) <- c("Code", "Forg_Freq")
df <- aggregate(. ~ Code, data = df, sum)
df
})
return(IDbehav)
}
IDbehav <- ID_forg(aux)
IDbehav_sexage <- ID_forg(aux_sexage)
# Clump all the HI behaviors together------------------------------------------
clump_behav <- function(aux_data) {
for (i in seq_along(aux_data)) {
aux_data[[i]]$ConfHI <- ifelse(aux_data[[i]]$ConfHI != "0", 1, 0)}
# Categorize ConfHI to IDs
rawHI <- lapply(aux_data, function(df) {
# Sum up the frequencies of HI by code
aggregated_df <- aggregate(ConfHI ~ Code, data = df, sum)
unique_codes_df <- data.frame(Code = unique(df$Code))
# Merge the unique codes data frame with the aggregated data frame
merged_df <- merge(unique_codes_df, aggregated_df, by = "Code", all.x = TRUE)
# Fill missing Freq values (if any) with 0
merged_df$ConfHI[is.na(merged_df$ConfHI)] <- 0
return(merged_df)
})
return(rawHI)
}
rawHI <- clump_behav(aux)
rawHI_sexage <- clump_behav(aux_sexage)
# Get HI Freq
create_IDbehav_HI <- function(IDbehav){
IDbehav_HI <- lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HI <- rawHI[[i]]$ConfHI
colnames(df) <- c("Code", "Foraging", "HI")
df
})
return(IDbehav_HI)
}
IDbehav_HI <- create_IDbehav_HI(IDbehav)
IDbehav_HI_sexage <- create_IDbehav_HI(IDbehav_sexage)
# Proportion of time Foraging spent in HI
Prop_HI <- function(IDbehav) {
lapply(seq_along(IDbehav), function(i) {
df <- IDbehav[[i]]
df$HIprop <- as.numeric(df$HI) / as.numeric(df$Foraging)
df$HIprop[is.na(df$HIprop)] <- 0
# Keep only 'Code' and 'HIprop' columns
df <- df[, c('Code', 'HIprop')]
df
})
}
prob_HI <- Prop_HI(IDbehav_HI)
prop_HI_sexage <- Prop_HI(IDbehav_HI_sexage)
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
dis_matr <- function(Prop_HI) {
dissimilarity_HI <- list()
for (i in seq_along(Prop_HI)) {
fake_HIprop <- Prop_HI[[i]]$HIprop
dissimilarity_HI[[i]] <- as.matrix(dist(matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[[i]][is.na(dissimilarity_HI[[i]])] <- 0
#dissimilarity_HI[[i]] <- as.dist(dissimilarity_HI[[i]]) # HI dissimilarity
}
dissimilarity_HI
}
dist_HI <- dis_matr(prob_HI)
dist_HI_sexage <- dis_matr(prop_HI_sexage)
# Set a number of permutations and year
year <- 5
# Set a number of permutations and year
year <- 5
Nperm <- 1000
# Make a function that calculates all of the following code with and without sex and age
create_coord_data <- function(list_years) {
# Make a list of years
coord_data_list <- list_years
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
# Extract IDs and coordinates
ids <- df$Code
coordinates <- df[, c("StartLon", "StartLat")]
# Convert to data frame
ids_df <- data.frame(id = ids)
# Create a SpatialPointsDataFrame with coordinates
coords_sp <- SpatialPointsDataFrame(coords = coordinates, data = ids_df)
# Set CRS and transform to UTM
proj4string(coords_sp) <- CRS("+proj=longlat +datum=WGS84")
coords_sp_utm <- spTransform(coords_sp, CRS("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
})
return(dolph.sp)
}
dolph.sp <- create_coord_data(list_years)
dolph.sp_sexage <- create_coord_data(list_sexage_years)
# Visualize data extent
create.sf <- function(dolph.sp) {
dolph.sf <- lapply(dolph.sp, function (df) {st_as_sf(df)})
ggplot(dolph.sf[[3]]) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
return(dolph.sf)
}
dolph.sf <- create.sf(dolph.sp)
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(rgdal) #
# Visualize data extent
create.sf <- function(dolph.sp) {
dolph.sf <- lapply(dolph.sp, function (df) {st_as_sf(df)})
ggplot(dolph.sf[[3]]) +
geom_sf(aes(color = "Data Points"), size = 2, alpha = 0.5) +
theme_bw() +
labs(title = "Distribution of Data Points") +
scale_color_manual(values = c("Data Points" = "blue"))
return(dolph.sf)
}
dolph.sf <- create.sf(dolph.sp)
dolph.sf_sexage <- create.sf(dolph.sp_sexage)
# Calculate kernel values
create_kernel <- function(dolph.sp) {
kernel <- lapply(dolph.sp, function(sp_obj) {
kernelUD(sp_obj, h = 10000)
})
return(kernel)
}
kernel <- create_kernel(dolph.sp)
kernel_sexage <- create_kernel(dolph.sp_sexage)
# Calculate kernel overlap values
create_kov <- function(kernel) {
kov <- lapply(kernel, function(kern) {
kerneloverlaphr(kern, method = "HR", lev = 95)
})
return(kov)
}
kov <- create_kov(kernel)
kov_sexage <- create_kov(kernel_sexage)
## Without sex and age included
mrqap_full <- mrqap.dsp(nxn[[year]] ~ kov[[year]] + dist_HI[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
## Without sex and age included and with behaviors divided
mrqap_sepHI <- mrqap.dsp(nxn[[year]] ~ kov[[year]] + dist_HI[[year]] +
dist_Beg[[year]] + dist_Dep[[year]] + dist_Pat[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
## With sex and age included
mrqap_sexage <- mrqap.dsp(nxn_sexage[[year]] ~ kov_sexage[[year]] +
dist_HI_sexage[[year]],
randomisations = Nperm,
intercept = FALSE,
test.statistic = "beta")
View(dist_HI_sexage)
