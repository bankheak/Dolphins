cv_null <- rep(NA,reps)
foreach(i = 1:reps,
.combine = c) %dopar% {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100}
# remove NAs, if any
cv_null = cv_null[!is.na(cv_null)]
stopImplicitCluster()
# Calculate 95% confidence interval, in a two-tailed test
cv_ci = quantile(cv_null, probs=c(0.025, 0.975), type=2)
# histogram of null CVs
hist(cv_null,
breaks=50,
col='grey70',
main = 'Restrictive null model',
xlab="Null CV SRI")
cv_null
foreach(i = 1:reps,
.combine = c) %dopar% {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100}
#' Calculate the association and CV for each of the 1000 permuted matrices to
#' create null distribution
cv_null <- rep(NA,reps)
foreach(i = 1:reps,
.combine = c) %dopar% {
sri_null = as.matrix(SRI.func(nF[[i]]))
cv_null[i] <- ( sd(sri_null) / mean(sri_null) ) * 100}
require(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(ncf) # For weights
# Read in social association matrix
gbi<- read.csv("gbi.csv")
source("../code/functions.R") # SRI & null permutation
nxn<- SRI.func(gbi)
# Transforming SRI similarity into distance
dolp_dist = nxn + 0.00001
dolp_dist <- 1-nxn
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
## Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
## Group each individual by date and sighting
area_vect <- cbind(orig_data[,c(8,17)]) # Seperate subarea and ID
area_vect <- rbind(area_vect[1:test,]) # To check that calculations are correct
area_vect <- transform(area_vect, Area = as.numeric(factor(Subarea)))
area_vect <- cbind(area_vect[,c(2,3)]) # Seperate subarea and Code
area_vect <- as.vector(area_vect$Area)
axi<- get_group_by_individual(area_vect, data_format = "individuals")
## Group each individual by date and sighting
area_vect <- cbind(orig_data[,c(8,17)]) # Seperate subarea and ID
test <- 100
area_vect <- rbind(area_vect[1:test,]) # To check that calculations are correct
area_vect <- transform(area_vect, Area = as.numeric(factor(Subarea)))
area_vect <- cbind(area_vect[,c(2,3)]) # Seperate subarea and Code
axi<- get_group_by_individual(area_vect, data_format = "individuals")
## Group each individual by date and sighting
area_vect <- cbind(orig_data[,c(8,17)]) # Seperate subarea and ID
test <- 100
area_vect <- rbind(area_vect[1:test,]) # To check that calculations are correct
## Group each individual by date and sighting
area_vect <- cbind(orig_data[,c(8,17)]) # Seperate subarea and ID
View(area_vect)
View(orig_data)
# Read file in to retain ILV
orig_data <- read.csv("secondgen_data.csv")
View(orig_data)
## Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
View(orig_data)
# Read file in
orig_data<- read.csv("firstgen_data.csv")
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Use only one year
sample_data <- subset(orig_data, subset=c(orig_data$Year == 1993))
# Make sure every ID has >10 obs
ID <- unique(sample_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(sample_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(sample_data, sample_data$Code %in% c(sub$ID))
write.csv(sample_data, "sample_data.csv")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Gambit of the group index
gbi<- get_group_by_individual(group_data, data_format = "individuals")
write.csv(gbi, "gbi.csv")
# Create association matrix
source("../code/functions.R") # SRI & null permutation
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn<- SRI.func(gbi)
})
nxn<-as.matrix(nxn)
# end parallel processing
stopImplicitCluster()
# Read files in
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
# Make date into a date class
c(firstgen_data, secondgen_data)$Date <- as.Date(as.character(firstgen_data$Date), format="%d-%b-%y")
orig_data <- rbind(firstgen_data, secondgen_data)
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
length(orig_data)
length(orig_data[i,])
length(orig_data$Year)
length(unique(orig_data$Year))
length(1993:2014)
# Make a list of only one year per dataframe
sample_data <- list()
for (i in 1:length(unique(orig_data$Year))) {
for (j in 1993:2014) {
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == j))
}
}
View(sample_data)
length(unique(orig_data$Year)
length(unique(orig_data$Year))
# Make a list of only one year per dataframe
sample_data <- list()
for (j in 1993:2014) {
sample_data[[]] <- subset(orig_data, subset=c(orig_data$Year == j))
}
for (j in 1993:2014) {
sample_data[[j]] <- subset(orig_data, subset=c(orig_data$Year == j))
}
View(orig_data)
View(sample_data)
# Make a list of only one year per dataframe
sample_data <- list()
for (i in 1:length(unique(orig_data$Year))) {
for (j in 1993:2014) {
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == j))
}                    }
View(sample_data)
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == 1993))
View(sample_data)
sample_data <- subset(orig_data, subset=c(orig_data$Year == 1993))
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == j))
View(sample_data)
# Make a list of only one year per dataframe
sample_data <- list()
for (i in 1:length(unique(orig_data$Year))) {
for (j in 1993:2014) {
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == j))
}                    }
# Make sure every ID has >10 obs
ID <- unique(orig_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(orig_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(orig_data, orig_data$Code %in% c(sub$ID))
write.csv(sample_data, "sample_data.csv")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:4]) # Subset ID and group #
# Make a list of only one year per dataframe
sample_data <- list()
for (i in 1:length(unique(orig_data$Year))) {
for (j in 1993:2014) {
sample_data[[i]] <- subset(orig_data, subset=c(orig_data$Year == j))
}                    }
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17)]) # Seperate date, group and ID
sample_data[[1]][,c(2,11,17)]
# Group each individual by date and sighting
group_data <- cbind(sample_data[[]][,c(2,11,17)]) # Seperate date, group and ID
# Group each individual by date and sighting
group_data <- cbind(sample_data[[1:22]][,c(2,11,17)]) # Seperate date, group and ID
# Group each individual by date and sighting
group_data <- cbind(sample_data[[c(1:22)]][,c(2,11,17)]) # Seperate date, group and ID
# Group each individual by date and sighting
group_data <- cbind(sample_data[[1]][,c(2,11,17)]) # Seperate date, group and ID
sample_data[[c(1:10)]]
sample_data[[1:10]]
sample_data[[1]]
sample_data[[1,2]]
# Make a list of only one year per dataframe
years <- length(unique(orig_data$Year))
View(group_data)
# Group each individual by date and sighting
group_data <- list()
for (i in 1:years) {
group_data[[i]] <- cbind(sample_data[[i]][,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:4]) # Subset ID and group #
}
# Group each individual by date and sighting
group_data <- list()
for (i in 1:years) {
group_data[[i]] <- cbind(sample_data[[i]][,c(2,11,17)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,c(3:4)]) # Subset ID and group #
}
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Make sure every ID has >10 obs
ID <- unique(orig_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(orig_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(orig_data, orig_data$Code %in% c(sub$ID))
write.csv(sample_data, "sample_data.csv")
View(sample_data)
View(orig_data)
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data <- rbind(firstgen_data, secondgen_data)
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Make sure every ID has >10 obs
ID <- unique(orig_data$Code)
obs_vect <- NULL
for (i in 1:length(ID)) {
obs_vect[i]<- sum(orig_data$Code == ID[i])
}
sub <- data.frame(ID, obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
sample_data <- subset(orig_data, orig_data$Code %in% c(sub$ID))
write.csv(sample_data, "sample_data.csv")
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17,21)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
View(group_data)
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Make a list of only one year per dataframe
years <- length(unique(group_data$Year))
years
group_data <- list()
for (i in 1:years) {
for (j in 1993:2014) {
group_data[[i]] <- subset(group_data, subset=c(group_data$Year == j))
}                    }
View(group_data)
unique(group_data$Year)
# Group each individual by date and sighting
group_data <- cbind(sample_data[,c(2,11,17,21)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
unique(group_data$Year)
# Make a list of only one year per dataframe
years <- length(unique(group_data$Year))
years
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
View(list_years)
View(list_years)
list_years[[1]]
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
group_data <- list_years[[i]][,c(1, 3)]
}
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
View(list_years)
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
group_data <- cbind(sample_data[,c(2,11,17,21)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
years
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
years
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
for (j in 1:length(years)) {
group_data[[i]] <- list_years[[i]][,c(1, 3)]
}
list_years[[1]][,c(1, 3)]
## Test a smaller amount of data for faster results
year <- c(1:22)
group_data <- list_years[[year]][,c(1, 3)]
group_data <- list_years[[year]][,c(1, 3)]
list_years <- list()
years
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
## Test a smaller amount of data for faster results
year <- 1
group_data <- list_years[[year]][,c(1, 3)]
list_years[[i]][1:test,c(1, 3)]
first <- list_years[[1]][,c(1, 3)]
class(first)
# Gambit of the group index
gbi<- get_group_by_individual(list_years[[i]][1:test,c(1, 3)], data_format = "individuals")
# Gambit of the group index
for (y in 1:length(years)) {
gbi<- get_group_by_individual(list_years[[y]][,c(1, 3)], data_format = "individuals")
}
View(gbi)
# Gambit of the group index
gbi <- list()
# Gambit of the group index
gbi <- list()
for (y in 1:length(years)) {
gbi[[y]] <- get_group_by_individual(list_years[[y]][,c(1, 3)], data_format = "individuals")
}
View(gbi)
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in 1:length(years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
})
# end parallel processing
stopImplicitCluster()
# Save list
saveRDS(nxn, file="nxn.RData")
# Read in social association matrix and data
readRDS("nxn.RData")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
# Transforming SRI similarity into distance
dolp_dist = nxn + 0.00001
# Transforming SRI similarity into distance
year <- 1
dolp_dist = nxn[[1]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
# Select variables from the raw data
aux = orig_data[1:1000,
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Read file in to retain ILV
orig_data <- read.csv("sample_data.csv")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
# Transforming SRI similarity into distance
year <- 1
dolp_dist = nxn[[year]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
# Select variables from the raw data
aux = orig_data[1:1000,
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging = "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Group each individual by date and sighting
group_data <- cbind(orig_data[,c(2,11,17,21)]) # Seperate date, group and ID
group_data$Group <- cumsum(!duplicated(group_data[1:2])) # Create sequential group # by date
group_data <- cbind(group_data[,3:5]) # Subset ID and group #
# Make a list of only one year per dataframe
years <- unique(group_data$Year)
list_years <- list()
for (i in 1:length(years)) {
list_years[[i]] <- subset(group_data, subset=c(group_data$Year == years[i]))
}
View(aux)
IDbehav = table(aux$Code, aux$Foraging)
IDbehav
# Categorize ID to Foraging
rawHI = as.matrix(table(aux$Code, aux$ConfHI)[,2:6])
View(aux)
table(aux$Code, aux$ConfHI)
table(aux$Code, aux$ConfHI)[,2:6]
# Categorize ID to Foraging
rawHI = as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
rawHI
IDbehav
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = IDbehav[,1])
IDdata
rawHI
colSums(rawHI)
ncol(rawHI)
nrow(rawHI)
rowSums(rawHI)
rowSums(rawHI[,2])
rowSums(rawHI[,2])
rawHI
class(rawHI)
# Categorize ConfHI to IDs
rawHI = as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
class(rawHI)
rawHI <- as.matrix(rawHI)
class(rawHI)
## Add up the # of times each ID was seen in HI
IDdata$HI = as.vector(rowSums(rawHI))
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Categorize ID to Foraging
IDbehav <- cbind(aux$Code, aux$Foraging)
IDbehav
# Read file in to retain ILV
orig_data <- read.csv("sample_data.csv")
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Select variables from the raw data
aux <- orig_data[1:nrow(list_years[[year]]),
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Select variables from the raw data
aux <- orig_data[1:1000]),
# Select variables from the raw data
aux <- orig_data[1:1000,
c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
# Categorize ID to Foraging
IDbehav <- cbind(aux$Code, aux$Foraging)
IDbehav
# Categorize ConfHI to IDs
rawHI <- as.matrix(cbind(aux$Code, aux$ConfHI)[,1:2])
rawHI
rawHI
IDbehav
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = IDbehav[,2])
IDdata
rawHI
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav
IDbehav <- data.frame(IDbehav)
IDbehav
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI)[,1:2])
rawHI
View(IDbehav)
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = IDbehav[,1 == "Feed"])
IDdata
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = IDbehav$Var2 == "Feed")
rawHI <- data.frame(rawHI)
rawHI
View(rawHI)
## Add up the # of times each ID was seen in HI
IDdata$HI = as.vector(sum(rawHI$Freq[rawHI$Var2 == "P"]))
View(IDbehav)
View(IDdata)
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = subset(IDbehav, IDbehav$Var2 == "Feed"))
IDdata
## Add up the # of times each ID was seen in HI
IDdata$HI = as.vector(sum(rawHI$Freq[rawHI$Var2 == "P"]))
IDdata
rawHI$Freq[rawHI$Var2 == "P"]
## Add up the # of times each ID was seen in HI
IDdata$HI = rawHI$Freq[rawHI$Var2 == "P"]
IDdata
# Take out the number of foraging events per ID
IDdata = data.frame(Foraging = subset(IDbehav, IDbehav$Var2 == "Feed"))[,c(1,3:4)]
IDdata <- IDdata[,c(1,3:4)]
IDdata
IDdata$HIprop = IDdata$HI/IDdata$Foraging
IDdata
IDdata$HIprop = IDdata[,3]/IDdata[,2]
IDdata
IDdata = as.data.frame(table(aux$Code))
IDdata
