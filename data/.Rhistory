}
logit.colonization<- 4.8 - 1.48*distan
p<- 1/(1+exp(-logit.colonization))
# Now ask if vector is occupied or not
for (col in 1:ncol(distan)) {
if (persistance[col]==0)
persistance[col]<-min(sum(rbinom(length(persistance[-col]),
persistance[col],p[col,-col])), 1)
}
return(persistance)
}
colonization<- colonize(persistance = persistance)
colonization
final.occ<- function(yrs)
final.occ<- function(yrs){
initial.occupancy<- o(mn = 0.7, s = 0.2)
persistance<- p(mn = 180, s = 70, ic = initial.occupancy)
vect<- NULL
for (i in 1:yrs) {
colonization<- colonize(persistance = persistance)
persistance<- p(p.mean, p.sd, ic = colonization[i])
vect<-c(vect,mean(persistance))
}
return(vect)
}
newt.occ<- final.occ(yrs = 10)
final.occ<- function(yrs){
initial.occupancy<- o(mn = 0.7, s = 0.2)
persistance<- p(mn = 180, s = 70, ic = initial.occupancy)
vect<- NULL
for (i in 1:yrs) {
colonization<- colonize(persistance = persistance)
persistance<- p(180, 70, ic = colonization[i])
vect<-c(vect,mean(persistance))
}
return(vect)
}
newt.occ<- final.occ(yrs = 10)
newt.occ
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years, decision){
#   N.suitable=50
#   N.unsuit=50
#   years=50
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4)
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
# Beta Function
beta.mom<-function(mean,v){
x<-mean
a<-x*(x*(1-x)/v-1)
b<-(1-x)*(x*(1-x)/v-1)
c(a,b)}
# Extinction
colon_extinct_vect<-function(p.persist,N.occupied.t,p.colon, N.empty.t){
N.occupied.t.plus<-rbinom(length(N.occupied.t),N.occupied.t,p.persist)+
rbinom(length(N.empty.t),N.empty.t,p.colon)
return(c(N.occupied.t.plus))}
# Initial Occupancy
initial_occup_vect<-function(p.persist,p.colon, N.suitable){
p.initial<- p.colon/(p.colon + (1-p.persist))
N.occupied.t<-rbinom(length(N.suitable),N.suitable,p.initial)
return(c(N.occupied.t))}
# Change Persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(1,1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Veg dynamics
veg.dynamics_vect<-function(N.suitable,N.unsuit,time,hurricane){
Total.patch= N.suitable + N.unsuit
N.unsuit <- N.unsuit + ifelse(hurricane==1,rbinom(length(N.suitable),N.suitable,0.5),0)
N.suitable <- Total.patch - N.unsuit
time <- ifelse(hurricane==1, 0, time + 1)
wt.slow<-0.5
wt.fast<- 1- wt.slow
p.suitable<- 1/(1+exp(-(-5 + 0.05*time + 0.05*time^2)))*wt.slow +
1/(1+exp(-(-5 + 0.75*time)))*wt.fast
N.suitable <- N.suitable + rbinom(length(N.suitable),N.unsuit,p.suitable)
N.unsuit<- Total.patch - N.suitable
veg.data<- cbind(N.suitable,N.unsuit,time)
return(veg.data)}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years, decision){
#   N.suitable=50
#   N.unsuit=50
#   years=50
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
# Change Persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years,decision){
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
vect <- rnorm(100, 30, 2)
# Stop the cluster
stopCluster(cl)
library(parallel)
library(doParallel)
# Stop the cluster
stopCluster(cl)
stopImplicitCluster()
# Specify the number of nodes/workers in the cluster
num_nodes <- 2
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Stop the cluster
stopCluster(cl)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
library(Matrix)
library(survival)
library(combinat)
library(matrixStats)
library(foreach)
library(doParallel)
library(tcltk)
together_apart <- function(sightings){
IDs <- colnames(sightings)
num_sightings <- as.data.frame(colSums(sightings))
# create a matrix that contains a measure of how many times each dyad has been seen together
together_matrix <- matrix(-99, nrow=length(IDs), ncol=length(IDs))
row.names(together_matrix) <- IDs # assign IDs as rownames
colnames(together_matrix) <- IDs # assign IDs as colnames
for (j in 1:length(IDs)){ # loop through each individual
for (i in 1:length(IDs)){
sub <- subset(sightings, subset=sightings[,i]==1 &sightings[,j]==1) # subset the data to where individuals i and j have been seen together
together <- length(sub[,1]) # extract how many times this was the case
together_matrix[j,i] <- together # save in matrix
together_matrix[i,j] <- together # save in matrix
} # close inner loop
} # close outer loop
# create a matrix that contains a measure of how many times each dyad has been seen independenly of each other
apart_matrix <- matrix(-99, nrow=length(IDs), ncol=length(IDs))
row.names(apart_matrix) <- IDs # assign IDs as rownames
colnames(apart_matrix) <- IDs # assign IDs as colnames
for (j in 1:length(IDs)){ # loop through each individual
for (i in 1:length(IDs)){
sub <- subset(sightings, subset=sightings[,i]==1 &sightings[,j]!=1) # subset the data to where individual i has been seen without j
apart_i <- length(sub[,1]) # extract how many times this was the case
sub2 <- subset(sightings, subset=sightings[,i]!=1 &sightings[,j]==1) # subset the data to where individuals j has been seen without i
apart_j <- length(sub2[,1])
apart_matrix[j,i] <- apart_i+apart_j  # save in matrix
apart_matrix[i,j] <- apart_i+apart_j  # save in matrix
} # close inner loop
} # close outer loop
# save the two matrices in an array with two dimensions
together_apart_array <-  array(c(together_matrix,apart_matrix),
dim=c(length(together_matrix[,1]), length(apart_matrix[,1]), 2),
dimnames=c(list(IDs),list(IDs))) # assign IDs as column and rownames
return(together_apart_array)
} #close function loop
sensitivity_NBDA_ind_error <- function(x, sightings, cutoff, association_index, iterations, s, num_ind_learn, cores=NULL, keep_learners=FALSE, delta_AICc=2){ # define function and parameters
assoc_SRI <- asnipe::get_network(sightings, data_format="GBI", association_index=association_index) ## calculate association matrix SRI
IDs <- row.names(assoc_SRI) # save names of individuals into a list called 'ID'
pb <- tkProgressBar(title = "progress bar", min = 0, # create a progress bar
max = length(cutoff), width = 300)
#Set number of cores to that in the machine if not specified
if(is.null(cores)) cores= detectCores(logical = FALSE)
#Correct number of iterations so it is a multiple of the number of cores
iterations<-round(iterations/cores)*cores
# split data by ourselves
chunk.size <- iterations/cores
num_sightings <- as.data.frame(colSums(sightings)) ## extract how many times each animal has been seen from the error object
cutoff_vector <- cutoff # reassign the specified cut off values to a vector with a different name
## create dataframe to store results
df7 <- data.frame(matrix(ncol=13, nrow=length(cutoff_vector)*iterations)) ## prepare data frame to store results
colnames(df7) <- paste(c("cutoff_point", "number_ind", "number_ind_learn", "iteration", "s", "aicc", "aiccNull", "deltaAICc", "p", "loglike_estimated_s", "loglike_set_s", "within_95", "under_over"))
#Save a blank version to be used in parallel processing
df7Blank<-df7[1:chunk.size,]
# create a dataframe to store results
results <- data.frame(matrix(nrow=length(cutoff_vector), ncol=12))
colnames(results) <- paste(c("cutoff",
"num_IDs_total",
"num_IDs_learn",
"delta_AICc",
"perc_outisde_delta_AICc",
"perc_social_learning_supported",
"set_s",
"mean_s",
"sd_s",
"within_95%_CI",
"overestimate",
"underestimate"))
for (e in cutoff_vector){ ## run loop for each value of the cutoff vector (e refers to the position of the value in the vector)
#Set up for parallel processing as detailed in http://www.parallelr.com/r-with-parallel-computing/
cl <- makeCluster(cores)
registerDoParallel(cl, cores=cores)
res2.p <- foreach(i=1:cores, .combine='rbind') %dopar%
{
#re-define the classes and methods used in the loop here
source("NBDA code 1.2.15.R")
# local data for results
res <- df7Blank
for(m in ((i-1)*chunk.size+1):(i*chunk.size)) {
# create an empty vector where the order of acquisition will be stored in. No length specified
order_acq <- NULL
# create a vector to save the total association with informed individuals
totalAssocDemon <- rep(length=length(IDs), 0)
# create a vector of length(individuals) filled with 0 to track the status
statusTracker <- rep(length=length(IDs), 0)
for (k in 1:num_ind_learn){
# fill totalAssocDemon with the association multiplied with the status tracker
totalAssocDemon <- assoc_SRI%*%statusTracker
Ri <- ((1-statusTracker)*(s*totalAssocDemon+1)) # calculate the learning rate. +1 in the end as no individual-level variables specified
summedRi <- sum(Ri) # calculate overall learning rate
learning_probability <- Ri/summedRi # calculate individual learning probability
b <- rmultinom(n=1, size=1, prob=learning_probability) # choose the next individual to learn based on the calculated learning probability
statusTracker <- as.vector(b)+statusTracker  ## update the status vector with 1 for the individual that learned
order_acq[k] <- which(as.vector(b==1)) # save the individual that learned in a vector order_acq
}
if(keep_learners==TRUE){ # if informed individuals should be kept even though they don't make the cutoff
sub <- subset(num_sightings, num_sightings$`colSums(sightings)` >= e) ## drop all animals from list with equal or less than e sightings
list_included <- rownames(sub) ## convert names into a list
OAc1 <- IDs[order_acq] # extract a list of individuals that learned
list_included_plus <- unique(c(list_included,OAc1))  # create a list of individuals who made the cutoff plus individuals who would not have made it, but acquired the behaviour
OAc2 <- match(OAc1, list_included_plus) # reassing the new positions of these individuals in the reduced social network
IDs_sub <- list_included_plus
} else { # if learners are to be dropped
sub <- subset(num_sightings, num_sightings$`colSums(sightings)` >= e) ## drop all animals from list with equal or less than e sightings
list_included <- rownames(sub) ## convert names into a list
dropped_IDs <- setdiff(IDs, list_included) # extract names of individuals that did not make the cutoff
dropped_num <- match(dropped_IDs, IDs) # extract the positions of these individuals in the full list of names
new_order_acq <- order_acq [! order_acq %in% dropped_num] ## drop individuals from the order of acquistion that did not make the cutoff point
OAc1 <- IDs[new_order_acq] # extract a list of individuals that learned (after dropping the ones that did not make the cutoff)
OAc2 <- match(OAc1, list_included) # reassing the new positions of these individuals in the reduced social network
IDs_sub <- list_included
}
assoc_sim <- matrix(rbeta(length(x[,,1]),x[,,1]+1,x[,,2]+1), ncol=length((IDs))) # generate a association matrix based on the number of times dyads have been seen together and seen apart
# uses a beta distribution B(a+x,b+n-x), where x is the matrix containing the number of times each dyad has been seen together (successes) and n-x a matrix with how many times they have been seen apart (fails)
# a and b are both 1 (for uniform distribution)
colnames(assoc_sim) <- IDs
rownames(assoc_sim) <- IDs
assoc_sim_sub_pre <- assoc_sim[IDs_sub,]
assoc_sim_sub <- assoc_sim_sub_pre[,IDs_sub]
if(length(OAc2!=0)){ # if there are individuals left who learned after removing those below the cutoff
OADA <- oaData(assMatrix=assoc_sim_sub, asoc=NULL, orderAcq=OAc2) ## use the associations with error to run OADA
OADA_model <- addFit(oadata=OADA, asocialVar=NULL)
param_s <- OADA_model@optimisation$minimum ## corresponds to social learning parameter s
aicc <- OADA_model@aicc ## extracts aicc of OADA model
aiccNull <- OADA_model@aiccNull ## extracts aiccNull of OADA model (which is the Nullmodel where s is constrained to 0)
p <- OADA_model@LRTsocTransPV ## extract p value of the likelihood ratio test
loglike_s_estimate <- OADA_model@loglik # get log likelihood for the estimated value of s
loglike_s_set <- addLikelihood(data=OADA, l=s, asocialVar=NULL) # get likelihood for the set s
## save results into local results to be put into dataframe 7
res[m - (i-1)*chunk.size, "cutoff_point"] <- e
res[m - (i-1)*chunk.size, "number_ind"] <- length(IDs_sub)
res[m - (i-1)*chunk.size, "number_ind_learn"] <- length(OAc2)
res[m - (i-1)*chunk.size,"iteration"] <- m
res[m - (i-1)*chunk.size,"s"] <- param_s
res[m - (i-1)*chunk.size,"aicc"] <- aicc
res[m - (i-1)*chunk.size,"aiccNull"] <- aiccNull
res[m - (i-1)*chunk.size,"deltaAICc"] <- abs(aicc-aiccNull)
res[m - (i-1)*chunk.size, "p"] <- p
res[m - (i-1)*chunk.size,"loglike_estimated_s"] <- loglike_s_estimate
res[m - (i-1)*chunk.size, "loglike_set_s"] <- loglike_s_set
# test if the difference between loglikelihood for the set and estimated s is within 1.92 units (95% confidence interval)
if(abs(loglike_s_set-loglike_s_estimate) <= 1.92){res[m - (i-1)*chunk.size, "within_95"] <- "yes"}  else {
res[m - (i-1)*chunk.size, "within_95"] <- "no"}
# if it falls outside the 95% confidence interval, calculate the difference between the set s and estimated s to see if the estimated s is an over- or underestimate
if(res[m - (i-1)*chunk.size,"within_95"]=="yes"){res[m - (i-1)*chunk.size, "under_over"] <- NA} else {
if(param_s-s > 0) {res[m - (i-1)*chunk.size,"under_over"] <- "over"}
else {res[m - (i-1)*chunk.size,"under_over"] <- "under"}
}
}
else{
res[m - (i-1)*chunk.size, "cutoff_point"] <- e
res[m - (i-1)*chunk.size, "number_ind"] <- length(IDs_sub)
res[m - (i-1)*chunk.size, "number_ind_learn"] <- 0
res[m - (i-1)*chunk.size,"iteration"] <- m
res[m - (i-1)*chunk.size,"s"] <- NA
res[m - (i-1)*chunk.size,"aicc"] <- NA
res[m - (i-1)*chunk.size,"aiccNull"] <- NA
res[m - (i-1)*chunk.size,"deltaAICc"] <- NA
res[m - (i-1)*chunk.size, "p"] <- NA
res[m - (i-1)*chunk.size,"loglike_estimated_s"] <- NA
res[m - (i-1)*chunk.size, "loglike_set_s"] <- NA
res[m - (i-1)*chunk.size, "within_95"] <- NA
res[m - (i-1)*chunk.size, "under_over"] <- NA
}
}
res
}
stopImplicitCluster()
stopCluster(cl)
#Put the results into the df7 object
df7[(which(cutoff == e)-1)*iterations +1:iterations,]<-res2.p
#temp file that allow us to restart a simulation that crashed partway through
save(df7, file="sensitivity_NBDA_ind_error_TempOut.dat")
# retrieve results for each cut-off point separately
subset <- subset(df7,df7$cutoff_point==e)
subset2 <- subset(subset, !is.nan(subset[,"deltaAICc"])) # remove rows where AICc difference is NaN (not a number)
subset2$deltaAICc[subset2[,"deltaAICc"]==Inf] <- 1000 # replace infitinity in the aicc differences with an arbitrary large value
perc_outside_delta_AICc <- (sum(subset2["deltaAICc"]>=delta_AICc, na.rm=TRUE))/iterations*100 # percentage of models where aicc difference is above threshold (excluding NAs)
subset3 <- subset(subset2,subset2$deltaAICc>=delta_AICc) #create a subset with models only where aicc difference is above the set threshold
perc_support <- (sum(subset3["p"]<0.05, na.rm=TRUE))/iterations*100
mean_s <- mean(subset3$s, na.rm=TRUE)
sd_s <- sd(subset3$s, na.rm=TRUE)
perc_outside <- sum(subset3["within_95"]=="no", na.rm=TRUE )/iterations*100
perc_over <- sum(subset3["under_over"]=="over", na.rm=TRUE)/sum(subset3["within_95"]=="no", na.rm=TRUE)*100
perc_under <- sum(subset3["under_over"]=="under", na.rm=TRUE)/sum(subset3["within_95"]=="no", na.rm=TRUE)*100
results[which(cutoff==e),1] <- e
results[which(cutoff==e),2] <- round(mean(subset2$number_ind), digits=2)
results[which(cutoff==e),3] <- round(mean(subset2$number_ind_learn), digits=2)
results[which(cutoff==e),4] <- delta_AICc
results[which(cutoff==e),5] <- perc_outside_delta_AICc
results[which(cutoff==e),6] <- round(perc_support, digits=2)
results[which(cutoff==e),7] <- s
results[which(cutoff==e),8] <- round(mean_s, digits=2)
results[which(cutoff==e),9] <- round(sd_s, digits=2)
results[which(cutoff==e),10] <- round(100-perc_outside, digits=2)
results[which(cutoff==e),11] <- round(perc_over, digits=2)
results[which(cutoff==e),12] <- round(perc_under, digits=2)
setTkProgressBar(pb, which(cutoff_vector==e), label=paste( round(which(cutoff_vector==e)/length(cutoff_vector)*100, 0), "% done"))
}
df7 <- df7
object <- NULL
object$raw <- df7
object$summary <- results
return(object)  # return the created object with the two slots $raw and $summary
close(pb)
}
gbi<readRDS("gbi.RData")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
gbi<readRDS("gbi.RData")
gbi<-readRDS("gbi.RData")
together_apart_output <- together_apart(sightings=gbi[[1]])
# Extract how many times each dyad has been seen together and how many times they have been seen apart
## These two matrices will be used to create a social network with observational error using a Bayesian approach
year <- 21
together_apart_output <- together_apart(sightings=gbi[[year]])
write.csv(together_apart_output[,,1], file="../data/together_matrix.csv") # save 'together' matrix
write.csv(together_apart_output[,,2], file="../data/apart_matrix.csv") # save 'apart' matrix
# Determine how many times individuals have been seen and choose cut-off points
cutoff <- sort(unique(colSums(gbi[[year]]))) # extract colSums (=number of sightings)
cutoff <- cutoff[-length(cutoff)] # remove the maximum number of sightings
cutoff <- cutoff[-length(cutoff)] # remove second last as well
# Run a first simulation with dropping learners and s=8 (social learning)
sim1 <- sensitivity_NBDA_ind_error(x=together_apart_output, # with s=8 and dropping learners
sightings=gbi[[year]],
cutoff=cutoff,
association_index="SRI",
iterations=10000,
s=8,
num_ind_learn=20,
cores=2,
keep_learners = FALSE,
delta_AICc = 2)
