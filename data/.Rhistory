stopImplicitCluster()
# Specify the number of nodes/workers in the cluster
num_nodes <- 2
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Stop the cluster
stopCluster(cl)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(parallel)
library(sfsmisc, verbose=F)
# Read in file and add months
sample_data <- read.csv("sample_data.csv")
# Get all unique Code values in the entire sample_data
all_codes <- unique(sample_data$Code)
# Create a function that counts the IDs in each element
count_instances <- function(df) {
code_counts <- table(df$Code)
code_counts <- code_counts[match(all_codes, names(code_counts))]
code_counts[is.na(code_counts)] <- 0
return(code_counts)
}
# -------------------- 22 sets of 1 year increments----------------------------
# Make a list of only 1 year per dataframe
list_years <- split(sample_data, sample_data$Year)
# Apply the count_instances function to each year
instances_per_year <- lapply(list_years, count_instances)
# Convert the list of counts to a data frame
p1y <- do.call(rbind, instances_per_year)
# Transforming into binary matrices
p1y <- as.matrix(p1y); p1y[which(p1y>=1)] = 1; p1y[which(p1y<1)] = 0
# -------------------- 11 sets of 2 year increments----------------------------
# Make a list of 2 years per dataframe
sample_data$TwoYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 2, by = 2), labels = FALSE)
list_twoyears <- split(sample_data, sample_data$TwoYearIncrement)
# Apply the count_instances function to each two years
instances_per_twoyear <- lapply(list_twoyears, count_instances)
# Convert the list of counts to a data frame
p2y <- do.call(rbind, instances_per_twoyear)
# Transforming into binary matrices
p2y <- as.matrix(p2y); p2y[which(p2y>=1)] = 1; p2y[which(p2y<1)] = 0
# -------------------- 7 sets of 3 year increments----------------------------
# Make a list of 3 years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Apply the count_instances function to each two years
instances_per_threeyear <- lapply(list_threeyears, count_instances)
# Convert the list of counts to a data frame
p3y <- do.call(rbind, instances_per_threeyear)
# Transforming into binary matrices
p3y <- as.matrix(p3y); p3y[which(p3y>=1)] = 1; p3y[which(p3y<1)] = 0
# -------------------- 6 sets of 4 year increments----------------------------
# Make a list of 4 years per dataframe
sample_data$FourYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 4, by = 4), labels = FALSE)
list_fouryears <- split(sample_data, sample_data$FourYearIncrement)
# Apply the count_instances function to each two years
instances_per_fouryear <- lapply(list_fouryears, count_instances)
# Convert the list of counts to a data frame
p4y <- do.call(rbind, instances_per_fouryear)
# Transforming into binary matrices
p4y <- as.matrix(p4y); p4y[which(p4y>=1)] = 1; p4y[which(p4y<1)] = 0
# -------------------- 4 sets of 5 year increments----------------------------
# Make a list of 5 years per dataframe
sample_data$FiveYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 5, by = 5), labels = FALSE)
list_fiveyears <- split(sample_data, sample_data$FiveYearIncrement)
# Apply the count_instances function to each two years
instances_per_fiveyear <- lapply(list_fiveyears, count_instances)
# Convert the list of counts to a data frame
p5y <- do.call(rbind, instances_per_fiveyear)
# Transforming into binary matrices
p5y <- as.matrix(p5y); p5y[which(p5y>=1)] = 1; p5y[which(p5y<1)] = 0
# -------------------- 4 sets of 6 year increments----------------------------
# Make a list of 6 years per dataframe
sample_data$SixYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 6, by = 6), labels = FALSE)
list_sixyears <- split(sample_data, sample_data$SixYearIncrement)
# Apply the count_instances function to each two years
instances_per_sixyear <- lapply(list_sixyears, count_instances)
# Convert the list of counts to a data frame
p6y <- do.call(rbind, instances_per_sixyear)
# Transforming into binary matrices
p6y <- as.matrix(p6y); p6y[which(p6y>=1)] = 1; p6y[which(p6y<1)] = 0
# -------------------- 3 sets of 7 year increments----------------------------
# Make a list of 7 years per dataframe
sample_data$SevenYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 7, by = 7), labels = FALSE)
list_sevenyears <- split(sample_data, sample_data$SevenYearIncrement)
# Apply the count_instances function to each two years
instances_per_sevenyear <- lapply(list_sevenyears, count_instances)
# Convert the list of counts to a data frame
p7y <- do.call(rbind, instances_per_sevenyear)
# Transforming into binary matrices
p7y <- as.matrix(p7y); p7y[which(p7y>=1)] = 1; p7y[which(p7y<1)] = 0
# -------------------- 3 sets of 8 year increments----------------------------
# Make a list of 8 years per dataframe
sample_data$EightYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 8, by = 8), labels = FALSE)
list_eightyears <- split(sample_data, sample_data$EightYearIncrement)
# Apply the count_instances function to each two years
instances_per_eightyear <- lapply(list_eightyears, count_instances)
# Convert the list of counts to a data frame
p8y <- do.call(rbind, instances_per_eightyear)
# Transforming into binary matrices
p8y <- as.matrix(p8y); p8y[which(p8y>=1)] = 1; p8y[which(p8y<1)] = 0
source("../code/functions.R") # WDI & WDI permutation
# Turn over results
t1 = turnover_w(data = p1y, iter = 1000, subseq=F, plot=FALSE)
t2 = turnover_w(data = p2y, iter = 1000, subseq=F, plot=FALSE)
t3 = turnover_w(data = p3y, iter = 1000, subseq=F, plot=FALSE)
t4 = turnover_w(data = p4y, iter = 1000, subseq=F, plot=FALSE)
t5 = turnover_w(data = p5y, iter = 1000, subseq=F, plot=FALSE)
t6 = turnover_w(data = p6y, iter = 1000, subseq=F, plot=FALSE)
t7 = turnover_w(data = p7y, iter = 1000, subseq=F, plot=FALSE)
t8 = turnover_w(data = p8y, iter = 1000, subseq=F, plot=FALSE)
all = rbind(t1, t2, t3, t4, t5, t6, t7, t8)
all = cbind(c(1, 2, 3, 4, 5, 6, 7, 8), all)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.34,0.43), xlim=c(1,8), cex.axis=0.8)
axis(1, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, cex.axis=0.7)
mtext(side = 1, "Length of periods (years)", line = 2, font = 1)
axis(3, at=c(1, 2, 3, 4, 5, 6, 7, 8),las=1, labels=c(22, 11, 7, 6, 5, 4, 3, 3), cex.axis=0.7)
mtext(side = 3, "Number of periods", line = 2, font = 1)
# Print final results
all
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.30,0.7), xlim=c(1,8), cex.axis=0.8)
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.5,0.6), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.56,0.61), xlim=c(1,8), cex.axis=0.8)
par(mar=c(4,5,4,1))
# Plot the final results. Whisker represent 95%CI generated by the null model. X-axis represent the number of periods and their respective lengths
errbar(x=c(1, 2, 3, 4, 5, 6, 7, 8), y=all[,2], all[,4], all[,5], ylab="Turnover (Averaged Whittaker Dissimilarity)",
pch=1, cap=0.02, xaxt='n', xlab="", las=1, cex=1.0, ylim=c(0.565,0.61), xlim=c(1,8), cex.axis=0.8)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(adehabitatHR) # Kernel density
library(rgdal) # Overlap
# Read in file
sample_data <- read.csv("sample_data.csv")
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'subYear', 'ConfHI')]) # Subset Date and Coordinates #
# Read in file
sample_data <- read.csv("sample_data.csv")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
gc()
# Set working directory here
setwd("C:/Users/bankh/My_Repos/DolphiNP/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphin/data")
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Could do permutatioNP
require(assocInd)
require(vegan)
# Run multiple cores for faster computing
require(doParallel)
require(foreach)
sample_data <- read.csv("sample_data.csv")
length(unique(sample_data$Code))
list_years <- readRDS("list_years.RData")
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- tapply(sample_data$Date, sample_data$Year, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years$Date, function(x) length(unique(x)))
effort
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
effort
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
unique_ID_year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- cbind(effort, pop) # Days per year and pop size per year
pop_effort
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
pop_effort
rownames(pop_effort) <- c('Days Surveyed', 'Number of Indivduals')
pop_effort
plot(pop_effort$effort ~ pop_effort$unique_ID_year)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,] ~ pop_effort[2,])
pop_effort[1,]
pop_effort[2,]
pop_effort[1,c(1:7)]
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
# Estimate sampling effort and size for each year
## Get estimate of sampling effort
effort <- lapply(list_years, function(df) length(unique(df$Date)))
## Get estimate of population size
unique_ID_year <- lapply(list_years, function(df) length(unique(df$Code)))
## Compare effort to population size
pop_effort <- rbind(effort, pop) # Days per year and pop size per year
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
## Compare effort to population size
effort <- as.data.frame(effort)
pop <- as.data.frame(unique_ID_year)
pop_effort <- as.data.frame(rbind(effort, pop)) # Days per year and pop size per year
pop_effort
colnames(pop_effort) <- c(1:7)
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals')
plot(pop_effort[1,c(1:7)] ~ pop_effort[2,c(1:7)])
effort
pop_effort
# Read in different behavior's data frames
IDbehav_Beg <- readRDS("IDbehav_Beg.RData")
IDbehav_Pat <- readRDS("IDbehav_Pat.RData")
IDbehav_Dep <- readRDS("IDbehav_Dep.RData")
Beg_effort <- lapply(IDbehav_Beg, function(df) length(unique(df$Code)))
View(IDbehav_Beg)
length(unique(IDbehav_Beg[[1]]$Code=='B'))
IDbehav_Beg[[1]][["HI"]]
length(unique(IDbehav_Beg[[1]]$Code[IDbehav_Beg[[1]]$HI > 0]))
Beg_effort <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0])))
Beg_effort
Pat_effort <- as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI > 0]))))
Pat_effort
Beg_effort <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI > 0]))))
Beg_effort
Dep_effort <- as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI > 0]))))
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
colnames(Beg_effort) <- c(1:7)
colnames(Pat_effort) <- c(1:7)
colnames(Dep_effort) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
effort
colnames(effort) <- c(1:7)
colnames(unique_ID_year) <- c(1:7)
## Get estimate of population size
unique_ID_year <- as.data.frame(lapply(list_years, function(df) length(unique(df$Code))))
unique_ID_year
colnames(unique_ID_year) <- c(1:7)
## Compare effort to population size
pop_effort <- as.data.frame(rbind(effort, unique_ID_year, Beg_effort, Pat_effort, Dep_effort)) # Days per year and pop size per year
pop_effort
rownames(pop_effort) <- c('Days_Surveyed', 'Number_of_Indivduals', 'Beggars', 'Patrollers', 'Depredators')
pop_effort
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
length(unique(orig_data$Code))
length(unique(sample_data$Code))
sum(pop_effort[2,])
sum(pop_effort[c(3:5),])
sum(unique(sample_data$Code[sample_data$ConfHI != 0]))
unique(sample_data$Code[sample_data$ConfHI != 0])
length(unique(sample_data$Code[sample_data$ConfHI != 0]))
sum(pop_effort[c(3),])
sum(pop_effort[c(4),])
sum(pop_effort[c(5),])
lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
sum(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
nb <- lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0])))
nb <- as.data.frame(lapply(IDbehav_Beg, function(df)
length(unique(df$Code[df$HI == 0]))))
sum(nb)
sum(as.data.frame(lapply(IDbehav_Pat, function(df)
length(unique(df$Code[df$HI == 0])))))
sum(as.data.frame(lapply(IDbehav_Dep, function(df)
length(unique(df$Code[df$HI == 0])))))
all <- subset(IDbehav_Beg, IDbehav_Beg$Code %in% IDbehav_Pat$Code)
all
Beg <- as.data.frame(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0]))
View(Beg)
?append
Beg <- append(lapply(IDbehav_Beg, function(df)
unique(df$Code[df$HI > 0])))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0])))
Beg
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI > 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI = 0]))))
length(unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0]))))
length(unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0]))))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI == 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI == 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI == 0])))
Beg %in% Pat %in% Dep
sum(Beg %in% Pat %in% Dep)
sum(Beg %in% Dep)
sum(Pat %in% Dep)
Reduce(intersect, list(Beg, Pat, Dep))
Beg <- unique(unlist(sapply(IDbehav_Beg, function(df) df$Code[df$HI != 0])))
Pat <- unique(unlist(sapply(IDbehav_Pat, function(df) df$Code[df$HI != 0])))
Dep <- unique(unlist(sapply(IDbehav_Dep, function(df) df$Code[df$HI != 0])))
Reduce(intersect, list(Beg, Pat, Dep))
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
setwd("C:/Users/bankh/My_Repos/Dolphins/code")
# Set working directory here
setwd("../data")
library(ade4) # Look at Dai Shizuka/Jordi Bascompte
library(ncf) # For weights
library(vegan)
library(igraph) # graph_adj
require(asnipe) # mrqap.dsp
library(assortnet)
library(ggplot2)
library(doParallel)
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
list_years <- readRDS("list_years.RData")
## Make a list of years
coord_data_list <- list_years
coord_data <- lapply(coord_data_list, function(df) {
# Extract coordinates
df <- df[, c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]
# Format date and year
df$Date <- as.Date(as.character(df$Date), format = "%Y-%m-%d")
# Give descriptive names
colnames(df) <- c("date", "y", "x", "id", "year", "HI")
return(df)
})
View(coord_data)
# Only include three columns (id, x, and y coordinates) for making MCP's
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
return(df_sub)
})
# Create a simple feature data frame (sf)
coord_data_sf <- lapply(dolph.sp, function(df) {
st_as_sf(df, coords = c("x", "y"), crs = 4326)})
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
library(scales) # Helps make polygons partly transparent using the alpha argument
library(ggmap) # Download tiles using ggmap
library(viridis) # Color pallette
library(gridExtra) # grid.arrange function
library(ggplot2)
library(rgdal)
# Create a simple feature data frame (sf)
coord_data_sf <- lapply(dolph.sp, function(df) {
st_as_sf(df, coords = c("x", "y"), crs = 4326)})
# UTM zone for study area
dolph.sf <- lapply(coord_data_sf, function(df) {
st_transform(df, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))})
# Process the coord_data_list
dolph.sp <- lapply(coord_data_list, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
coordinates(df) <- c("x", "y")
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
st_set_geometry(df, NULL)
})
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
st_set_geometry(df, NULL)
# Set the initial CRS for data to WGS84 (latitude and longitude)
proj4string(df) <- CRS( "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
})
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
st_geometry(df) <- NULL  # Remove geometry column
st_crs(df) <- "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"  # Set CRS
df
})
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
df <- st_as_sf(df, coords = c("x", "y"), crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df
})
# 95% of estimated distribution
kernel <- kernelUD(dolph.sp, h = 500)
View(dolph.sp)
# 95% of estimated distribution
kernel <- kernelUD(dolph.sp[[1]], h = 500)
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
df_sf <- st_as_sf(df, coords = c("x", "y"), crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_sp <- as(df_sf, "Spatial")
df_sp
})
View(dolph.sp)
# Process the coord_data_list
dolph.sp <- lapply(coord_data, function(df) {
df_sub <- df[, c("id", "y", "x")]
df_sf <- st_as_sf(df_sub, coords = c("x", "y"), crs = 4326)
df_utm <- st_transform(df_sf, crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_utm
})
View(dolph.sp)
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp <- lapply(dolph.sp, function(df) {
df$x <- st_coordinates(df)[, 1]
df$y <- st_coordinates(df)[, 2]
df_sf <- st_as_sf(df, coords = c("x", "y"), crs = "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs")
df_sp <- as(df_sf, "Spatial")
df_sp
})
# 95% of estimated distribution
kernel <- kernelUD(dolph.sp[[1]], h = 500)
kernel
View(dolph.sp)
# 95% of estimated distribution
kernel <- list()
dolph.kernel.poly <- list()
for (i in seq_along(dolph.sp)) {
kernel[[i]] <- kernelUD(dolph.sp[[i]], h = 500)
dolph.kernel.poly[[i]] <- getverticeshr(kernel[[i]], percent = 95)
}
# Get HRO
kernel <- list()
for (i in seq_along(dolph.sp)) {
kernel[[i]] <- kernelUD(dolph.sp[[i]], h = 1000)
}
