dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4)
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
# Beta Function
beta.mom<-function(mean,v){
x<-mean
a<-x*(x*(1-x)/v-1)
b<-(1-x)*(x*(1-x)/v-1)
c(a,b)}
# Extinction
colon_extinct_vect<-function(p.persist,N.occupied.t,p.colon, N.empty.t){
N.occupied.t.plus<-rbinom(length(N.occupied.t),N.occupied.t,p.persist)+
rbinom(length(N.empty.t),N.empty.t,p.colon)
return(c(N.occupied.t.plus))}
# Initial Occupancy
initial_occup_vect<-function(p.persist,p.colon, N.suitable){
p.initial<- p.colon/(p.colon + (1-p.persist))
N.occupied.t<-rbinom(length(N.suitable),N.suitable,p.initial)
return(c(N.occupied.t))}
# Change Persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(1,1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Veg dynamics
veg.dynamics_vect<-function(N.suitable,N.unsuit,time,hurricane){
Total.patch= N.suitable + N.unsuit
N.unsuit <- N.unsuit + ifelse(hurricane==1,rbinom(length(N.suitable),N.suitable,0.5),0)
N.suitable <- Total.patch - N.unsuit
time <- ifelse(hurricane==1, 0, time + 1)
wt.slow<-0.5
wt.fast<- 1- wt.slow
p.suitable<- 1/(1+exp(-(-5 + 0.05*time + 0.05*time^2)))*wt.slow +
1/(1+exp(-(-5 + 0.75*time)))*wt.fast
N.suitable <- N.suitable + rbinom(length(N.suitable),N.unsuit,p.suitable)
N.unsuit<- Total.patch - N.suitable
veg.data<- cbind(N.suitable,N.unsuit,time)
return(veg.data)}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years, decision){
#   N.suitable=50
#   N.unsuit=50
#   years=50
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
# Change Persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(length(hurricane),cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
# Change persistence
change.persistence_vect<-function(decision, hurricane){
p.persist.hat<- 0.75
p.persist.var<- 0.05
cur.per.beta<-beta.mom(p.persist.hat,p.persist.var)
p.persist.curr<-rbeta(1,cur.per.beta[1],cur.per.beta[2])
if (decision==2){
ln.odds<- log(p.persist.curr/(1-p.persist.curr))
change<- rnorm(length(hurricane),1.3,0.17)
p.persist<- 1/(1+exp(-(ln.odds*change)))
} else p.persist <- p.persist.curr
p.persist<-ifelse(hurricane==1, 0.3,p.persist)
return(p.persist)
}
#' Mouse dynamics function
mouse.patch.dyn_vect<-function(N.suitable,N.unsuit,years,decision){
# set initial values
p.colon.hat<- 0.5
p.colon.var<- 0.1
beta.colon<-beta.mom(p.colon.hat,p.colon.var)
p.persist.hat=0.75
time=0
# initial occupancy
occ.states<-initial_occup_vect(p.persist=p.persist.hat,p.colon=p.colon.hat, N.suitable=N.suitable)
# place to hold proportion occupied
for(yr in 1:years){
# implement decision in year 5
decis<- ifelse(yr==5,decision,1)
hurricane<-rbinom(length(N.suitable),1,0.02)
p.persist=change.persistence_vect(decision=decis,hurricane=hurricane)
p.colon<-rbeta(length(N.suitable),beta.colon[1],beta.colon[2])
# if nothing is occupied colonization is zero
p.colon<-ifelse(occ.states > 0,p.colon,0)
occ.states<- colon_extinct_vect(p.persist=p.persist,N.occupied.t=occ.states,
p.colon=p.colon, N.empty.t=(N.suitable-occ.states))
tmp<-veg.dynamics_vect(N.suitable=N.suitable,N.unsuit=N.unsuit,time=time,
hurricane=hurricane)
N.suitable=tmp[,1]
N.unsuit=tmp[,2]
time=tmp[,3]
# restore 5 patches decision
N.suitable=ifelse(decis==3, N.suitable+5, N.suitable)
Total.ptch = N.suitable+N.unsuit
}
occ.states<-ifelse(is.na(occ.states),0,occ.states)
return(occ.states)
}
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
dec.1<-mouse.patch.dyn_vect(N.suitable=rep(50, 1e4),N.unsuit=rep(50,1e4),
years= 50, decision=rep(1, 1e4))
dec.1
vect <- rnorm(100, 30, 2)
# Stop the cluster
stopCluster(cl)
library(parallel)
library(doParallel)
# Stop the cluster
stopCluster(cl)
stopImplicitCluster()
# Specify the number of nodes/workers in the cluster
num_nodes <- 2
# Create a cluster with the specified number of nodes/workers
cl <- makeCluster(num_nodes)
# Stop the cluster
stopCluster(cl)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
# Read in file
sample_data <- read.csv("sample_data.csv")
# Read in file
sample_data <- read.csv("sample_data.csv")
list_years <- readRDS("list_years.RData")
# # Test one year at a time
year <- 5
coord_data <- list_years[[year]]
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
## Format date and year
coord_data$Date <- as.Date(as.character(coord_data$Date), format="%Y-%m-%d")
## Give descriptive names
colnames(coord_data) <- c("date", "y", "x", "id", "year", "HI")
# Only include three columns (id, x, and y coordinates) for making MCP's
dolph.sp <- coord_data[, c("id", "y", "x")]
# Create a simple feature data frame (sf)
coord_data_sf <- st_as_sf(dolph.sp, coords = c("x", "y"), crs = 4326)
## load all necessary packages
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
# Create a simple feature data frame (sf)
coord_data_sf <- st_as_sf(dolph.sp, coords = c("x", "y"), crs = 4326)
# UTM zone for study area
dolph.sf <- st_transform(coord_data_sf, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp$x <- st_coordinates(dolph.sf)[, 1]
dolph.sp$y <- st_coordinates(dolph.sf)[, 2]
# Remove two rows with NA's
dolph.sp <- dolph.sp[!is.na(dolph.sp$x) & !is.na(dolph.sp$y),]
# Only include three columns (id, x, and y coordinates) for making MCP's
dolph.sp <- coord_data[, c("id", "y", "x")]
is.na(dolph.sp$x)
sum(is.na(dolph.sp$x))
sum(is.na(dolph.sp$y))
# Create a simple feature data frame (sf)
coord_data_sf <- st_as_sf(dolph.sp, coords = c("x", "y"), crs = 4326)
sum(is.na(dolph.sp$y))
sum(is.na(dolph.sp$x))
# UTM zone for study area
dolph.sf <- st_transform(coord_data_sf, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
sum(is.na(dolph.sf$x))
sum(is.na(dolph.sf$y))
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp$lon <- st_coordinates(dolph.sf)[, 1]
dolph.sp$lat <- st_coordinates(dolph.sf)[, 2]
sum(is.na(dolph.sp$lon))
# Calculate the number of NA values in dolph.sp$lon
num_na <- sum(is.na(dolph.sp$lon))
# Check if the result is NA
is_result_na <- is.na(num_na)
# Print the result
print(is_result_na)
# Check which rows contain NAs
rows_with_na <- complete.cases(dolph.sp)
# Filter the rows containing NAs
rows_containing_na <- dolph.sp[!rows_with_na, ]
rows_containing_na
# Read in & combine files
firstgen_data <- read.csv("firstgen_data.csv")
secondgen_data <- read.csv("secondgen_data.csv")
orig_data <- rbind(firstgen_data, secondgen_data)
orig_data <- subset(orig_data, subset=c(orig_data$Code != "None"))
# Make date into a date class
orig_data$Date <- as.Date(as.character(orig_data$Date), format="%d-%b-%y")
orig_data$Year <- as.numeric(format(orig_data$Date, format = "%Y"))
# Get rid of any data with no location data
orig_data <- orig_data[!is.na(orig_data$StartLat) & !is.na(orig_data$StartLon),]
class(orig_data$StartLat)
orig_data <- subset(orig_data, subset=c(orig_data$StartLat != 999))
sample_data <- subset(orig_data, subset=c(orig_data$StartLon != 999))
write.csv(sample_data, "sample_data.csv")
# Make a list of three years per dataframe
sample_data$ThreeYearIncrement <- cut(sample_data$Year, breaks = seq(min(sample_data$Year), max(sample_data$Year) + 3, by = 3), labels = FALSE)
list_threeyears <- split(sample_data, sample_data$ThreeYearIncrement)
# Eliminate IDs with less than 5 locations
ID <- list()
for (i in seq_along(list_threeyears)) {
ID[[i]] <- unique(list_threeyears[[i]]$Code)
obs_vect <- NULL
for (j in 1:length(ID[[i]])) {
obs_vect[j] <- sum(list_threeyears[[i]]$Code == ID[[i]][j])
}
sub <- data.frame(ID = ID[[i]], obs_vect = obs_vect)
sub <- subset(sub, subset=c(sub$obs_vect > 10))
list_threeyears[[i]] <- subset(list_threeyears[[i]], list_threeyears[[i]]$Code %in% c(sub$ID))}
# Save list
saveRDS(list_threeyears, file="list_years.RData")
# Calculate Gambit of the group
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
list_years <- readRDS("list_years.RData")
# Calculate Gambit of the group
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
# Load all necessary packages
require(asnipe) # get_group_by_individual--Damien Farine
# Calculate Gambit of the group
gbi <- list()
group_data <- list()
for (i in seq_along(list_years)) {
# Group each individual by date and sighting
group_data[[i]] <- cbind(list_years[[i]][,c("Date","Sighting","Code","Year")])
group_data[[i]]$Group <- cumsum(!duplicated(group_data[[i]][1:2])) # Create sequential group # by date
group_data[[i]] <- cbind(group_data[[i]][,3:5]) # Subset ID and group #
# Gambit of the group index
gbi[[i]] <- get_group_by_individual(group_data[[i]][,c("Code", "Group")], data_format = "individuals")
}
saveRDS(gbi, file="gbi.RData")
# SIMPLE-RATIO INDEX ------------------------------------------------------
#' @description This function creates an association matrix using the simple-ratio index (SRI).
#' @param matr A binary matrix depicting individuals in the columns and groups in the rows
#' @return A square matrix in which each cell is an estimate of a dyadic social relationship, from 0 (never seen in the same group) to 1 (always seen in the same group)
SRI.func <-  function (matr) {
if (any(is.na(matr))) {
matr <- na.omit(matr)
cat("The data matrix contains NA, and have been removed.\n")
}
matr1 = matr
N <- nrow(matr1)
matr1[matr1 > 1] <- 1
n <- apply(matr1, 2, sum)
tmatr <- t(matr1)
df <- as.matrix(t(matr))
a <- df %*% t(df) # Dyad in same group
b <- df %*% (1 - t(df)) # A present, B absent
c <- (1 - df) %*% t(df) # A absent, B present
d <- ncol(df) - a - b - c # Double absent
Dice <- data.frame()
for (i in 1:nrow(a)) {
for (j in 1:ncol(a)) {
Dice[i, j] <- a[i, j]/(a[i, j] + b[i, j] + c[i, j])
}
}
rownames(Dice)=colnames(Dice)=colnames(matr)
Dice
}
# NULL PERMUTATIONS -------------------------------------------------------
#' @description shuffles binary matrices under different restrictions.
#' @param mat A quantitative matrix
#' @param iter Number of random matrices to be created
#' @param model Function to be chosen.
#' @param ... Further arguments from \code{permatswap} or \code{permatfull}
#' @return a list with \code{iter} random matrices
#' @details Totally restricted null model is called. Cell values are permuted restricting all features of the original matrix: column sums, row sums, matrix fill and total sum.
#' @references \code{citation("vegan")}
null <- function (mat, iter, ...){
require(vegan)
aux <- permatswap(mat, times=iter, method="quasiswap", fixedmar="both", shuffle="both", mtype="prab")
return(aux$perm)
}
# Run multiple cores for faster computing
require(doParallel)
n.cores <- detectCores()
system.time({
registerDoParallel(n.cores)
nxn <- list()
for (i in seq_along(list_years)) {
nxn[[i]] <- as.matrix(SRI.func(gbi[[i]]))
}
# End parallel processing
stopImplicitCluster()
})
# Save nxn list
saveRDS(nxn, file="nxn.RData")
library(sf) # Convert degrees to meters
library(sp) # Creates a SpatialPointsDataFrame by defining the coordinates
library(adehabitatHR) # Caluculate MCPs and Kernel density
# Read in file
sample_data <- read.csv("sample_data.csv")
list_years <- readRDS("list_years.RData")
# # Test one year at a time
year <- 5
coord_data <- list_years[[year]]
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
## Format date and year
coord_data$Date <- as.Date(as.character(coord_data$Date), format="%Y-%m-%d")
## Give descriptive names
colnames(coord_data) <- c("date", "y", "x", "id", "year", "HI")
# Only include three columns (id, x, and y coordinates) for making MCP's
dolph.sp <- coord_data[, c("id", "y", "x")]
# Create a simple feature data frame (sf)
coord_data_sf <- st_as_sf(dolph.sp, coords = c("x", "y"), crs = 4326)
# UTM zone for study area
dolph.sf <- st_transform(coord_data_sf, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp$x <- st_coordinates(dolph.sf)[, 1]
dolph.sp$y <- st_coordinates(dolph.sf)[, 2]
# Remove two rows with NA's
dolph.sp <- dolph.sp[!is.na(dolph.sp$x) & !is.na(dolph.sp$y),]
coordinates(dolph.sp) <- c("x", "y")
# Set the initial CRS for data to WGS84 (latitude and longitude)
proj4string(dolph.sp) <- CRS( "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs" )
# Get HRO
kernel <- kernelUD(dolph.sp, h = 1000)
coord_data <- list_years[[year]]
# Extract coordinates
coord_data <- cbind(sample_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
View(list_years)
coord_data <- list_years[[year]]
# Extract coordinates
coord_data <- cbind(coord_data[,c('Date', 'StartLat', 'StartLon', 'Code', 'Year', 'ConfHI')]) # Subset Date and Coordinates #
## Format date and year
coord_data$Date <- as.Date(as.character(coord_data$Date), format="%Y-%m-%d")
## Give descriptive names
colnames(coord_data) <- c("date", "y", "x", "id", "year", "HI")
# Only include three columns (id, x, and y coordinates) for making MCP's
dolph.sp <- coord_data[, c("id", "y", "x")]
# Create a simple feature data frame (sf)
coord_data_sf <- st_as_sf(dolph.sp, coords = c("x", "y"), crs = 4326)
# UTM zone for study area
dolph.sf <- st_transform(coord_data_sf, crs = paste0("+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs"))
# Extract coordinates (latitude and longitude) and create new columns
dolph.sp$x <- st_coordinates(dolph.sf)[, 1]
dolph.sp$y <- st_coordinates(dolph.sf)[, 2]
coordinates(dolph.sp) <- c("x", "y")
# Set the initial CRS for data to WGS84 (latitude and longitude)
proj4string(dolph.sp) <- CRS( "+proj=utm +zone=17 +datum=WGS84 +units=m +no_defs" )
# Get HRO
kernel <- kernelUD(dolph.sp, h = 1000)
kov <- kerneloverlaphr(kernel, method="HR", lev=95)
saveRDS(kov, "kov.RDS")
require(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(ncf) # For weights
require(vegan)
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
dolp_dist = nxn[[year]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
kov <- as.dist(kov) # Home range overlap
# Dissimilarity matrix
mantel.rtest(kov, dolp_dist, nrepet=999)
# Dissimilarity matrices
hro_test <- mantel.rtest(kov, dolp_dist, nrepet=999)
HI_test <- mantel.rtest(dissimilarity_HI, dolp_dist, nrepet=999)
# Select variables from the raw data
data <- list_years[[year]]
aux <- data[, c('Code', 'Behaviors', 'HumanInteraction', 'ConfHI')]
# Use 'Behaviors' variable to extract "Feed" and create another variable with two classes (Feed, Other)
aux$Foraging <- "Other"
aux$Foraging[grepl(pattern = 'Feed',
x = aux$Behaviors,
ignore.case = FALSE, perl = FALSE,
fixed = FALSE, useBytes = FALSE)] = "Feed"
#aux <- subset(aux, aux$Foraging == "Feed")
aux$ConfHI <- ifelse(aux$ConfHI == "0", 0, 1)
# Categorize ID to Foraging
IDbehav <- table(aux$Code, aux$Foraging)
IDbehav <- as.data.frame(IDbehav, stringsAsFactors = FALSE)
IDbehav <- IDbehav[,c(1,3)]
colnames(IDbehav) <- c("Code", "Forg_Freq")
# Group by the 'Code' column and sum the frequencies
IDbehav <- aggregate(. ~ Code, data = IDbehav, sum)
# Categorize ConfHI to IDs
rawHI <- as.matrix(table(aux$Code, aux$ConfHI))
rawHI <- as.data.frame(rawHI, stringsAsFactors = FALSE)
colnames(rawHI) <- c("Code", "ConfHI", "Freq")
## Add up the # of times each ID was seen in HI
IDbehav$HI <- rawHI$Freq[rawHI$ConfHI != 0]
IDdata <- IDbehav
colnames(IDdata) <- c("Code", "Foraging", "HI")
## Proportion of time Foraging spent in HI
IDdata$HIprop <- as.numeric(IDdata$HI)/as.numeric(IDdata$Foraging)
IDdata[is.na(IDdata)] <- 0
length(unique(aux$Code)) # 381 individuals should stay consistent
# Only ID to prop
HIprop_ID <- IDdata[,c(1, 4)]
# Dissimilarity of HI proportion among individual dolphins, using Euclidean distance
fake_HIprop <- HIprop_ID$HIprop
dissimilarity_HI <- as.matrix(dist(as.matrix(fake_HIprop), method = "euclidean"))
dissimilarity_HI[is.na(dissimilarity_HI)] <- 0
dissimilarity_HI <- as.dist(dissimilarity_HI) # HI dissimilarity
HI_test <- mantel.rtest(dissimilarity_HI, dolp_dist, nrepet=999)
plot(hro_test)
plot(HI_test)
plot(dissimilarity_HI, kov)
# So far no correlation with HI engagement and associations
hro_HI <- mantel.rtest(dissimilarity_HI, kov, nrepet=999)
# So far no correlation with HI engagement and associations
hro_HI <- mantel.rtest(dissimilarity_HI, kov, resamp=1000)
plot(hro_HI)
# Dissimilarity matrices
HI_test <- partial.mantel.test(dolp_dist, dissimilarity_HI, kov, resamp = 1000)
dim(kov)
dim(dissimilarity_HI)
dim(dolp_dist)
# Dissimilarity matrices
HI_test <- partial.mantel.test(dolp_dist, dissimilarity_HI, kov, resamp = 1000)
plot(HI_test)
knitr::opts_chunk$set(echo = TRUE)
# Set working directory here
setwd("C:/Users/bankh/My_Repos/Dolphins/data")
## load all necessary packages
require(ade4) # Look at Dai Shizuka/Jordi Bascompte
require(ncf) # For weights
require(vegan)
# Read file in to retain ILV
sample_data <- read.csv("sample_data.csv")
kov <- readRDS("kov.RDS")
# Read in social association matrix and data
nxn <- readRDS("nxn.RData")
list_years <- readRDS("list_years.RData")
# Transforming SRI similarity into distance
year <- 5
dolp_dist = nxn[[year]] + 0.00001
dolp_dist <- 1-nxn[[year]]
## Remove the redundant cells and the diagonal
dolp_dist <- as.dist(dolp_dist)
